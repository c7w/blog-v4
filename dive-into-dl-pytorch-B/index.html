<!DOCTYPE html><html lang="zh" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>《动手学深度学习》 Pytorch ver. 阅读摘录 Part B | c7w 的博客</title><meta name="keywords" content="机器学习"><meta name="author" content="c7w"><meta name="copyright" content="c7w"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="《动手学深度学习》原书地址：https:&#x2F;&#x2F;github.com&#x2F;d2l-ai&#x2F;d2l-zh 《动手学深度学习》(Pytorch ver.)：https:&#x2F;&#x2F;tangshusen.me&#x2F;Dive-into-DL-PyTorch&#x2F;#&#x2F;  知识架构：  本文的主要作用是在阅读过程中做一些摘录。对于「机器学习」领域， c7w 虽然曾尝试从各个领域入门，也尝试训过一些模型，但是还是缺少系统性、结构性的">
<meta property="og:type" content="article">
<meta property="og:title" content="《动手学深度学习》 Pytorch ver. 阅读摘录 Part B">
<meta property="og:url" content="https://www.c7w.tech/dive-into-dl-pytorch-B/index.html">
<meta property="og:site_name" content="c7w 的博客">
<meta property="og:description" content="《动手学深度学习》原书地址：https:&#x2F;&#x2F;github.com&#x2F;d2l-ai&#x2F;d2l-zh 《动手学深度学习》(Pytorch ver.)：https:&#x2F;&#x2F;tangshusen.me&#x2F;Dive-into-DL-PyTorch&#x2F;#&#x2F;  知识架构：  本文的主要作用是在阅读过程中做一些摘录。对于「机器学习」领域， c7w 虽然曾尝试从各个领域入门，也尝试训过一些模型，但是还是缺少系统性、结构性的">
<meta property="og:locale">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2022-01-26T04:01:54.000Z">
<meta property="article:modified_time" content="2022-01-27T06:59:48.550Z">
<meta property="article:author" content="c7w">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="/img/avatar.jpg"><link rel="canonical" href="https://www.c7w.tech/dive-into-dl-pytorch-B/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?08d6b753db81329ea728103fd0d2f84e";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '《动手学深度学习》 Pytorch ver. 阅读摘录 Part B',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-01-27 14:59:48'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="c7w 的博客" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s2.loli.net/2022/01/03/i8fNgXEPZDqnIlS.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">39</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">31</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://docs.net9.org/"><i class="fa-fw fas fa-book"></i><span> Docs</span></a></div><div class="menus_item"><a class="site-page" href="/friends/"><i class="fa-fw fas fa-heart"></i><span> Friends</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">c7w 的博客</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://docs.net9.org/"><i class="fa-fw fas fa-book"></i><span> Docs</span></a></div><div class="menus_item"><a class="site-page" href="/friends/"><i class="fa-fw fas fa-heart"></i><span> Friends</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">《动手学深度学习》 Pytorch ver. 阅读摘录 Part B</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-01-26T04:01:54.000Z" title="发表于 2022-01-26 12:01:54">2022-01-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-01-27T06:59:48.550Z" title="更新于 2022-01-27 14:59:48">2022-01-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%90%86%E8%AE%BA/">理论</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%90%86%E8%AE%BA/%E7%90%86%E8%AE%BA-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">理论/机器学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">12.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>45分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="《动手学深度学习》 Pytorch ver. 阅读摘录 Part B"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/cover.png" alt=""></p>
<ul>
<li>《动手学深度学习》原书地址：<a target="_blank" rel="noopener" href="https://github.com/d2l-ai/d2l-zh">https://github.com/d2l-ai/d2l-zh</a></li>
<li>《动手学深度学习》(Pytorch ver.)：<a target="_blank" rel="noopener" href="https://tangshusen.me/Dive-into-DL-PyTorch/#/">https://tangshusen.me/Dive-into-DL-PyTorch/#/</a></li>
</ul>
<p>知识架构：</p>
<p><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/book-org.svg" alt="封面"></p>
<p>本文的主要作用是在阅读过程中做一些摘录。对于「机器学习」领域， c7w 虽然曾尝试从各个领域入门，也尝试训过一些模型，但是还是缺少系统性、结构性的学习。希望阅读本书能带来更多的收获吧。</p>
<p>与前面的一些笔记相比，本文更加侧重于「实践」。也就是说切实地提升自己的代码能力。</p>
<p>Part B 包含：</p>
<ul>
<li>§ 5 CNN<ul>
<li>基本概念：卷积层、填充与步长、多通道、池化、批量归一化</li>
<li>模型的例子：LeNet、AlexNet、VGG、NiN、GoogLeNet、ResNet、DenseNet</li>
</ul>
</li>
<li>§ 6 RNN<ul>
<li>语言模型及其计算，N-gram 的概念</li>
<li>RNN 基本模型及其实现，字符数据集的制作</li>
<li>GRU, LSTM 的原理</li>
<li>Deep-RNN, bi-RNN</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><h4 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h4><ul>
<li><strong>二维互相关运算</strong></li>
</ul>
<p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.1_correlation.svg" alt="img"></p>
<p>如图所示，输入是一个高和宽均为3的二维数组。我们将该数组的形状记为 $3 \times 3$ 或$（3，3）$。</p>
<p>核数组的高和宽分别为 2。该数组在卷积计算中又称卷积核或过滤器（filter）。</p>
<p>卷积核窗口（又称卷积窗口）的形状取决于卷积核的高和宽，即 $2 \times 2$。</p>
<p>图中的阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：</p>
<p>$0\times0+1\times1+3\times2+4\times3=19$​​​​。</p>
<p>在二维互相关运算中，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，按照特定的步长，依次在输入数组上滑动。</p>
<p>当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素。</p>
<ul>
<li><strong>从互相关运算到卷积运算</strong></li>
</ul>
<p>实际上，卷积运算与互相关运算类似。<strong>为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算</strong>。</p>
<ul>
<li><strong>Feature Map 与 Receptive Field</strong></li>
</ul>
<p>二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。</p>
<p>影响元素 $x$ 的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做的 $x$ 感受野（receptive field）。</p>
<p>以图为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。</p>
<p>我们将图中形状为 $2 \times 2$​​​ 的输出记为 $Y$​​，并考虑一个更深的卷积神经网络：将 $Y$ 与另一个形状为 $2 \times 2$ 的核数组做互相关运算，输出单个元素 $z$。那么，$z$ 在 $Y$ 上的 Receptive Field 为 $Y$ 的全部四个元素，在 $x$ 上的感受野包括其中全部 9 个元素。</p>
<p>可见，我们可以<strong>通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征</strong>。</p>
<h4 id="Padding-amp-Stride"><a href="#Padding-amp-Stride" class="headerlink" title="Padding &amp; Stride"></a>Padding &amp; Stride</h4><p>本节我们将介绍卷积层的两个超参数，即填充和步幅。它们可以对给定形状的输入和卷积核改变输出形状。</p>
<ul>
<li><strong>Padding</strong></li>
</ul>
<p>填充（padding）是指在输入高和宽的两侧填充元素（通常是 0 元素）。</p>
<p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.2_conv_pad.svg" alt="img"></p>
<p>图中我们在原输入高和宽的两侧分别添加了值为 0 的元素，使得输入高和宽从 3 变成了 5 ，并导致输出高和宽由 2 增加到 4。</p>
<p>一般来说，如果在高的两侧一共填充 $p_h$ 行，在宽的两侧一共填充 $p_w$ 列，在很多情况下，我们会设置 $p_h=k_h-1$ 和 $p_w=k_w-1$ 来使输入和输出具有相同的高和宽，其中 $k_h\times k_w$ 是卷积核窗口形状。这样会方便在构造网络时推测每个层的输出形状。</p>
<p>假设这里 $k_h$ 是奇数，我们会在高的两侧分别填充 $p_h/2$ 行。如果 $k_h$ 是偶数，一种可能是在输入的顶端一侧填充 $\lceil p_h/2\rceil$ 行，而在底端一侧填充 $\lfloor p_h/2\rfloor$​ 行。在宽的两侧填充同理。卷积神经网络经常使用<strong>奇数高宽的卷积核</strong> $k_h \times k_w$，如 1、3、5 和 7，所以两端上的填充个数相等。</p>
<p>对任意的二维数组 <code>X</code>，设它的第 <code>i</code> 行第 <code>j</code> 列的元素为 <code>X[i,j]</code>。当两端上的填充个数相等，并使输入和输出具有相同的高和宽时，我们就知道输出 <code>Y[i,j]</code> 是由输入以 <code>X[i,j]</code> 为中心的窗口同卷积核进行互相关计算得到的。</p>
<ul>
<li><strong>Stride</strong></li>
</ul>
<p>在上一节里我们介绍了二维互相关运算。卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。我们将每次滑动的行数和列数称为步幅（stride）。</p>
<p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.2_conv_stride.svg" alt="img"></p>
<p>目前我们看到的例子里，在高和宽两个方向上步幅均为1。我们也可以使用更大步幅。</p>
<p>图中展示了在高上步幅为 3、在宽上步幅为 2 的二维互相关运算。可以看到，输出第一列第二个元素时，卷积窗口向下滑动了 3 行，而在输出第一行第二个元素时卷积窗口向右滑动了 2 列。当卷积窗口在输入上再向右滑动 2 列时，由于输入元素无法填满窗口，无结果输出。</p>
<h4 id="多通道"><a href="#多通道" class="headerlink" title="多通道"></a>多通道</h4><p>前面两节里我们用到的输入和输出都是二维数组，但真实数据的维度经常更高。</p>
<p>例如，彩色图像在高和宽 2 个维度外还有 RGB（红、绿、蓝）3 个颜色通道。</p>
<p>假设彩色图像的高和宽分别是 $h$ 和 $w$（像素），那么它可以表示为一个 $3\times h\times w$ 的多维数组。</p>
<p>我们将大小为 3 的这一维称为通道（channel）维。</p>
<p>本节我们将介绍含多个输入通道或多个输出通道的卷积核。</p>
<ul>
<li><strong>多输入通道</strong></li>
</ul>
<p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.3_conv_multi_in.svg" alt="img"></p>
<p>当输入数据含多个通道时，我们需要构造一个输入通道数与输入数据的通道数相同的卷积核，从而能够与含多通道的输入数据做互相关运算。</p>
<p>含多个通道的输入数据与多输入通道的卷积核做二维互相关运算的输出：在各个通道上对输入的二维数组和卷积核的二维核数组做互相关运算，再将这些互相关运算的输出相加。</p>
<ul>
<li><strong>多输出通道</strong></li>
</ul>
<p>当输入通道有多个时，因为我们对各个通道的结果做了累加，所以不论输入通道数是多少，输出通道数总是为 1。设卷积核输入通道数和输出通道数分别为 $c_i$ 和 $c_o$，高和宽分别为 $k_h$ 和 $k_w$。</p>
<p>如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为 $c_i\times k_h\times k_w$ 的核数组。将它们在输出通道维上连结，卷积核的形状即 $c_o\times c_i\times k_h\times k_w$。</p>
<ul>
<li><strong>1 x 1 卷积层</strong></li>
</ul>
<p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.3_conv_1x1.svg" alt="img"></p>
<p>因为使用了最小窗口，$1\times 1$ 卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。实际上，$1\times 1$ 卷积的主要计算发生在通道维上。</p>
<p>值得注意的是，输入和输出具有相同的高和宽。输出中的每个元素来自输入中在高和宽上相同位置的元素<strong>在不同通道之间的按权重累加</strong>。</p>
<p>假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，<strong>那么 $1\times 1$​ 卷积层的作用与全连接层等价</strong>。</p>
<h4 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h4><p>设任意二维数组 <code>X</code> 的 <code>i</code>行 <code>j</code> 列的元素为 <code>X[i, j]</code>。如果我们构造的 $1 \times 2$卷积核 $[1, -1]$ 输出 <code>Y[i, j]=1</code>，那么说明输入中 <code>X[i, j]</code> 和 <code>X[i, j+1]</code> 数值不一样。这可能意味着物体边缘通过这两个元素之间。</p>
<p>实际图像里，我们感兴趣的物体不会总出现在固定位置：即使我们连续拍摄同一个物体也极有可能出现像素位置上的偏移。这会导致同一个边缘对应的输出可能出现在卷积输出 <code>Y</code> 中的不同位置，进而对后面的模式识别造成不便。</p>
<p>在本节中我们介绍池化（pooling）层，它的提出是<strong>为了缓解卷积层对位置的过度敏感性</strong>。</p>
<ul>
<li><strong>2D-MaxPooling &amp; Mean Pooling</strong></li>
</ul>
<p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.4_pooling.svg" alt="img"></p>
<p>同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出。</p>
<p>不同于卷积层里计算输入和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也分别叫做最大池化或平均池化。</p>
<p>让我们再次回到本节开始提到的物体边缘检测的例子。现在我们将卷积层的输出作为 $2\times 2$ 最大池化的输入。设该卷积层输入是 <code>X</code>、池化层输出为 <code>Y</code>。无论是 <code>X[i, j]</code> 和 <code>X[i, j+1]</code> 值不同，还是 <code>X[i, j+1]</code> 和 <code>X[i, j+2]</code> 不同，池化层输出均有 <code>Y[i, j]=1</code>。也就是说，使用 $2\times 2$ 最大池化层时，只要卷积层识别的模式在高和宽上移动不超过一个元素，我们依然可以将它检测出来。</p>
<ul>
<li><strong>Padding &amp; Stride</strong></li>
</ul>
<p>同卷积层一样，池化层也可以在输入的高和宽两侧的填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。我们将通过 <code>nn</code> 模块里的二维最大池化层 <code>MaxPool2d</code> 来演示池化层填充和步幅的工作机制。</p>
<pre class="language-python" data-language="python"><code class="language-python">X <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># C_o * C_i * K_h * K_w</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span></code></pre>
<p>输出：<br><pre class="language-none"><code class="language-none">tensor([[[[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.],
          [ 8.,  9., 10., 11.],
          [12., 13., 14., 15.]]]])</code></pre></p>
<p>默认情况下，<code>MaxPool2d</code> 实例里步幅和池化窗口形状相同。下面使用形状为 $(3, 3)$ 的池化窗口，默认获得形状为 $(3, 3)$ 的步幅。</p>
<pre class="language-python" data-language="python"><code class="language-python">pool2d <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>
pool2d<span class="token punctuation">(</span>X<span class="token punctuation">)</span> </code></pre>
<p>输出：<br><pre class="language-none"><code class="language-none">tensor([[[[10.]]]])</code></pre></p>
<p>我们可以手动指定步幅和填充。</p>
<pre class="language-python" data-language="python"><code class="language-python">pool2d <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
pool2d<span class="token punctuation">(</span>X<span class="token punctuation">)</span></code></pre>
<p>输出：<br><pre class="language-none"><code class="language-none">tensor([[[[ 5.,  7.],
          [13., 15.]]]])</code></pre></p>
<p>当然，我们也可以指定非正方形的池化窗口，并分别指定高和宽上的填充和步幅。</p>
<pre class="language-python" data-language="python"><code class="language-python">pool2d <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
pool2d<span class="token punctuation">(</span>X<span class="token punctuation">)</span></code></pre>
<p>输出：<br><pre class="language-none"><code class="language-none">tensor([[[[ 1.,  3.],
          [ 9., 11.],
          [13., 15.]]]])</code></pre></p>
<ul>
<li><strong>多通道</strong></li>
</ul>
<p>在处理多通道输入数据时，<strong>池化层对每个输入通道分别池化，而不是像卷积层那样将各通道的输入按通道相加</strong>。</p>
<p>这意味着池化层的输出通道数与输入通道数相等。下面将数组 <code>X</code> 和 <code>X+1</code> 在通道维上连结来构造通道数为 2 的输入。</p>
<pre class="language-python" data-language="python"><code class="language-python">X <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> X <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
X</code></pre>
<p>输出：<br><pre class="language-none"><code class="language-none">tensor([[[[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.],
          [ 8.,  9., 10., 11.],
          [12., 13., 14., 15.]],
         [[ 1.,  2.,  3.,  4.],
          [ 5.,  6.,  7.,  8.],
          [ 9., 10., 11., 12.],
          [13., 14., 15., 16.]]]])</code></pre></p>
<p>池化后，我们发现输出通道数仍然是2。</p>
<pre class="language-python" data-language="python"><code class="language-python">pool2d <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
pool2d<span class="token punctuation">(</span>X<span class="token punctuation">)</span></code></pre>
<p>输出：<br><pre class="language-none"><code class="language-none">tensor([[[[ 5.,  7.],
          [13., 15.]],
         [[ 6.,  8.],
          [14., 16.]]]])</code></pre></p>
<h4 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h4><p>本节我们介绍批量归一化（batch normalization）层，它能让较深的神经网络的训练变得更加容易。</p>
<ul>
<li>为什么要有 Batch Normalization?</li>
</ul>
<p>在预测回归问题里，我们对输入数据做了标准化处理：处理后的任意一个特征在数据集中所有样本上的均值为 0、标准差为 1。标准化处理输入数据使各个特征的分布相近：这往往更容易训练出有效的模型。</p>
<p>通常来说，数据标准化预处理对于浅层模型就足够有效了。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。但对深层神经网络来说，即使输入数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。</p>
<p>批量归一化的提出正是为了应对深度模型训练的挑战。在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。<strong>批量归一化和下一节将要介绍的残差网络为训练和设计深度模型提供了两类重要思路</strong>。</p>
<ul>
<li>怎么做 Batch Normalization?</li>
</ul>
<p>对全连接层和卷积层做批量归一化的方法稍有不同。下面我们将分别介绍这两种情况下的批量归一化。</p>
<p><strong>对 Fully Connected Layer 的 Batch Normalization</strong></p>
<p>我们先考虑如何对全连接层做批量归一化。通常，我们将批量归一化层置于全连接层中的仿射变换和激活函数之间。设全连接层的输入为 $\boldsymbol{u}$​​，权重参数和偏差参数分别为 $\boldsymbol{W}$​ ​和 $\boldsymbol{b}$​​，激活函数为 $\phi$​​。设批量归一化的运算符为 $\text{BN}$​​。那么，使用批量归一化的全连接层的输出为 $\phi(\text{BN}(\boldsymbol{Wu+b}))$​。</p>
<p>下面我们解释 $\text{BN}$​ 算符是什么。</p>
<p>考虑一个由 $m$​​ 个样本组成的 Mini-batch，仿射变换的输出为一个新的 Mini-batch $\mathcal{B} = {\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(m)} }$​​。它们正是批量归一化层的输入。对于小批量 $\mathcal{B}$​ ​中任意样本 $\boldsymbol{x}^{(i)} \in \mathbb{R}^d, 1 \leq i \leq m$​​，批量归一化层的输出同样是 $d$​ ​维向量$\boldsymbol{y}^{(i)} = \text{BN}(\boldsymbol{x}^{(i)})$​，并由以下几步求得。</p>
<script type="math/tex; mode=display">
\boldsymbol{\mu}_\mathcal{B} \leftarrow \frac{1}{m}\sum_{i = 1}^{m} \boldsymbol{x}^{(i)} \\
\boldsymbol{\sigma}_\mathcal{B}^2 \leftarrow \frac{1}{m} \sum_{i=1}^{m}(\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_\mathcal{B})^2 \\ 
\hat{\boldsymbol{x}}^{(i)} \leftarrow \frac{\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_\mathcal{B}}{\sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon}}</script><p>这里 $\epsilon &gt; 0$ 是一个很小的常数，是为了保证分母大于 0。在上面标准化的基础上，批量归一化层引入了两个可以学习的模型参数，拉伸（scale）参数 $\boldsymbol{\gamma}$ 和偏移（shift）参数 $\boldsymbol{\beta}$。这两个参数和 $\boldsymbol{x}^{(i)}$ 形状相同，皆为 $d$ 维向量。它们与 $\boldsymbol{x}^{(i)}$ 分别做 Hadamard Product（符号$\odot$​）和加法计算：</p>
<script type="math/tex; mode=display">
{\boldsymbol{y}}^{(i)} \leftarrow \boldsymbol{\gamma} \odot \hat{\boldsymbol{x}}^{(i)} + \boldsymbol{\beta}</script><p>至此，我们得到了 $\boldsymbol{x}^{(i)}$​ ​的批量归一化的输出 $\boldsymbol{y}^{(i)}$​​。值得注意的是，可学习的拉伸和偏移参数保留了不对 $\hat{\boldsymbol{x}}^{(i)}$ ​​做批量归一化的可能：此时只需学出 $\boldsymbol{\gamma} = \sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon}$ ​​和 $\boldsymbol{\beta} = \boldsymbol{\mu}_\mathcal{B}$​​。我们可以对此这样理解：如果批量归一化无益，理论上，学出的模型可以不使用批量归一化。</p>
<p><strong>对 Conv. Layer 的 Batch Normalization</strong></p>
<p>对卷积层来说，批量归一化发生在卷积计算之后、应用激活函数之前。</p>
<p>如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且<strong>每个通道都拥有独立的拉伸和偏移参数，并均为标量</strong>。</p>
<p>设小批量中有 $m$ 个样本，在单个通道上，假设卷积计算输出的高和宽分别为 $p$ 和 $q$。我们需要对该通道中 $m \times p \times q$ 个元素同时做批量归一化。对这些元素做标准化计算时，我们使用相同的均值和方差，即该通道中 $m \times p \times q$​ 个元素的均值和方差。</p>
<p><strong>预测时的 Batch Normalization</strong></p>
<p>使用批量归一化训练时，我们可以将批量大小设得大一点，从而使批量内样本的均值和方差的计算都较为准确。将训练好的模型用于预测时，我们希望模型对于任意输入都有确定的输出。因此，<strong>单个样本的输出不应取决于批量归一化所需要的随机小批量中的均值和方差</strong>。一种常用的方法是通过移动平均<strong>估算整个训练数据集的样本均值和方差</strong>，并在预测时使用它们得到确定的输出。可见，和丢弃层一样，批量归一化层在训练模式和预测模式下的计算结果也是不一样的。</p>
<ul>
<li>实现（Simple ver.）</li>
</ul>
<p>与我们刚刚自己定义的 <code>BatchNorm</code> 类相比，Pytorch 中 <code>nn</code> 模块定义的 <code>BatchNorm1d</code> 和 <code>BatchNorm2d</code> 类使用起来更加简单，二者分别用于全连接层和卷积层，都需要指定输入的 <code>num_features</code> 参数值。下面我们用 PyTorch 实现使用批量归一化的 LeNet。</p>
<pre class="language-python" data-language="python"><code class="language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># in_channels, out_channels, kernel_size</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    
            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># kernel_size, stride</span>
    
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    
            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    
            d2l<span class="token punctuation">.</span>FlattenLayer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token operator">*</span><span class="token number">4</span><span class="token operator">*</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span></code></pre>
<h3 id="CNN-的例子"><a href="#CNN-的例子" class="headerlink" title="CNN 的例子"></a>CNN 的例子</h3><h4 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h4><p>之前我们曾使用 MLP 对 Fashion-MNIST 数据集中的图像进行分类。每张图像高和宽均是 28 像素。我们将图像中的像素逐行展开，得到长度为 784 的向量，并输入进全连接层中。然而，这种分类方法有一定的局限性。</p>
<ol>
<li>图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。</li>
<li>对于大尺寸的输入图像，使用全连接层容易造成模型过大。假设输入是高和宽均为 1000 像素的彩色照片（含 3 个通道）。即使全连接层输出个数仍是 256，该层权重参数的形状是$ 3,000,000\times 256$​：它占用了大约 3 GB 的内存或显存。这带来过复杂的模型和过高的存储开销。</li>
</ol>
<p>卷积层尝试解决这两个问题。一方面，卷积层保留输入形状，使图像的像素在高和宽两个方向上的相关性均可能被有效识别；另一方面，卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。</p>
<p>卷积神经网络就是含卷积层的网络。本节里我们将介绍一个早期用来识别手写数字图像的卷积神经网络：LeNet。</p>
<p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.5_lenet.png" alt="img"></p>
<p>LeNet分为卷积层块和全连接层块两个部分。下面我们分别介绍这两个模块。</p>
<p>卷积层块里的基本单位是<strong>卷积层后接最大池化层</strong>：</p>
<ul>
<li>卷积层用来识别图像里的空间模式，如线条和物体局部</li>
<li>之后的最大池化层则用来降低卷积层对位置的敏感性</li>
</ul>
<p>卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用 $5\times 5$​ 的窗口，并在输出上使用 sigmoid 激活函数。第一个卷积层输出通道数为 6，第二个卷积层输出通道数则增加到 16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层的窗口形状均为 $2\times 2$​​，且步幅为 2。由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。</p>
<p>卷积层块的输出形状为 (批量大小, 通道, 高, 宽)。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含 3 个全连接层。它们的输出个数分别是 120、84 和 10，其中 10 为输出的类别个数。</p>
<p>下面我们通过 <code>Sequential</code> 类来实现 LeNet 模型。</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LeNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>LeNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># in_channels, out_channels, kernel_size</span>
            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># kernel_size, stride</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token operator">*</span><span class="token number">4</span><span class="token operator">*</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img<span class="token punctuation">)</span><span class="token punctuation">:</span>
        feature <span class="token operator">=</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>img<span class="token punctuation">)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>feature<span class="token punctuation">.</span>view<span class="token punctuation">(</span>img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> output</code></pre>
<p>这里使用 GPU 进行计算，对相关函数的修改如下：</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 本函数已保存在d2lzh_pytorch包中方便以后使用</span>
<span class="token keyword">def</span> <span class="token function">train_ch5</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> device<span class="token punctuation">,</span> num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    net <span class="token operator">=</span> net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"training on "</span><span class="token punctuation">,</span> device<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        train_l_sum<span class="token punctuation">,</span> train_acc_sum<span class="token punctuation">,</span> n<span class="token punctuation">,</span> batch_count<span class="token punctuation">,</span> start <span class="token operator">=</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span>
            X <span class="token operator">=</span> X<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
            y <span class="token operator">=</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
            y_hat <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            train_l_sum <span class="token operator">+=</span> l<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
            train_acc_sum <span class="token operator">+=</span> <span class="token punctuation">(</span>y_hat<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
            n <span class="token operator">+=</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            batch_count <span class="token operator">+=</span> <span class="token number">1</span>
        test_acc <span class="token operator">=</span> evaluate_accuracy<span class="token punctuation">(</span>test_iter<span class="token punctuation">,</span> net<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'</span>
              <span class="token operator">%</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> train_l_sum <span class="token operator">/</span> batch_count<span class="token punctuation">,</span> train_acc_sum <span class="token operator">/</span> n<span class="token punctuation">,</span> test_acc<span class="token punctuation">,</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> start<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 本函数已保存在d2lzh_pytorch包中方便以后使用。该函数将被逐步改进。</span>
<span class="token keyword">def</span> <span class="token function">evaluate_accuracy</span><span class="token punctuation">(</span>data_iter<span class="token punctuation">,</span> net<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> device <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">and</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 如果没指定device就使用net的device</span>
        device <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>device
    acc_sum<span class="token punctuation">,</span> n <span class="token operator">=</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> data_iter<span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
                net<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 评估模式, 这会关闭 dropout</span>
                acc_sum <span class="token operator">+=</span> <span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
                net<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 改回训练模式</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span> <span class="token comment"># 自定义的模型, 3.13节之后不会用到, 不考虑GPU</span>
                <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token string">'is_training'</span> <span class="token keyword">in</span> net<span class="token punctuation">.</span>__code__<span class="token punctuation">.</span>co_varnames<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 如果有 is_training 这个参数</span>
                    <span class="token comment"># 将 is_training 设置成 False</span>
                    acc_sum <span class="token operator">+=</span> <span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">,</span> is_training<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> 
                <span class="token keyword">else</span><span class="token punctuation">:</span>
                    acc_sum <span class="token operator">+=</span> <span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> 
            n <span class="token operator">+=</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token keyword">return</span> acc_sum <span class="token operator">/</span> n</code></pre>
<h4 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h4><p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.6_alexnet.png" alt="img"></p>
<ul>
<li>Larger parameter size</li>
<li>Use ReLU instead of sigmoid</li>
<li>Introducing Dropout</li>
<li>Data augmentation</li>
</ul>
<h4 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h4><p>VGG 提出了可以通过重复使用简单的基础块来构建深度模型的思路。</p>
<p>VGG块的组成规律是：连续使用数个相同的填充为 1、窗口形状为 $3\times 3$ 的卷积层后接上一个步幅为 2、窗口形状为 $2\times 2$​ ​的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。我们使用<code>vgg_block</code>函数来实现这个基础的VGG 块，它可以指定卷积层的数量和输入输出通道数。</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">vgg_block</span><span class="token punctuation">(</span>num_convs<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
    blk <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_convs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            blk<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            blk<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        blk<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    blk<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 这里会使宽高减半</span>
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>blk<span class="token punctuation">)</span></code></pre>
<p>现在我们构造一个 VGG 网络。它有 5 个 <code>vgg_block</code>，前 2 块使用单卷积层 <code>num_convs=1</code>，而后 3 块使用双卷积层 <code>num_convs=2</code>。第一块的输入输出通道分别是 1（因为下面要使用的 Fashion-MNIST 数据的通道数为 1）和 64，之后每次对输出通道数翻倍，直到变为 512。因为这个网络使用了 8 个卷积层和 3 个全连接层，所以经常被称为 VGG-11。</p>
<pre class="language-python" data-language="python"><code class="language-python">conv_arch <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 经过5个 vgg_block, 宽高会减半5次, 变成 224/32 = 7</span>
fc_features <span class="token operator">=</span> <span class="token number">512</span> <span class="token operator">*</span> <span class="token number">7</span> <span class="token operator">*</span> <span class="token number">7</span> <span class="token comment"># c * w * h</span>
fc_hidden_units <span class="token operator">=</span> <span class="token number">4096</span> <span class="token comment"># 任意</span>

<span class="token keyword">def</span> <span class="token function">vgg</span><span class="token punctuation">(</span>conv_arch<span class="token punctuation">,</span> fc_features<span class="token punctuation">,</span> fc_hidden_units<span class="token operator">=</span><span class="token number">4096</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 卷积层部分</span>
    <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>num_convs<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>conv_arch<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 每经过一个vgg_block都会使宽高减半</span>
        net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"vgg_block_"</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> vgg_block<span class="token punctuation">(</span>num_convs<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># 全连接层部分</span>
    net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"fc"</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>d2l<span class="token punctuation">.</span>FlattenLayer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                 nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>fc_features<span class="token punctuation">,</span> fc_hidden_units<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                 nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                 nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                 nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>fc_hidden_units<span class="token punctuation">,</span> fc_hidden_units<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                 nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                 nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                 nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>fc_hidden_units<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
                                <span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> net</code></pre>
<h4 id="NiN"><a href="#NiN" class="headerlink" title="NiN"></a>NiN</h4><p>前几节介绍的 LeNet、AlexNet 和 VGG 在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。其中，AlexNet 和 VGG 对 LeNet 的改进主要在于如何对这两个模块加宽（增加通道数）和加深。</p>
<p>本节我们介绍网络中的网络（NiN）。它提出了另外一个思路，即串联多个由卷积层和“全连接”层构成的小网络来构建一个深层网络。NiN使用 $1\times 1$ 卷积层来替代全连接层，从而使空间信息能够自然传递到后面的层中去。</p>
<p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.8_nin.svg" alt="img"></p>
<p>左图是 AlexNet 和 VGG 的网络结构局部，右图是 NiN 的网络结构局部。</p>
<p><strong>NiN 块</strong>是 NiN 中的基础块。它由一个卷积层加两个充当全连接层的 $1\times 1$ 卷积层串联而成。其中第一个卷积层的超参数可以自行设置，而第二和第三个卷积层的超参数一般是固定的。</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">nin_block</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> stride<span class="token punctuation">,</span> padding<span class="token punctuation">)</span><span class="token punctuation">:</span>
    blk <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> stride<span class="token punctuation">,</span> padding<span class="token punctuation">)</span><span class="token punctuation">,</span>
                        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> blk</code></pre>
<p><strong>NiN 模型</strong>使用卷积窗口形状分别为 $11\times 11$​、$5\times 5$​ 和 $3\times 3$ ​的卷积层，相应的输出通道数也与 AlexNet 中的一致。每个 NiN 块后接一个步幅为 2、窗口形状为 $3\times 3$​​ 的最大池化层。</p>
<p>除使用 NiN 块以外，NiN 还有一个设计与 AlexNet 显著不同：NiN 去掉了 AlexNet 最后的3个全连接层，取而代之地，NiN 使用了输出通道数等于标签类别数的NiN 块，然后使用全局平均池化层对每个通道中所有元素求平均并直接用于分类。这里的全局平均池化层即窗口形状等于输入空间维形状的平均池化层。NiN 的这个设计的好处是可以显著减小模型参数尺寸，从而缓解过拟合。然而，该设计有时会造成获得有效模型的训练时间的增加。</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">class</span> <span class="token class-name">GlobalAvgPool2d</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>GlobalAvgPool2d<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> F<span class="token punctuation">.</span>avg_pool2d<span class="token punctuation">(</span>x<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    nin_block<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">96</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">11</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nin_block<span class="token punctuation">(</span><span class="token number">96</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nin_block<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">384</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
    nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># 标签类别数是10</span>
    nin_block<span class="token punctuation">(</span><span class="token number">384</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    GlobalAvgPool2d<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
    <span class="token comment"># 将四维的输出转成二维的输出，其形状为(批量大小, 10)</span>
    d2l<span class="token punctuation">.</span>FlattenLayer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>
<h4 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h4><p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.9_inception.svg" alt="img"></p>
<p>图为 Inception 块的结构。从这里我们可以意识到的一点是，以 Block 为单位来拼凑模型的这种方法逐渐火热…</p>
<h4 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h4><p>让我们先思考一个问题：对神经网络模型添加新的层，充分训练后的模型是否只可能更有效地降低训练误差？</p>
<p>理论上，原模型解的空间只是新模型解的空间的子空间。也就是说，如果我们能将新添加的层训练成恒等映射 $f(x) = x$​，新模型和原模型将同样有效。由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。</p>
<p>然而在实践中，添加过多的层后训练误差往往不降反升。即使利用批量归一化带来的数值稳定性使训练深层模型更加容易，该问题仍然存在。针对这一问题，残差网络被提出。</p>
<ul>
<li>残差块</li>
</ul>
<p>让我们聚焦于神经网络局部。如图所示，设输入为 $\boldsymbol{x}$。假设我们希望学出的理想映射为 $f(\boldsymbol{x})$，从而作为图上方激活函数的输入。左图虚线框中的部分需要直接拟合出该映射 $f(\boldsymbol{x})$，而右图虚线框中的部分则需要拟合出有关恒等映射的残差映射 $f(\boldsymbol{x})-\boldsymbol{x}$​。</p>
<p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.11_residual-block.svg" alt="img"></p>
<p>残差映射在实际中往往更容易优化。以本节开头提到的恒等映射作为我们希望学出的理想映射 $f(\boldsymbol{x})$。我们只需将图中右图虚线框内上方的加权运算（如仿射）的权重和偏差参数学成 0，那么 $f(\boldsymbol{x})$ 即为恒等映射。</p>
<p>实际中，当理想映射 $f(\boldsymbol{x})$ 极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。右图也是 ResNet 的基础块，即<strong>残差块</strong>（residual block）。在残差块中，输入可通过跨层的数据线路更快地向前传播。</p>
<p>ResNet 沿用了 VGG 全 $3\times 3$ 卷积层的设计。残差块里首先有 2 个有相同输出通道数的 $3\times 3$ 卷积层。每个卷积层后接一个批量归一化层和ReLU激活函数。然后我们将输入跳过这两个卷积运算后直接加在最后的ReLU激活函数前。这样的设计要求两个卷积层的输出与输入形状一样，从而可以相加。如果想改变通道数，就需要引入一个额外的 $1\times 1$​​ 卷积层来将输入变换成需要的形状后再做相加运算。</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Residual</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 本类已保存在d2lzh_pytorch包中方便以后使用</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> use_1x1conv<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Residual<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> use_1x1conv<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>conv3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>conv3 <span class="token operator">=</span> <span class="token boolean">None</span>
        self<span class="token punctuation">.</span>bn1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bn2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        Y <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>bn1<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        Y <span class="token operator">=</span> self<span class="token punctuation">.</span>bn2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>Y<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>conv3<span class="token punctuation">:</span>
            X <span class="token operator">=</span> self<span class="token punctuation">.</span>conv3<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
        <span class="token keyword">return</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>Y <span class="token operator">+</span> X<span class="token punctuation">)</span></code></pre>
<ul>
<li>ResNet 模型</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">resnet_block</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> num_residuals<span class="token punctuation">,</span> first_block<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> first_block<span class="token punctuation">:</span>
        <span class="token keyword">assert</span> in_channels <span class="token operator">==</span> out_channels <span class="token comment"># 第一个模块的通道数同输入通道数一致</span>
    blk <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_residuals<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">and</span> <span class="token keyword">not</span> first_block<span class="token punctuation">:</span>
            blk<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Residual<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> use_1x1conv<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            blk<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Residual<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>blk<span class="token punctuation">)</span>

net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"resnet_block1"</span><span class="token punctuation">,</span> resnet_block<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> first_block<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"resnet_block2"</span><span class="token punctuation">,</span> resnet_block<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"resnet_block3"</span><span class="token punctuation">,</span> resnet_block<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"resnet_block4"</span><span class="token punctuation">,</span> resnet_block<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"global_avg_pool"</span><span class="token punctuation">,</span> d2l<span class="token punctuation">.</span>GlobalAvgPool2d<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># GlobalAvgPool2d的输出: (Batch, 512, 1, 1)</span>
net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"fc"</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>d2l<span class="token punctuation">.</span>FlattenLayer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> </code></pre>
<h4 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h4><p>ResNet 中的跨层连接设计引申出了数个后续工作。本节我们介绍其中的一个：稠密连接网络。</p>
<p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.12_densenet.svg" alt="img"></p>
<p>图中将部分前后相邻的运算抽象为模块 A 和模块 B。</p>
<p>与 ResNet 的主要区别在于，DenseNet 里模块 B 的输出不是像 ResNet 那样和模块 A 的输出相加，而是在通道维上连结。这样模块 A 的输出可以直接传入模块 B 后面的层。在这个设计里，模块 A 直接跟模块 B 后面的所有层连接在了一起。这也是它被称为“稠密连接”的原因。</p>
<ul>
<li>DenseBlock</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">conv_block</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
    blk <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">)</span><span class="token punctuation">,</span> 
                        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> blk

<span class="token keyword">class</span> <span class="token class-name">DenseBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_convs<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>DenseBlock<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        net <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_convs<span class="token punctuation">)</span><span class="token punctuation">:</span>
            in_c <span class="token operator">=</span> in_channels <span class="token operator">+</span> i <span class="token operator">*</span> out_channels
            net<span class="token punctuation">.</span>append<span class="token punctuation">(</span>conv_block<span class="token punctuation">(</span>in_c<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span>net<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>out_channels <span class="token operator">=</span> in_channels <span class="token operator">+</span> num_convs <span class="token operator">*</span> out_channels <span class="token comment"># 计算输出通道数</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> blk <span class="token keyword">in</span> self<span class="token punctuation">.</span>net<span class="token punctuation">:</span>
            Y <span class="token operator">=</span> blk<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
            X <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> Y<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 在通道维上将输入和输出连结</span>
        <span class="token keyword">return</span> X</code></pre>
<ul>
<li>过渡层</li>
</ul>
<p>由于每个稠密块都会带来通道数的增加，使用过多则会带来过于复杂的模型。过渡层用来控制模型复杂度。它通过 $1\times1$ ​卷积层来减小通道数，并使用步幅为 2 的平均池化层减半高和宽，从而进一步降低模型复杂度。</p>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">transition_block</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>
    blk <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">)</span><span class="token punctuation">,</span> 
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> blk</code></pre>
<ul>
<li>DenseNet 模型</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

num_channels<span class="token punctuation">,</span> growth_rate <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">32</span>  <span class="token comment"># num_channels为当前的通道数</span>
num_convs_in_dense_blocks <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span>

<span class="token keyword">for</span> i<span class="token punctuation">,</span> num_convs <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>num_convs_in_dense_blocks<span class="token punctuation">)</span><span class="token punctuation">:</span>
    DB <span class="token operator">=</span> DenseBlock<span class="token punctuation">(</span>num_convs<span class="token punctuation">,</span> num_channels<span class="token punctuation">,</span> growth_rate<span class="token punctuation">)</span>
    net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"DenseBlock_%d"</span> <span class="token operator">%</span> i<span class="token punctuation">,</span> DB<span class="token punctuation">)</span>
    <span class="token comment"># 上一个稠密块的输出通道数</span>
    num_channels <span class="token operator">=</span> DB<span class="token punctuation">.</span>out_channels
    <span class="token comment"># 在稠密块之间加入通道数减半的过渡层</span>
    <span class="token keyword">if</span> i <span class="token operator">!=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>num_convs_in_dense_blocks<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">:</span>
        net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"transition_block_%d"</span> <span class="token operator">%</span> i<span class="token punctuation">,</span> transition_block<span class="token punctuation">(</span>num_channels<span class="token punctuation">,</span> num_channels <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        num_channels <span class="token operator">=</span> num_channels <span class="token operator">//</span> <span class="token number">2</span>

net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"BN"</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>num_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>
net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"relu"</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"global_avg_pool"</span><span class="token punctuation">,</span> d2l<span class="token punctuation">.</span>GlobalAvgPool2d<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># GlobalAvgPool2d的输出: (Batch, num_channels, 1, 1)</span>
net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"fc"</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>d2l<span class="token punctuation">.</span>FlattenLayer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_channels<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> </code></pre>
<h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><p>语言模型（language model）是自然语言处理的重要技术。自然语言处理中最常见的数据是文本数据。</p>
<p>我们可以把一段自然语言文本看作一段离散的时间序列。假设一段长度为 $T$ 的文本中的词依次为 $w_1, w_2, \ldots, w_T$，那么在离散的时间序列中，$w_t$（$1 \leq t \leq T$）可看作在时间步（time step）$t$ 的输出或标签。</p>
<p>给定一个长度为 $T$ 的词的序列 $w_1, w_2, \ldots, w_T$，语言模型将计算该序列的概率：$P(w_1, w_2, \ldots, w_T)$.</p>
<p>语言模型可用于提升语音识别和机器翻译的性能。</p>
<p>例如，在语音识别中，给定一段“厨房里食油用完了”的语音，有可能会输出“厨房里食油用完了”和“厨房里石油用完了”这两个读音完全一样的文本序列。如果语言模型判断出前者的概率大于后者的概率，我们就可以根据相同读音的语音输出“厨房里食油用完了”的文本序列。</p>
<p>在机器翻译中，如果对英文“you go first”逐词翻译成中文的话，可能得到“你走先”“你先走”等排列方式的文本序列。如果语言模型判断出“你先走”的概率大于其他排列方式的文本序列的概率，我们就可以把“you go first”翻译成“你先走”。</p>
<h4 id="语言模型的计算"><a href="#语言模型的计算" class="headerlink" title="语言模型的计算"></a>语言模型的计算</h4><p>根据《概率论》课程学过的有关知识，我们不难理解：</p>
<script type="math/tex; mode=display">
P(w_1, w_2, \ldots, w_T) = \prod_{t=1}^T P(w_t \mid w_1, \ldots, w_{t-1})</script><p>那么，这些概率该如何获得呢？</p>
<p>设训练数据集为一个大型文本语料库，如维基百科的所有条目。词的概率可以通过<strong>该词在训练数据集中的相对词频来计算</strong>。例如，$P(w_1)$ 可以计算为 $w_1$ 在训练数据集中的词频（词出现的次数）与训练数据集的总词数之比。因此，根据条件概率定义，一个词在给定前几个词的情况下的条件概率也可以通过训练数据集中的相对词频计算。再例如，$P(w_2 \mid w_1)$ 可以计算为 $w_1, w_2$ 两词相邻的频率与 $w_1$ 词频的比值，因为该比值即 $P(w_1, w_2)$ 与 $P(w_1)$ 之比；而 $P(w_3 \mid w_1, w_2)$ 同理可以计算为 $w_1$、$w_2$ 和 $w_3$ 三词相邻的频率与 $w_1$ 和 $w_2$ 两词相邻的频率的比值。以此类推。</p>
<h4 id="N-grams"><a href="#N-grams" class="headerlink" title="N-grams"></a>N-grams</h4><p>当序列长度增加时，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。$n$​ 元语法通过马尔可夫假设（虽然并不一定成立）简化了语言模型的计算。这里的马尔可夫假设是指，<strong>一个词的出现只与前面 $n$ ​个词相关</strong>。</p>
<p>如果 $n=1$，那么有 $P(w_3 \mid w_1, w_2) = P(w_3 \mid w_2)$。</p>
<p>如果基于 $n-1$ 阶马尔可夫链，我们可以将语言模型改写为：$P(w_1, w_2, \ldots, w_T) \approx \prod_{t=1}^T P(w_t \mid w_{t-(n-1)}, \ldots, w_{t-1}) $。</p>
<p>当 $n$ 分别为 1、2 和 3 时，我们将其分别称作一元语法（unigram）、二元语法（bigram）和三元语法（trigram）。</p>
<p>当 $n$ 较小时，$n$ 元语法往往并不准确。然而，当 $n$ 较大时，$n$ 元语法需要计算并存储大量的词频和多词相邻频率。那么，有没有方法在语言模型中更好地平衡以上这两点呢？我们将在本章探究这样的方法。</p>
<h3 id="RNN-1"><a href="#RNN-1" class="headerlink" title="RNN"></a>RNN</h3><p>本节将介绍循环神经网络。它并非刚性地记忆所有固定长度的序列，而是通过隐藏状态来存储之前时间步的信息。首先我们回忆一下前面介绍过的多层感知机，然后描述如何添加隐藏状态来将它变成循环神经网络。</p>
<ul>
<li>不含隐藏状态的神经网络</li>
</ul>
<p>让我们考虑一个含单隐藏层的多层感知机。给定样本数为 $n$、输入个数（特征数或特征向量维度）为 $d$ 的小批量数据样本 $\boldsymbol{X} \in \mathbb{R}^{n \times d}$。设隐藏层的激活函数为 $\phi$，那么隐藏层的输出 $\boldsymbol{H} \in \mathbb{R}^{n \times h}$ 计算为</p>
<script type="math/tex; mode=display">
\boldsymbol{H} = \phi(\boldsymbol{X} \boldsymbol{W}_{xh} + \boldsymbol{b}_h)</script><p>其中隐藏层权重参数 $\boldsymbol{W}_{xh} \in \mathbb{R}^{d \times h}$，隐藏层偏差参数 $\boldsymbol{b}_h \in \mathbb{R}^{1 \times h}$，$h$ 为隐藏单元个数。上式相加的两项形状不同，因此将按照广播机制相加。把隐藏变量 $\boldsymbol{H}$ 作为输出层的输入，且设输出个数为 $q$​​（如分类问题中的类别数），输出层的输出为 $\boldsymbol{O} = \boldsymbol{H} \boldsymbol{W}_{hq} + \boldsymbol{b}_q$.​ 其中输出变量 $\boldsymbol{O} \in \mathbb{R}^{n \times q}$, 输出层权重参数 $\boldsymbol{W}_{hq} \in \mathbb{R}^{h \times q}$, 输出层偏差参数 $\boldsymbol{b}_q \in \mathbb{R}^{1 \times q}$。如果是分类问题，我们可以使用 $\text{softmax}(\boldsymbol{O})$ 来计算输出类别的概率分布。</p>
<ul>
<li>含隐藏状态的 RNN</li>
</ul>
<p>现在我们考虑输入数据存在时间相关性的情况。假设 $\boldsymbol{X}_t \in \mathbb{R}^{n \times d}$ 是序列中时间步 $t$ 的小批量输入，$\boldsymbol{H}_t \in \mathbb{R}^{n \times h}$ 是该时间步的隐藏变量。</p>
<p>与多层感知机不同的是，这里我们保存上一时间步的隐藏变量 $\boldsymbol{H}_{t-1}$，并引入一个新的权重参数 $\boldsymbol{W}_{hh} \in \mathbb{R}^{h \times h}$，该参数用来描述在当前时间步如何使用上一时间步的隐藏变量。</p>
<p>具体来说，时间步 $t$ 的隐藏变量的计算由当前时间步的输入和上一时间步的隐藏变量共同决定：</p>
<script type="math/tex; mode=display">
\boldsymbol{H}_t = \phi(\boldsymbol{X}_t \boldsymbol{W}_{xh} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hh} + \boldsymbol{b}_h)</script><p>与多层感知机相比，我们在这里添加了 $\boldsymbol{H}_{t-1} \boldsymbol{W}_{hh}$ 一项。由上式中相邻时间步的隐藏变量 $\boldsymbol{H}_t$ ​和 $\boldsymbol{H}_{t-1}$ ​之间的关系可知，这里的隐藏变量能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。因此，该隐藏变量也称为隐藏状态。</p>
<p><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/chapter06/6.2_rnn.svg" alt="img"></p>
<p>由于隐藏状态在当前时间步的定义使用了上一时间步的隐藏状态，上式的计算是循环的。使用循环计算的网络即循环神经网络（recurrent neural network）。</p>
<p>而在时间步 $t$，输出层的输出和多层感知机中的计算类似：$\boldsymbol{O}_t = \boldsymbol{H}_t \boldsymbol{W}_{hq} + \boldsymbol{b}_q$.</p>
<h3 id="字符数据集的制作"><a href="#字符数据集的制作" class="headerlink" title="字符数据集的制作"></a>字符数据集的制作</h3><ul>
<li>读取数据集</li>
<li><p>建立字符索引 idx_to_char 与 char_to_idx</p>
</li>
<li><p>时序数据的采样</p>
<ul>
<li>随机采样：在随机采样中，每个样本是原始序列上任意截取的一段序列。相邻的两个随机小批量在原始序列上的位置不一定相毗邻。因此，我们无法用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态。在训练模型时，每次随机采样前都需要重新初始化隐藏状态。</li>
<li>相邻采样：令相邻的两个随机小批量在原始序列上的位置相毗邻。这时候，我们就可以用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态，从而使下一个小批量的输出也取决于当前小批量的输入，并如此循环下去。<ul>
<li>在训练模型时，我们只需在每一个迭代周期开始时初始化隐藏状态。</li>
<li>当多个相邻小批量通过传递隐藏状态串联起来时，模型参数的梯度计算将依赖所有串联起来的小批量序列。同一迭代周期中，随着迭代次数的增加，梯度的计算开销会越来越大。为了使模型参数的梯度计算只依赖一次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图中分离出来。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="RNN-的实现"><a href="#RNN-的实现" class="headerlink" title="RNN 的实现"></a>RNN 的实现</h3><h4 id="From-scratch"><a href="#From-scratch" class="headerlink" title="From scratch"></a>From scratch</h4><ul>
<li>单个词的表示：One-hot 向量</li>
<li>初始化模型参数与模型定义</li>
<li>预测函数的定义</li>
<li>裁剪梯度</li>
<li>模型评估：困惑度<ul>
<li>困惑度是对交叉熵损失函数做指数运算后得到的值<ul>
<li>最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；</li>
<li>最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；</li>
<li>基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Simple"><a href="#Simple" class="headerlink" title="Simple"></a>Simple</h4><p>Pytorch 实现：大调库</p>
<pre class="language-python" data-language="python"><code class="language-python">num_hiddens <span class="token operator">=</span> <span class="token number">256</span>
<span class="token comment"># rnn_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens) # 已测试</span>
rnn_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>RNN<span class="token punctuation">(</span>input_size<span class="token operator">=</span>vocab_size<span class="token punctuation">,</span> hidden_size<span class="token operator">=</span>num_hiddens<span class="token punctuation">)</span></code></pre>
<p>与上一节中实现的循环神经网络不同，这里 <code>rnn_layer</code> 的输入形状为 <code>(时间步数, 批量大小, 输入个数)</code>。其中输入个数即 one-hot 向量长度（词典大小）。</p>
<p>此外，<code>rnn_layer</code> 作为 <code>nn.RNN</code> 实例，在前向计算后会分别返回<strong>隐藏层的输出 $H$</strong> 和<strong>隐藏状态 $h$</strong>。</p>
<ul>
<li>$H$​ 指的是隐藏层在<strong>各个时间步</strong>上计算并输出的隐藏状态，它们通常作为后续输出层的输入，形状为 <code>(时间步数, 批量大小, 隐藏单元个数)</code>。</li>
<li>$h$ 指的是隐藏层在<strong>最后时间步</strong>的隐藏状态：当隐藏层有多层时，每一层的隐藏状态都会记录在该变量中；对于像长短期记忆（LSTM），隐藏状态是一个元组 $(h, c)$，即 hidden state 和 cell state。</li>
</ul>
<pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 本类已保存在d2lzh_pytorch包中方便以后使用</span>
<span class="token keyword">class</span> <span class="token class-name">RNNModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> rnn_layer<span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>RNNModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> rnn_layer
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> rnn_layer<span class="token punctuation">.</span>hidden_size <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token keyword">if</span> rnn_layer<span class="token punctuation">.</span>bidirectional <span class="token keyword">else</span> <span class="token number">1</span><span class="token punctuation">)</span> 
        self<span class="token punctuation">.</span>vocab_size <span class="token operator">=</span> vocab_size
        self<span class="token punctuation">.</span>dense <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>state <span class="token operator">=</span> <span class="token boolean">None</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># inputs: (batch, seq_len)</span>
        <span class="token comment"># 获取 one-hot 向量表示</span>
        X <span class="token operator">=</span> d2l<span class="token punctuation">.</span>to_onehot<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> self<span class="token punctuation">.</span>vocab_size<span class="token punctuation">)</span> <span class="token comment"># X 是个 list</span>
        Y<span class="token punctuation">,</span> self<span class="token punctuation">.</span>state <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">,</span> state<span class="token punctuation">)</span>
        <span class="token comment"># 全连接层会首先将 Y 的形状变成 (num_steps * batch_size, num_hiddens)，它的输出</span>
        <span class="token comment"># 形状为 (num_steps * batch_size, vocab_size)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>Y<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> Y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> output<span class="token punctuation">,</span> self<span class="token punctuation">.</span>state</code></pre>
<p>在前面两节中，如果不裁剪梯度，模型将无法正常训练。当总的时间步数较大或者当前时间步较小时，循环神经网络的梯度较容易出现衰减或爆炸。</p>
<h3 id="RNN-的改进"><a href="#RNN-的改进" class="headerlink" title="RNN 的改进"></a>RNN 的改进</h3><h4 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h4><p>虽然裁剪梯度可以应对梯度爆炸，但无法解决梯度衰减的问题。通常由于这个原因，循环神经网络在实际中较难捕捉时间序列中时间步距离较大的依赖关系。</p>
<p>门控循环神经网络（gated recurrent neural network）的提出，正是为了更好地捕捉时间序列中时间步距离较大的依赖关系。它通过可以学习的门来控制信息的流动。</p>
<ul>
<li>门控循环单元</li>
</ul>
<p><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/chapter06/6.7_gru_3.svg" alt="img"></p>
<script type="math/tex; mode=display">
\begin{aligned}
\boldsymbol{R}_t & = \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xr} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hr} + \boldsymbol{b}_r),\\
\boldsymbol{Z}_t & = \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xz} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hz} + \boldsymbol{b}_z), \\ 
\tilde{\boldsymbol{H}}_t & = \text{tanh}(\boldsymbol{X}_t \boldsymbol{W}_{xh} + \left(\boldsymbol{R}_t \odot \boldsymbol{H}_{t-1}\right) \boldsymbol{W}_{hh} + \boldsymbol{b}_h), \\ 
\boldsymbol{H}_t & = \boldsymbol{Z}_t \odot \boldsymbol{H}_{t-1}  + (1 - \boldsymbol{Z}_t) \odot \tilde{\boldsymbol{H}}_t.

\end{aligned}</script><ul>
<li>重置门有助于捕捉时间序列里短期的依赖关系<ul>
<li>重置门控制了上一时间步的隐藏状态如何流入当前时间步的候选隐藏状态</li>
<li>上一时间步的隐藏状态可能包含了时间序列截至上一时间步的全部历史信息</li>
<li>重置门可以用来丢弃与预测无关的历史信息</li>
</ul>
</li>
<li>更新门有助于捕捉时间序列里长期的依赖关系<ul>
<li>更新门可以控制隐藏状态应该如何被包含当前时间步信息的候选隐藏状态所更新</li>
<li>假设更新门在时间步 $t’$ 到 $t$（$t’ &lt; t$）之间一直近似 1。那么，在时间步 $t’$ 到 $t$ 之间的输入信息几乎没有流入时间步 $t$ 的隐藏状态 $\boldsymbol{H}_t$。实际上，这可以看作是较早时刻的隐藏状态 $\boldsymbol{H}_{t’-1}$ 一直通过时间保存并传递至当前时间步 $t$。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较大的依赖关系。</li>
</ul>
</li>
</ul>
<p>实现也可以直接大调库：</p>
<pre class="language-python" data-language="python"><code class="language-python">lr <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">2</span> <span class="token comment"># 注意调整学习率</span>
gru_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>input_size<span class="token operator">=</span>vocab_size<span class="token punctuation">,</span> hidden_size<span class="token operator">=</span>num_hiddens<span class="token punctuation">)</span>
model <span class="token operator">=</span> d2l<span class="token punctuation">.</span>RNNModel<span class="token punctuation">(</span>gru_layer<span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
d2l<span class="token punctuation">.</span>train_and_predict_rnn_pytorch<span class="token punctuation">(</span>model<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> device<span class="token punctuation">,</span>
                                corpus_indices<span class="token punctuation">,</span> idx_to_char<span class="token punctuation">,</span> char_to_idx<span class="token punctuation">,</span>
                                num_epochs<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> clipping_theta<span class="token punctuation">,</span>
                                batch_size<span class="token punctuation">,</span> pred_period<span class="token punctuation">,</span> pred_len<span class="token punctuation">,</span> prefixes<span class="token punctuation">)</span></code></pre>
<h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/chapter06/6.8_lstm_3.svg" alt="img"></p>
<script type="math/tex; mode=display">
\begin{aligned}
\boldsymbol{I}_t &= \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xi} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hi} + \boldsymbol{b}_i),\\
\boldsymbol{F}_t &= \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xf} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hf} + \boldsymbol{b}_f),\\
\boldsymbol{O}_t &= \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xo} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{ho} + \boldsymbol{b}_o), \\ 
\tilde{\boldsymbol{C}}_t &= \text{tanh}(\boldsymbol{X}_t \boldsymbol{W}_{xc} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hc} + \boldsymbol{b}_c), \\ 
\boldsymbol{C}_t & = \boldsymbol{F}_t \odot \boldsymbol{C}_{t-1} + \boldsymbol{I}_t \odot \tilde{\boldsymbol{C}}_t, \\ 
\boldsymbol{H}_t &= \boldsymbol{O}_t \odot \text{tanh}(\boldsymbol{C}_t).
\end{aligned}</script><ul>
<li>遗忘门控制上一时间步的记忆细胞 $\boldsymbol{C}_{t-1}$ 中的信息是否传递到当前时间步，而输入门则控制当前时间步的输入 $\boldsymbol{X}_t$ 通过候选记忆细胞 $\tilde{\boldsymbol{C}}_t$ 如何流入当前时间步的记忆细胞。<ul>
<li>如果遗忘门一直近似 1 且输入门一直近似 0，过去的记忆细胞将一直通过时间保存并传递至当前时间步。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较大的依赖关系。</li>
</ul>
</li>
<li>当输出门近似 1 时，记忆细胞信息将传递到隐藏状态供输出层使用；当输出门近似 0 时，记忆细胞信息只自己保留。</li>
</ul>
<p>实现也是大调库。</p>
<pre class="language-python" data-language="python"><code class="language-python">lr <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">2</span> <span class="token comment"># 注意调整学习率</span>
lstm_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>input_size<span class="token operator">=</span>vocab_size<span class="token punctuation">,</span> hidden_size<span class="token operator">=</span>num_hiddens<span class="token punctuation">)</span>
model <span class="token operator">=</span> d2l<span class="token punctuation">.</span>RNNModel<span class="token punctuation">(</span>lstm_layer<span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span>
d2l<span class="token punctuation">.</span>train_and_predict_rnn_pytorch<span class="token punctuation">(</span>model<span class="token punctuation">,</span> num_hiddens<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> device<span class="token punctuation">,</span>
                                corpus_indices<span class="token punctuation">,</span> idx_to_char<span class="token punctuation">,</span> char_to_idx<span class="token punctuation">,</span>
                                num_epochs<span class="token punctuation">,</span> num_steps<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> clipping_theta<span class="token punctuation">,</span>
                                batch_size<span class="token punctuation">,</span> pred_period<span class="token punctuation">,</span> pred_len<span class="token punctuation">,</span> prefixes<span class="token punctuation">)</span></code></pre>
<h4 id="Deep-RNN"><a href="#Deep-RNN" class="headerlink" title="Deep-RNN"></a>Deep-RNN</h4><p>本章到目前为止介绍的循环神经网络只有一个单向的隐藏层，在深度学习应用里，我们通常会用到含有多个隐藏层的循环神经网络，也称作深度循环神经网络。</p>
<p>图中演示了一个有 $L$ 个隐藏层的深度循环神经网络，每个隐藏状态不断传递至当前层的下一时间步和当前时间步的下一层。</p>
<p><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/chapter06/6.9_deep-rnn.svg" alt="img"></p>
<script type="math/tex; mode=display">
\begin{align}
\boldsymbol{H}_t^{(1)} &= \phi(\boldsymbol{X}_t \boldsymbol{W}_{xh}^{(1)} + \boldsymbol{H}_{t-1}^{(1)} \boldsymbol{W}_{hh}^{(1)}  + \boldsymbol{b}_h^{(1)}), \\ 
\boldsymbol{H}_t^{(\ell)} & = \phi(\boldsymbol{H}_t^{(\ell-1)} \boldsymbol{W}_{xh}^{(\ell)} + \boldsymbol{H}_{t-1}^{(\ell)} \boldsymbol{W}_{hh}^{(\ell)}  + \boldsymbol{b}_h^{(\ell)}), 1 \lt \ell \le L, \\ 
\boldsymbol{O}_t &= \boldsymbol{H}_t^{(L)} \boldsymbol{W}_{hq} + \boldsymbol{b}_q.
\end{align}</script><h4 id="bi-RNN"><a href="#bi-RNN" class="headerlink" title="bi-RNN"></a>bi-RNN</h4><p>之前介绍的循环神经网络模型都是假设当前时间步是由前面的较早时间步的序列决定的，因此它们都将信息通过隐藏状态从前往后传递。</p>
<p>有时候，当前时间步也可能由后面时间步决定。例如，当我们写下一个句子时，可能会根据句子后面的词来修改句子前面的用词。双向循环神经网络通过增加从后往前传递信息的隐藏层来更灵活地处理这类信息。</p>
<p><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/chapter06/6.10_birnn.svg" alt="img"></p>
<script type="math/tex; mode=display">
\begin{aligned}
\overrightarrow{\boldsymbol{H}}_t &= \phi(\boldsymbol{X}_t \boldsymbol{W}_{xh}^{(f)} + \overrightarrow{\boldsymbol{H}}_{t-1} \boldsymbol{W}_{hh}^{(f)}  + \boldsymbol{b}_h^{(f)}),\\
\overleftarrow{\boldsymbol{H}}_t &= \phi(\boldsymbol{X}_t \boldsymbol{W}_{xh}^{(b)} + \overleftarrow{\boldsymbol{H}}_{t+1} \boldsymbol{W}_{hh}^{(b)}  + \boldsymbol{b}_h^{(b)}),
\end{aligned}</script><p>然后我们连结两个方向的隐藏状态 $\overrightarrow{\boldsymbol{H}}_t$ 和 $\overleftarrow{\boldsymbol{H}}_t$ 来得到隐藏状态 $\boldsymbol{H}_t \in \mathbb{R}^{n \times 2h}$，并将其输入到输出层。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">c7w</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://www.c7w.tech/dive-into-dl-pytorch-B/">https://www.c7w.tech/dive-into-dl-pytorch-B/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.c7w.tech" target="_blank">c7w 的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/dive-into-dl-pytorch-practice/"><img class="prev-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">《动手学深度学习》 Pytorch ver. 阅读后练习</div></div></a></div><div class="next-post pull-right"><a href="/dive-into-dl-pytorch-A/"><img class="next-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">《动手学深度学习》 Pytorch ver. 阅读摘录 Part A</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s2.loli.net/2022/01/03/i8fNgXEPZDqnIlS.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">c7w</div><div class="author-info__description">Forever a c7w.</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">39</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">31</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/c7w" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:cc7w@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://twitter.com/c7wc7w" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN"><span class="toc-number">1.</span> <span class="toc-text">CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">1.1.</span> <span class="toc-text">基本概念</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">1.1.1.</span> <span class="toc-text">卷积层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Padding-amp-Stride"><span class="toc-number">1.1.2.</span> <span class="toc-text">Padding &amp; Stride</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E9%80%9A%E9%81%93"><span class="toc-number">1.1.3.</span> <span class="toc-text">多通道</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Pooling"><span class="toc-number">1.1.4.</span> <span class="toc-text">Pooling</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-number">1.1.5.</span> <span class="toc-text">批量归一化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CNN-%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="toc-number">1.2.</span> <span class="toc-text">CNN 的例子</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#LeNet"><span class="toc-number">1.2.1.</span> <span class="toc-text">LeNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#AlexNet"><span class="toc-number">1.2.2.</span> <span class="toc-text">AlexNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#VGG"><span class="toc-number">1.2.3.</span> <span class="toc-text">VGG</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#NiN"><span class="toc-number">1.2.4.</span> <span class="toc-text">NiN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GoogLeNet"><span class="toc-number">1.2.5.</span> <span class="toc-text">GoogLeNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ResNet"><span class="toc-number">1.2.6.</span> <span class="toc-text">ResNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DenseNet"><span class="toc-number">1.2.7.</span> <span class="toc-text">DenseNet</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RNN"><span class="toc-number">2.</span> <span class="toc-text">RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.1.</span> <span class="toc-text">语言模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="toc-number">2.1.1.</span> <span class="toc-text">语言模型的计算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#N-grams"><span class="toc-number">2.1.2.</span> <span class="toc-text">N-grams</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN-1"><span class="toc-number">2.2.</span> <span class="toc-text">RNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%97%E7%AC%A6%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%88%B6%E4%BD%9C"><span class="toc-number">2.3.</span> <span class="toc-text">字符数据集的制作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN-%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.4.</span> <span class="toc-text">RNN 的实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#From-scratch"><span class="toc-number">2.4.1.</span> <span class="toc-text">From scratch</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Simple"><span class="toc-number">2.4.2.</span> <span class="toc-text">Simple</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN-%E7%9A%84%E6%94%B9%E8%BF%9B"><span class="toc-number">2.5.</span> <span class="toc-text">RNN 的改进</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#GRU"><span class="toc-number">2.5.1.</span> <span class="toc-text">GRU</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#LSTM"><span class="toc-number">2.5.2.</span> <span class="toc-text">LSTM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Deep-RNN"><span class="toc-number">2.5.3.</span> <span class="toc-text">Deep-RNN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#bi-RNN"><span class="toc-number">2.5.4.</span> <span class="toc-text">bi-RNN</span></a></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By c7w</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      false && mermaid.init()
    })
  }
}</script></div><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>