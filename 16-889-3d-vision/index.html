<!DOCTYPE html><html lang="zh" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>16-889 三维视觉入门 | c7w 的博客</title><meta name="keywords" content="三维视觉"><meta name="author" content="c7w"><meta name="copyright" content="c7w"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="大概是一些笔记和个人想法吧。f.r. https:&#x2F;&#x2F;learning3d.github.io&#x2F;   个人想法用这种格式书写，与笔记区分。所以可以直接跳过这部分，只看正文。  本课程可能 cover 的内容：  三维模型的显式、隐式与 Nerual 表示 如何将三维的数据用可计算的方式来表示？ 不同类型的表示方式有哪些有点和缺点？   Image Formation Form 2D Images">
<meta property="og:type" content="article">
<meta property="og:title" content="16-889 三维视觉入门">
<meta property="og:url" content="https://www.c7w.tech/16-889-3d-vision/index.html">
<meta property="og:site_name" content="c7w 的博客">
<meta property="og:description" content="大概是一些笔记和个人想法吧。f.r. https:&#x2F;&#x2F;learning3d.github.io&#x2F;   个人想法用这种格式书写，与笔记区分。所以可以直接跳过这部分，只看正文。  本课程可能 cover 的内容：  三维模型的显式、隐式与 Nerual 表示 如何将三维的数据用可计算的方式来表示？ 不同类型的表示方式有哪些有点和缺点？   Image Formation Form 2D Images">
<meta property="og:locale">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2022-04-24T13:28:16.000Z">
<meta property="article:modified_time" content="2022-04-30T09:29:03.591Z">
<meta property="article:author" content="c7w">
<meta property="article:tag" content="三维视觉">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="/img/avatar.jpg"><link rel="canonical" href="https://www.c7w.tech/16-889-3d-vision/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?08d6b753db81329ea728103fd0d2f84e";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '16-889 三维视觉入门',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-04-30 17:29:03'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="c7w 的博客" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s2.loli.net/2022/01/03/i8fNgXEPZDqnIlS.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">62</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">44</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">35</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://docs.net9.org/"><i class="fa-fw fas fa-book"></i><span> Docs</span></a></div><div class="menus_item"><a class="site-page" href="/friends/"><i class="fa-fw fas fa-heart"></i><span> Friends</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">c7w 的博客</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://docs.net9.org/"><i class="fa-fw fas fa-book"></i><span> Docs</span></a></div><div class="menus_item"><a class="site-page" href="/friends/"><i class="fa-fw fas fa-heart"></i><span> Friends</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">16-889 三维视觉入门</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-04-24T13:28:16.000Z" title="发表于 2022-04-24 21:28:16">2022-04-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-04-30T09:29:03.591Z" title="更新于 2022-04-30 17:29:03">2022-04-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%90%86%E8%AE%BA/">理论</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%90%86%E8%AE%BA/%E7%90%86%E8%AE%BA-%E4%B8%89%E7%BB%B4%E8%A7%86%E8%A7%89/">理论/三维视觉</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>18分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="16-889 三维视觉入门"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>大概是一些笔记和个人想法吧。f.r. <a target="_blank" rel="noopener" href="https://learning3d.github.io/">https://learning3d.github.io/</a></p>
<blockquote>
<p> 个人想法用这种格式书写，与笔记区分。所以可以直接跳过这部分，只看正文。</p>
</blockquote>
<p>本课程可能 cover 的内容：</p>
<ul>
<li>三维模型的显式、隐式与 Nerual 表示<ul>
<li>如何将三维的数据用可计算的方式来表示？</li>
<li>不同类型的表示方式有哪些有点和缺点？</li>
</ul>
</li>
<li>Image Formation<ul>
<li>Form 2D Images from 3D Objects</li>
</ul>
</li>
<li>Data-driven 3D Prediction<ul>
<li>表示方法与学习目标</li>
<li>重建刚体、连接体、整个场景</li>
<li>弱监督？</li>
</ul>
</li>
<li>Instance-specific 3D from multi-view</li>
<li>Processing and Generating 3D</li>
<li>Potpurri + Project Presentations</li>
</ul>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>​        为什么要关注<strong>三维</strong>视觉（3D Vision）？数学家们考虑问题时，可以想象 $n$ 维空间中的情况，得出普遍性的结论之后，然后令 $n = 3$，便能得到一些有趣的结果。但是，<strong>我们所处的这个世界是三维的</strong>，针对三维的情况而特有的一些技巧或表示方法，可能比上述的普遍解法得到更优的效果。比如说，逆平方反比定律，就只有在给定<strong>三维</strong>这个条件之下才有效。</p>
<h2 id="3D-Representations"><a href="#3D-Representations" class="headerlink" title="3D Representations"></a>3D Representations</h2><p>下面介绍一些常用的三维场景表示方法。</p>
<ul>
<li><strong>深度图</strong>（Depth Maps）：传感器进行数据采集时的常用方法<ul>
<li>It is an <strong>image</strong> that represents <strong>how far</strong> each pixel <strong>p</strong> is.</li>
<li>For every pixel $p$ in the map, it stores $D[p] \in \mathbb R^+$.</li>
<li>深度图并没有捕捉到完整的 3D 结构，其只是记录了图像像素中的深度这一属性，可以称为 2.5D 表示</li>
</ul>
</li>
</ul>
<h3 id="Surface-Representations"><a href="#Surface-Representations" class="headerlink" title="Surface Representations"></a>Surface Representations</h3><p>这几种方法关注的是如何去刻画物体的表面。</p>
<ul>
<li><strong>点云</strong>（Point Clouds）：通常也是从传感器中采集，作为统一不同传感器不同输出数据格式的方式<ul>
<li>通常记作 $\{p_1, p_2, \cdots, p_N\}$，<strong>无序</strong>的点列表</li>
<li>点云是一种数据结构，通常表示成 $n \times 3$ 维的数组</li>
<li>为了保证其对无序性，或者说使得数组中任意两个元素交换位置后，点云整体的不变性，需要考虑具有置换不变性的处理方法（比如 FC 就不 work，因为输入位置不同对其结果有影响）</li>
<li>扩展：<strong>Oriented Point Clouds</strong>，同样是点的集合，不过集合元素为 $(p_i, n_i)$，记录了其法向用于渲染</li>
<li>点云没有记录各点之间的连接关系：虽然增大采样率可以在某种程度上缓解这个问题，但是明明加边更有效率…</li>
</ul>
</li>
<li><p><strong>网格</strong>（Meshes）：存储顶点与面的数据结构</p>
<ul>
<li>维护两个线性结构：顶点（Vertices）的集合、三角形面（Faces）的集合</li>
<li>对顶点标号，每一个面定义为一个三维数组，数组的内容是相应顶点的标号</li>
<li>是三维物体表面（Surface）的线性估计</li>
<li>但是通常来说，设计能够还原出<strong>任意</strong>连接方式的方法是困难的，这是因为即使是 8 个顶点和 10 个三角形面的网格也是 Non-trivial 的</li>
</ul>
</li>
<li><p><strong>参数曲面</strong>（Parametric Surfaces）</p>
<ul>
<li><p>通过一个映射 $f(u) = p \in \mathbb R^3; u \in M$ 来实现</p>
<ul>
<li>$f$ 需要是连续函数，$M$ 需要是二维流形</li>
</ul>
</li>
<li><p>更多例子：贝塞尔曲面</p>
</li>
<li><p>同时，我们之前的想法一直是手动构造这样的二维流形 $M$ 与函数 $f$，来拟合想要的三维模型。</p>
<ul>
<li>而我们可以固定流形 $M$ 为单位球面，$f$ 为函数 $\theta$ 的参数，这样我们得到 $f_\theta(u) = p \in \mathbb R^3; u \in \mathbb S^2$，是否可以让机器自己学习合适的 $f_\theta$ 满足拟合要求？</li>
<li>It is a nerual network with parameter $\theta$!</li>
</ul>
</li>
<li><p>参数曲面也有一些不利之处</p>
<ul>
<li><p>仅靠给定的流形和函数，很难对全局结构做出把握</p>
<ul>
<li>给定 $q$，$q$ 到底是在描述的三维形体之内还是之外？</li>
</ul>
</li>
<li><p>有时难以渲染（光追如何与 $f$ 求交？）</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Volume-Representations"><a href="#Volume-Representations" class="headerlink" title="Volume Representations"></a>Volume Representations</h3><p>这些方法关注于如何描述物体自身占有的空间。</p>
<ul>
<li>体素化存储（Voxelized 3D）<ul>
<li>这里 Voxel 的全拼是 Volume Pixel，类比 Pixel - Picture Element，Voxel 是 3D 存储的最基本单位（如下图，红色方块便是一个 Voxel）</li>
<li><img src="https://s2.loli.net/2022/04/25/WvgS9cR7LlYtibP.png" alt="Voxels.svg"></li>
<li>Voxel 将连续的三维空间离散化</li>
<li>一个 Voxel 可以代表 $(W , H, D)$ 大小的实际空间</li>
<li>这种类图像的表示对卷积等图像处理与生成方式友好</li>
<li>缺点：增大分辨率可能会导致计算性能的需求以立方的次数增长</li>
</ul>
</li>
<li><strong>隐式曲面</strong>（Implicit Surfaces）<ul>
<li>其定义：$\{ \mathbf p | f (\mathbf p) = 0 \}$.</li>
<li>我们可以对隐式曲面进行：<ul>
<li><strong>并</strong>操作：$\bigcup_i f_i(\mathbf p) = \min_i f_i(\mathbf p)$.</li>
<li><strong>交</strong>操作：$\bigcap_i f_i(\mathbf p) = \max_i f_i(\mathbf p)$.</li>
<li><strong>差</strong>操作：$(f-g)(\mathbf p) = \max(f(\mathbf p), -g(\mathbf p))$.</li>
</ul>
</li>
<li>当然我们可以将 $f$ 看做 $f_\theta$…</li>
<li>Signed Distance Function<ul>
<li>为函数增加条件：$f(p)$ 要代表点 $p$ 到所表示曲面的距离</li>
<li>这种函数满足条件：$|| \nabla f || = 1$.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Other-Representations"><a href="#Other-Representations" class="headerlink" title="Other Representations"></a>Other Representations</h3><ul>
<li><strong>八元树</strong>（Octrees）<ul>
<li>对于渲染和存储来说十分方便</li>
<li>很难编辑或预测</li>
</ul>
</li>
<li>Layered Depth Images<ul>
<li>Layer 1 = Visible Content</li>
<li>Layer $n + 1$ = Depth and color of the “next” surface point the ray hits after Layer $n$</li>
</ul>
</li>
<li>Multi-plane Images<ul>
<li>极其方便渲染，each layer is just a plane at fixed length</li>
<li>Limited resolution for far-away ccontent</li>
</ul>
</li>
<li>CAD Models</li>
<li>CSG Trees<ul>
<li>物体的交并差组成的“计算图”</li>
</ul>
</li>
</ul>
<h3 id="更多讨论"><a href="#更多讨论" class="headerlink" title="更多讨论"></a>更多讨论</h3><p>我们关注以下两个问题：</p>
<ol>
<li>从可微性的角度，连续的变换能否拟合出相应的变化？</li>
<li>从渲染的角度，将 3D 表示添加纹理后渲染出来是否简单？</li>
</ol>
<h2 id="Conversions-between-3D-Representations"><a href="#Conversions-between-3D-Representations" class="headerlink" title="Conversions between 3D Representations"></a>Conversions between 3D Representations</h2><p>本节我们分为以下三个方面，介绍三维表示之间的不同转换方法：</p>
<p><img src="https://s2.loli.net/2022/04/25/mKGudAtURpbiQSw.png" alt="image-20220425171456719"></p>
<ul>
<li>Surface Representations 之间的转换</li>
<li>Volume Representations 之间的转换</li>
<li>Surface Representations 和 Volume Representations 之间的转换</li>
</ul>
<h3 id="何谓“可微变换”"><a href="#何谓“可微变换”" class="headerlink" title="何谓“可微变换”"></a>何谓“可微变换”</h3><p>首先，变换的定义我们在《离散数学》课程中就已经学过，是一个集合到自身上的映射。那么，什么是可微变换呢？</p>
<p><img src="https://s2.loli.net/2022/04/25/XxWRFetBq3D7GhM.png" alt="image-20220425171919339"></p>
<p>我们假设给定图片输入 Input，其可用参数 $\theta_1$ 来表示，而经过了变换 $\mathcal T$ 之后，其得到了点云可用参数 $\theta_2$ 来表示。如果我们可以计算出 L 对两个参数的偏导，那么就可以计算出这个变换的微分。</p>
<blockquote>
<p>这个 Loss 都有哪些计算方式？也就是说，怎么去衡量这个 Loss 对 $\theta$ 的微分？</p>
</blockquote>
<h3 id="Volume-Representations-之间的转换"><a href="#Volume-Representations-之间的转换" class="headerlink" title="Volume Representations 之间的转换"></a>Volume Representations 之间的转换</h3><h4 id="Approximating-Continuous-from-Discrete"><a href="#Approximating-Continuous-from-Discrete" class="headerlink" title="Approximating Continuous from Discrete"></a>Approximating Continuous from Discrete</h4><p><img src="https://s2.loli.net/2022/04/25/k8X4gE79iKUPM1G.png" alt="image-20220425173204447"></p>
<p>没错，其核心问题在于图中的注释，给定离散的网格对应的值，怎样才能将其连续化呢？没错，插值。我们早已学过一阶线性插值和双线性插值的概念，而这里我们便使用<strong>三线性插值</strong>来处理三维的问题。</p>
<h4 id="From-Continuous-to-Discrete"><a href="#From-Continuous-to-Discrete" class="headerlink" title="From Continuous to Discrete"></a>From Continuous to Discrete</h4><p><img src="https://s2.loli.net/2022/04/25/yUAbrKpl4N5PMgu.png" alt="image-20220425173544322"></p>
<p>对于 sigmoid 函数，我们易于求 $\dfrac {\partial V}{\partial f}$.</p>
<h3 id="Surface-Representations-之间的转换"><a href="#Surface-Representations-之间的转换" class="headerlink" title="Surface Representations 之间的转换"></a>Surface Representations 之间的转换</h3><h4 id="From-Continuous-to-Discrete-1"><a href="#From-Continuous-to-Discrete-1" class="headerlink" title="From Continuous to Discrete"></a>From Continuous to Discrete</h4><p><img src="https://s2.loli.net/2022/04/25/S5pjk7ctMZF9D6o.png" alt="image-20220425174320830"></p>
<p>我们首先将定义域的二维流形离散化，然后将离散化后的区域使用 $f$ 进行映射。</p>
<h4 id="Sampling-Points-on-a-Triangle-Mesh"><a href="#Sampling-Points-on-a-Triangle-Mesh" class="headerlink" title="Sampling Points on a Triangle Mesh"></a>Sampling Points on a Triangle Mesh</h4><p><img src="https://s2.loli.net/2022/04/25/HsbmWnxOF2zBZUl.png" alt="image-20220425174422681">接下来，给定一个三角形，该如何在其上均匀采样呢？</p>
<p><img src="https://s2.loli.net/2022/04/25/X3GIHKdfevLlZBJ.png" alt="image-20220425174609673"></p>
<h3 id="Volume-Representation-和-Surface-Representation-之间的转换"><a href="#Volume-Representation-和-Surface-Representation-之间的转换" class="headerlink" title="Volume Representation 和 Surface Representation 之间的转换"></a>Volume Representation 和 Surface Representation 之间的转换</h3><h4 id="Volumes-to-Surface-Marching-Cubes"><a href="#Volumes-to-Surface-Marching-Cubes" class="headerlink" title="Volumes to Surface: Marching Cubes"></a>Volumes to Surface: Marching Cubes</h4><p><img src="https://s2.loli.net/2022/04/25/f4GtxzQM5AqlunS.png" alt="image-20220425184506421"></p>
<p>我们本质上是要解决如何找出<strong>曲面边界</strong>的问题。比如，上面这张图绿色表示点在曲面内部，红色表示点在曲面外部，而具体该如何决定这个曲面边界所在的位置。通过这张图我们可以看到，对于每一个 Cube，决定其表面所在的边界是较为简单的。于是我们的想法便是，打表…</p>
<p><img src="https://s2.loli.net/2022/04/25/CaYZAq1L8UWF7et.png" alt="image-20220425184311060"></p>
<p>同时，使用 Voxel 的属性值来影响顶点的位置，这也保证了可微性。</p>
<h4 id="Points-to-Meshes-via-Implicit-Functions"><a href="#Points-to-Meshes-via-Implicit-Functions" class="headerlink" title="Points to Meshes via Implicit Functions"></a>Points to Meshes via Implicit Functions</h4><p>Points to Meshes 本来是 Surface Representations 方面的内容，而直接在点云中建立不同点之间的连接关系让人感觉无从下手。</p>
<p>而我们可以认为，点云中的点是某个隐函数取值为 0 的点集中 sample 出来的。我们可以尝试先拟合出隐函数，然后再将其转换成 Meshes.</p>
<blockquote>
<p><strong>Poisson Reconstruction</strong></p>
<p>首先，Possion Equation 的定义：</p>
<script type="math/tex; mode=display">
\left(\frac{\partial^{2}}{\partial x^{2}}+\frac{\partial^{2}}{\partial y^{2}}+\frac{\partial^{2}}{\partial z^{2}}\right) \varphi(x, y, z)=f(x, y, z)</script></blockquote>
<h2 id="Image-Formation"><a href="#Image-Formation" class="headerlink" title="Image Formation"></a>Image Formation</h2><p>这一节我们关注成像相关的问题。我们关注的是<strong>几何</strong>与<strong>物理</strong>：几何是 3D 空间中的某个点该映射到图像中的什么地方，而<strong>物理</strong>是说像素的颜色该如何被计算。</p>
<ul>
<li>小孔成像的原理</li>
</ul>
<h3 id="Geometry"><a href="#Geometry" class="headerlink" title="Geometry"></a>Geometry</h3><ul>
<li><p>正交投影与透视投影</p>
</li>
<li><p>齐次坐标</p>
<ul>
<li><p>我们假设成像平面在 $Z = 1$.</p>
</li>
<li><p>将三维空间中的点映射到像平面</p>
<ul>
<li>$(X, Y, Z) \rightarrow (X/Z, Y/Z)$，即 $\mathbb R^3 \rightarrow \mathbb R^2$.</li>
</ul>
</li>
<li><p>定义 Projective 2D Space $\mathbb P^2$ 满足 $(x,y,z) \equiv (\lambda x, \lambda y, \lambda z)$，其满足的性质：</p>
<ul>
<li>$(x, y) \rightarrow(x,y,1)$，这是 $\mathbb R^2 \rightarrow \mathbb P^2$</li>
<li>$(x,y,z) \rightarrow(x/z, y/z)$，这是 $\mathbb P^2 \rightarrow \mathbb R^2$.</li>
</ul>
</li>
<li><p>然后，定义 Projective 3D Space $\mathbb P^3$，定义 $\mathbb P^3 \rightarrow \mathbb P^2$ 的 Projection Matrix $\mathbf P$ 如下：</p>
<ul>
<li><script type="math/tex; mode=display">
\left[\begin{array}{llll}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0
\end{array}\right]</script></li>
<li><p>这个 Projection Matrix 相当于取 $\mathbb P^3$ 这个四维坐标的前三项后得到 $\mathbb P^2$ 中的元素。</p>
</li>
<li><p>$\mathbf P X_{cam} = X_{img}$.</p>
</li>
</ul>
</li>
<li><p>更一般地，如果我们的像平面在 $Z = f$ 的位置，事实上我们可以把 $Z = 1$ 的这个“像”的 $x, y$ scale $f$ 倍，得出新的投影阵：</p>
<ul>
<li><script type="math/tex; mode=display">
P \equiv\left[\begin{array}{llll}
f & 0 & 0 & 0 \\
0 & f & 0 & 0 \\
0 & 0 & 1 & 0
\end{array}\right]</script></li>
</ul>
</li>
<li><p>而接下来，再考虑更加一般的情况，任意的图像中心。</p>
<ul>
<li><script type="math/tex; mode=display">
\mathbf{P}=\left[\begin{array}{llc}
f & 0 & p_{x} \\
0 & f & p_{y} \\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{lll:l}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0
\end{array}\right]</script></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://s2.loli.net/2022/04/25/xFypLgbDud4UavB.png" alt="image-20220425200131195"></p>
<p>然后是应用一个平移 + 旋转换基变换将摄像机的中心移到原点，up 与 y 方向对齐。</p>
<p>注意不同的库在建坐标系时有不同的 Convention，这是可能导致 Bug 的原因之一。</p>
<h3 id="Physics"><a href="#Physics" class="headerlink" title="Physics"></a>Physics</h3><p>熟悉的渲染方程：</p>
<script type="math/tex; mode=display">
L_{o}(x, v)=\int_{\Omega} f_{r}(x, v, \omega) L_{i}(x, \omega)(\omega \cdot n) d \omega</script><h2 id="Single-view-3D-History-2-5D"><a href="#Single-view-3D-History-2-5D" class="headerlink" title="Single-view 3D: History + 2.5D"></a>Single-view 3D: History + 2.5D</h2><p>我们关注的问题是，如何将一张 2D 的图像理解成 3D 的表示。上面几部分我们关注“怎样表示”，而接下来的几节我们关注以怎样的方法来进行理解。</p>
<p><img src="https://s2.loli.net/2022/04/26/6tKqbfw4adE8N7I.png" alt="image-20220426233313729"></p>
<p><img src="https://s2.loli.net/2022/04/26/pgz8Vkvjuh2L6JS.png" alt="image-20220426233335167"></p>
<p>虽然有些图像我们难以界定其三维场景（比如上面几张图中的问题），但我们所在的世界是一个<strong>有规律性</strong>的世界，见下面几张图。</p>
<p><img src="https://s2.loli.net/2022/04/27/CXagBjSJefUOWc5.png" alt="image-20220427191825020"></p>
<p><img src="https://s2.loli.net/2022/04/27/5cJalGIndSxE9eU.png" alt="image-20220427191832287"></p>
<p><img src="https://s2.loli.net/2022/04/27/pv3YeU6FDRJ2q1h.png" alt="image-20220427191842738"></p>
<p><img src="https://s2.loli.net/2022/04/27/TeS8cNIdrqMVtCj.png" alt="image-20220427191852638"></p>
<p>虽然有些时候，我们对一个场景有无穷多种解释，但是有些解释比其他的可能性更大。几何的知识虽然信息量足够大，但有时还是不足够的，我们需要对于可能的场景结构有一些先验知识，以尽可能减小我们的“搜索空间”。</p>
<p>那么这些先验知识都有哪些呢？一种是<strong>面向用户（User-specific）</strong>的，不同的用户的先验知识不同，另一种是<strong>数据驱动型（Data-Driven）</strong>的。目前的研究范式是：<strong>主要为数据驱动型，加入特定的 Inductive bias.</strong></p>
<blockquote>
<p> What is inductive bias? </p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%BD%92%E7%BA%B3%E5%81%8F%E7%BD%AE">https://zh.wikipedia.org/wiki/%E5%BD%92%E7%BA%B3%E5%81%8F%E7%BD%AE</a> </li>
<li><a target="_blank" rel="noopener" href="https://www.baeldung.com/cs/ml-inductive-bias">https://www.baeldung.com/cs/ml-inductive-bias</a></li>
</ul>
<p>大概我的理解就是，一个模型所做的假设吧，这个假设可能正确性很高(strong)或是很低(weak)）。比如后续我们提到说要对室内场景建模，给出的这个 World Model 那么就是 Inductive Bias.</p>
</blockquote>
<h3 id="3D-Inference-发展的历史"><a href="#3D-Inference-发展的历史" class="headerlink" title="3D Inference 发展的历史"></a>3D Inference 发展的历史</h3><ul>
<li>The first vision thesis: Model-based fitting<ul>
<li>Improving Model-fitting: A Representative Approach</li>
</ul>
</li>
<li>Automatic Photo Pop-up</li>
<li><p>Modeling Indoor Scenes (Given the World Model as Inductive Bias)</p>
<ul>
<li>在结构方面的先验知识（Structural Prior）</li>
</ul>
</li>
<li><p>Towards Data-driven Modeling: Morphable Models for Faces</p>
<ul>
<li>从 3D Faces 的数据集中学习模型参数</li>
<li>然后用来推断未见过的图像</li>
</ul>
</li>
<li><p>Data-driven Modeling: A modern view</p>
<ul>
<li>使用 CNN 等神经网络，在大笔训练资料上训练神经网络</li>
<li>Learning from direct supervision!</li>
<li>我们有数据集！比如 ImageNet，比如 COCO (Common Objects in Context)</li>
</ul>
</li>
</ul>
<h3 id="Depth-Prediction：场景深度预测任务"><a href="#Depth-Prediction：场景深度预测任务" class="headerlink" title="Depth Prediction：场景深度预测任务"></a>Depth Prediction：场景深度预测任务</h3><p>任务定义非常简单：输入 2D 图片，输出这个图片对应的场景中，每个点对应的深度。</p>
<p><img src="https://s2.loli.net/2022/04/27/kz9MBWvFSqNEtLr.png" alt="image-20220427192302730"></p>
<h4 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h4><p>这样设计 Loss 的核心问题：接近成像平面的较小的物体和远离成像平面的较大的物体可能最终在图像上的表现相同。绝对深度是难以确定的…于是我们的 Loss 需要满足：</p>
<script type="math/tex; mode=display">
L(y, y^*) = L(\alpha y, y^*), \forall \alpha > 0.</script><p>这样设计出的 Loss Function 为：</p>
<script type="math/tex; mode=display">
\begin{gathered}
L\left(y, y^{*}\right)=\sum_{i}\left\|\log y_{i}-\log y_{i}^{*}+\alpha\left(y, y^{*}\right)\right\|^{2} \\
\alpha\left(y, y^{*}\right)=\frac{1}{n} \sum_{i}\left(\log y_{i}^{*}-\log y_{i}\right)
\end{gathered}</script><blockquote>
<p>From intuition we can know that, 把 log 去掉的话这个 Loss 是 $\dfrac {y_i}{y_i^\star} \times \text{avg}(\dfrac{y^\star}{y})$，如果我们只关注预测值和真实值之间的比例关系的话，预测出 $y_i^\star = \alpha y_i, \forall i$ 的结果肯定是最好的。但是在真正运作的时候，我们没法保证这个 $\alpha$ 是个常数，于是我们就索性取了平均值。这个式子越接近于 1，取个 log，原来的 Loss Function 就越接近 0，不然就会因为范数的存在变大，非常符合直觉。</p>
</blockquote>
<h4 id="Architecture-Example"><a href="#Architecture-Example" class="headerlink" title="Architecture Example"></a>Architecture Example</h4><p><img src="https://s2.loli.net/2022/04/27/KxdplNeT9gtcOQu.png" alt="image-20220427193805996"></p>
<p>这是一种预测结构的例子。</p>
<blockquote>
<p>为啥把 Coarse 接回来 Refine 就能 work 了？肯定是因为又有了原图的信息吧？如果改用 ResNet 会不会可能更好？有没有可能搞个大模型学习到原图的 Dense Representation？（也就是说那种 Encoder 的感觉）</p>
</blockquote>
<h4 id="Depth-Datasets"><a href="#Depth-Datasets" class="headerlink" title="Depth Datasets"></a>Depth Datasets</h4><p>这个任务常用的一些数据集。</p>
<p><img src="https://s2.loli.net/2022/04/27/PrNcZdkaUJWmqsM.png" alt="image-20220427194154609"></p>
<h4 id="Scale-and-Shift-invariant-Depth-Prediction"><a href="#Scale-and-Shift-invariant-Depth-Prediction" class="headerlink" title="Scale and Shift-invariant Depth Prediction"></a>Scale and Shift-invariant Depth Prediction</h4><p>直接去翻了 1907 那篇 Paper: Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer.</p>
<p>We identify three major challenges. </p>
<p>1) Inherently different representations of depth: direct vs. inverse depth representations.<br>2) Scale ambiguity: for some data sources, depth is only given up to an unknown scale.<br>3) Shift ambiguity: some datasets provide disparity only up to an unknown scale and global disparity shift that is a function of the unknown baseline and a horizontal shift of the principal points due to post-processing.</p>
<p>这里的 disparity 可以理解成图像中“相对深度”的感觉。</p>
<p>假设我们的图片有 $M$ 个像素，$d, d^\star \in \mathbb R^m$ 是 GT / Predicted disparity。 </p>
<p>然后我们定义两个算子：$s: \mathbb R^M \rightarrow \mathbb R_+, t: \mathbb R^M \rightarrow \mathbb R$ 是 scale 和 translation 的 estimator.</p>
<p>接下来是如何解决 disparity 的 scale 和 translation 不统一的问题，论文提供两种方法。</p>
<p>一种是最小二乘拟合 $d_i^\star$ 和 $d_i$ 的关系（$d^\star_i = s d_i + t$），找到 (s, t) 后做映射 $\hat{\mathbf{d}}=s \mathbf{d}+t, \quad \hat{\mathbf{d}}^{\star}=\mathbf{d}^{\star}$。</p>
<p>另一种是做 normalize：$t(\mathbf{d})=\operatorname{median}(\mathbf{d}), \quad s(\mathbf{d})=\frac{1}{M} \sum_{i=1}^{M}|\mathbf{d}-t(\mathbf{d})|$，进而 $\hat{\mathbf{d}}=\frac{\mathbf{d}-t(\mathbf{d})}{s(\mathbf{d})}, \quad \hat{\mathbf{d}}^{\star}=\frac{\mathbf{d}^{\star}-t\left(\mathbf{d}^{\star}\right)}{s\left(\mathbf{d}^{\star}\right)}$.</p>
<p>然后定义 scale- and shift- invariant loss：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{s s i}\left(\hat{\mathbf{d}}, \hat{\mathbf{d}}^{*}\right)=\frac{1}{2 M} \sum_{i=1}^{M} \rho\left(\hat{\mathbf{d}}_{i}-\hat{\mathbf{d}}_{i}^{*}\right)</script><p>这里 $\rho$ 可以决定这个 Loss Function 的种类：第一种就用 $\rho(x) = x^2$，第二种用 $\rho(x) = |x|.$</p>
<h2 id="Lecture-6-Single-view-3D-Objects-amp-Scenes"><a href="#Lecture-6-Single-view-3D-Objects-amp-Scenes" class="headerlink" title="Lecture 6: Single-view 3D: Objects &amp; Scenes"></a>Lecture 6: Single-view 3D: Objects &amp; Scenes</h2><p>我们的任务是：给定一个 2D 图片，这个图片上仅有一个物体，输出其 3D 的结构。</p>
<p>首先我们要解决的问题是：该怎么建坐标系。</p>
<p>可能的方法：</p>
<p><img src="https://s2.loli.net/2022/04/27/sDhF5P6dywWoACL.png" alt="image-20220427204817186"></p>
<ul>
<li>Camera Coordinate System：以相机为中心，相机轴为坐标轴</li>
<li>View-aligned Object-centric Coordinate System: 以物体为中心，坐标轴与相机轴对齐</li>
<li>Object-centric ‘Canonical’ Coordinate System: 物体中心，但是物体的“前面”是 -Z 方向，上方是 Y 方向</li>
</ul>
<p>Whenever reading a 3D prediction work, understand the prediction frame!</p>
<p>第三种坐标系让机器学习起来更容易（Aligned shapes have less variation）</p>
<p><img src="https://s2.loli.net/2022/04/27/mK4XowydG8Z6Ijh.png" alt="image-20220427210056753"></p>
<p>所以我们的任务变成：Given a 2D picture of a single object, how can we reconstruct the object in 3D in canonical frame?</p>
<h3 id="Learning-to-Predict"><a href="#Learning-to-Predict" class="headerlink" title="Learning to Predict"></a>Learning to Predict</h3><p><img src="https://s2.loli.net/2022/04/27/tgYxnEyJ13lQIFr.png" alt="image-20220427210133626"></p>
<p>我们可以使用合成的数据（比如上图 ShapeNet 数据集）来学习，但这样的数据既不真实，还会有偏向刚体和人造物体的 Bias。</p>
<h4 id="Predict-Volumetric-3D"><a href="#Predict-Volumetric-3D" class="headerlink" title="Predict Volumetric 3D"></a>Predict Volumetric 3D</h4><p><img src="https://s2.loli.net/2022/04/27/oXYvfMA589FPd4t.png" alt="image-20220427210419988"></p>
<p>Why not use pixel-aligned prediction? 是不是因为要求输出有连续性？而且要求有 invariant to observed viewpoint 的要求？Pixel-aligned 是否会忽略不同组件间的连接关系？</p>
<p><strong>Loss Function</strong>  这个 Loss 该怎么计算？</p>
<script type="math/tex; mode=display">
-\dfrac 1 N \sum_{n=1}^N[p_n\log \hat p _n + (1-p_n) \log(1-\hat p _n)]</script><p>这个就相当于把三维的场景 Flattern，然后做 cross entropy。</p>
<blockquote>
<p>这样是否抹杀掉了一些“连接关系”对整体的影响？因为这个办法相当于是每个体素的权重是相同的。比如说在关注点高的地方缺了俩体素，和在一个本来就比较混乱的地方有两个体素的 difference 相比…</p>
</blockquote>
<p><strong>Training</strong>  Renderings of CAD models on <strong>random backgrounds</strong> (which helps to make up for the domain-shift between real images and CAD model images)</p>
<blockquote>
<p>这玩意是不是能在 Minecraft 里复现出来啊，xswl</p>
<p>写个插件，给它看一张二维图片，后面挂一个模型，然后直接三维重建，草草草</p>
<p>我记得现在只有二维复现二维的插件，颜色选择也只是逼近，绝对没有三维重建的，这不得卷爆插件区啊 :clown_face:</p>
<p>有空可以看下这个，看一下实现，比较好奇什么是 Context-awared，然后 fused volume 后面的 refine 经历了什么:</p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Xie_Pix2Vox_Context-Aware_3D_Reconstruction_From_Single_and_Multi-View_Images_ICCV_2019_paper.pdf">https://openaccess.thecvf.com/content_ICCV_2019/papers/Xie_Pix2Vox_Context-Aware_3D_Reconstruction_From_Single_and_Multi-View_Images_ICCV_2019_paper.pdf</a></p>
</blockquote>
<p>然后还可以 refine 一下表面，类似于 Octree 的思想。</p>
<p>逐层细分，对一个大体素 predict 3 个 label 其中之一：是 all empty，all occupied, 还是 mixed. 如果是 mixed，那么把这体素分成八份，每一份再递归 predict。 </p>
<blockquote>
<p>Paper 贴出来，回头细看：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1704.00710.pdf">https://arxiv.org/pdf/1704.00710.pdf</a></p>
</blockquote>
<h4 id="Predict-Implicit-Volumetric-Representations"><a href="#Predict-Implicit-Volumetric-Representations" class="headerlink" title="Predict Implicit Volumetric Representations"></a>Predict Implicit Volumetric Representations</h4><p>Instead of decoding a volumetric representation, answer occupancy query of an arbitrary point.</p>
<blockquote>
<p>也是类似于上面的 Encoder-Decoder 架构，不过上面 Decoder 在回答二分类（occupied or not）/三分类（empty, occupied or mixed），这里的 Decoder 在 answer occupancy query of an arbitrary point.</p>
<p>这样是不是要对空间 sample 足够多个点啊？看到下一页 PPT 确实，resolution 和 sample 的点数有关系。</p>
</blockquote>
<h4 id="Predict-Point-Clouds"><a href="#Predict-Point-Clouds" class="headerlink" title="Predict Point Clouds"></a>Predict Point Clouds</h4><p><img src="https://s2.loli.net/2022/04/27/jL2ksEySYFoGmZ8.png" alt="image-20220427213755786"></p>
<p><strong>We need permutation invariant learning objectives!</strong></p>
<p>定义 Loss 的方法：</p>
<ul>
<li>Chamfer Distance</li>
</ul>
<p><img src="https://s2.loli.net/2022/04/27/hRsmx6idVayJ3vl.png" alt="image-20220427213934586"></p>
<ul>
<li>Earth Mover’s Distance</li>
</ul>
<p><img src="https://s2.loli.net/2022/04/27/1fw5UtHpYLB9nVP.png" alt="image-20220427214144419"></p>
<blockquote>
<p>怎么变图论问题了啊，离散数学我没学会啊x</p>
</blockquote>
<h5 id="Predict-Parametric-Point-Clouds"><a href="#Predict-Parametric-Point-Clouds" class="headerlink" title="Predict Parametric Point Clouds"></a>Predict Parametric Point Clouds</h5><p><img src="https://s2.loli.net/2022/04/27/HF4wsGNn7felgc9.png" alt="image-20220427214514627"></p>
<h4 id="Predict-Meshes"><a href="#Predict-Meshes" class="headerlink" title="Predict Meshes"></a>Predict Meshes</h4><p><img src="https://s2.loli.net/2022/04/27/SRziLandU2YmCw3.png" alt="image-20220427214613371"></p>
<h5 id="Training-Objective"><a href="#Training-Objective" class="headerlink" title="Training Objective"></a>Training Objective</h5><p><img src="https://s2.loli.net/2022/04/27/4upMcDVQvmnkFYB.png" alt="image-20220427220947376"></p>
<h5 id="Regularization-Smoothness"><a href="#Regularization-Smoothness" class="headerlink" title="Regularization: Smoothness"></a>Regularization: Smoothness</h5><p>A vertex should get near to average of its neighbors, and that is: $\delta_{i}=\frac{1}{d_{i}} \sum_{j \in N(i)}\left(\mathbf{v}_{i}-\mathbf{v}_{j}\right)$.</p>
<p>于是我们把 $\sum |\delta_i|$ 作为正则项就好了。这也可以表述为 $||LV||^2$：</p>
<p><img src="https://s2.loli.net/2022/04/27/VdopXzWbZgTnj1U.png" alt="image-20220427221616389"></p>
<h5 id="Learning-to-Deform-Meshes"><a href="#Learning-to-Deform-Meshes" class="headerlink" title="Learning to Deform Meshes"></a>Learning to Deform Meshes</h5><p><img src="https://s2.loli.net/2022/04/27/MITv5VwAkCarp6g.png" alt="image-20220427221756418"></p>
<h3 id="Evaluation-Methods"><a href="#Evaluation-Methods" class="headerlink" title="Evaluation Methods"></a>Evaluation Methods</h3><ul>
<li>IoU: $\dfrac {\text{Intersection}}{\text{Union}}$</li>
<li>Chamfer Distance</li>
<li>F1-score @ t<ul>
<li>Precision @ t: fraction of predicted points within $t$ of some GT points</li>
<li>Recall @ t: fraction of GT points within t of some predicted point</li>
<li>F1 @ t = 2 $\times$ (Precision @ t) $\times$ (Recall @ t) / (Precision @ t + Recall @ t).</li>
</ul>
</li>
</ul>
<blockquote>
<p>暂时先这样吧 有时间可能去看看另一个课 或者装一下 Pytorch3D 试着做一下 Assignments 2022-4-27 22:24:51</p>
</blockquote>
<h3 id="Single-view-3D-Scenes"><a href="#Single-view-3D-Scenes" class="headerlink" title="Single-view 3D: Scenes"></a>Single-view 3D: Scenes</h3><p>Decouple prediction of objects, locations and layout.</p>
<ul>
<li>Objects: Last section…</li>
<li>Locations</li>
<li>Layout: what would the scene look like if there were no objects?</li>
</ul>
<p>Putting it together: Factored 3D Reconstruction</p>
<blockquote>
<p> 然后有各式各样的论文…感觉这一块儿没有研究范式</p>
</blockquote>
<h2 id="3D-Prediction-without-3D-supervision"><a href="#3D-Prediction-without-3D-supervision" class="headerlink" title="3D Prediction without 3D supervision"></a>3D Prediction without 3D supervision</h2><h3 id="Reasoning-about-Relationships"><a href="#Reasoning-about-Relationships" class="headerlink" title="Reasoning about Relationships"></a>Reasoning about Relationships</h3><p><img src="C:/Users/c7w/AppData/Roaming/Typora/typora-user-images/image-20220428193055357.png" alt="image-20220428193055357"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">c7w</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://www.c7w.tech/16-889-3d-vision/">https://www.c7w.tech/16-889-3d-vision/</a></span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E4%B8%89%E7%BB%B4%E8%A7%86%E8%A7%89/">三维视觉</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/dl-for-computer-vision/"><img class="prev-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Deep Learning for Computer Vision</div></div></a></div><div class="next-post pull-right"><a href="/photon-mapping/"><img class="next-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">光子映射与 SPPM</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s2.loli.net/2022/01/03/i8fNgXEPZDqnIlS.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">c7w</div><div class="author-info__description">Forever a c7w.</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">62</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">44</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">35</div></a></div></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/c7w" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:cc7w@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://twitter.com/c7wc7w" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3D-Representations"><span class="toc-number">2.</span> <span class="toc-text">3D Representations</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Surface-Representations"><span class="toc-number">2.1.</span> <span class="toc-text">Surface Representations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Volume-Representations"><span class="toc-number">2.2.</span> <span class="toc-text">Volume Representations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Other-Representations"><span class="toc-number">2.3.</span> <span class="toc-text">Other Representations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9B%B4%E5%A4%9A%E8%AE%A8%E8%AE%BA"><span class="toc-number">2.4.</span> <span class="toc-text">更多讨论</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Conversions-between-3D-Representations"><span class="toc-number">3.</span> <span class="toc-text">Conversions between 3D Representations</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%95%E8%B0%93%E2%80%9C%E5%8F%AF%E5%BE%AE%E5%8F%98%E6%8D%A2%E2%80%9D"><span class="toc-number">3.1.</span> <span class="toc-text">何谓“可微变换”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Volume-Representations-%E4%B9%8B%E9%97%B4%E7%9A%84%E8%BD%AC%E6%8D%A2"><span class="toc-number">3.2.</span> <span class="toc-text">Volume Representations 之间的转换</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Approximating-Continuous-from-Discrete"><span class="toc-number">3.2.1.</span> <span class="toc-text">Approximating Continuous from Discrete</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#From-Continuous-to-Discrete"><span class="toc-number">3.2.2.</span> <span class="toc-text">From Continuous to Discrete</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Surface-Representations-%E4%B9%8B%E9%97%B4%E7%9A%84%E8%BD%AC%E6%8D%A2"><span class="toc-number">3.3.</span> <span class="toc-text">Surface Representations 之间的转换</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#From-Continuous-to-Discrete-1"><span class="toc-number">3.3.1.</span> <span class="toc-text">From Continuous to Discrete</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Sampling-Points-on-a-Triangle-Mesh"><span class="toc-number">3.3.2.</span> <span class="toc-text">Sampling Points on a Triangle Mesh</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Volume-Representation-%E5%92%8C-Surface-Representation-%E4%B9%8B%E9%97%B4%E7%9A%84%E8%BD%AC%E6%8D%A2"><span class="toc-number">3.4.</span> <span class="toc-text">Volume Representation 和 Surface Representation 之间的转换</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Volumes-to-Surface-Marching-Cubes"><span class="toc-number">3.4.1.</span> <span class="toc-text">Volumes to Surface: Marching Cubes</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Points-to-Meshes-via-Implicit-Functions"><span class="toc-number">3.4.2.</span> <span class="toc-text">Points to Meshes via Implicit Functions</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Image-Formation"><span class="toc-number">4.</span> <span class="toc-text">Image Formation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Geometry"><span class="toc-number">4.1.</span> <span class="toc-text">Geometry</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Physics"><span class="toc-number">4.2.</span> <span class="toc-text">Physics</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Single-view-3D-History-2-5D"><span class="toc-number">5.</span> <span class="toc-text">Single-view 3D: History + 2.5D</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3D-Inference-%E5%8F%91%E5%B1%95%E7%9A%84%E5%8E%86%E5%8F%B2"><span class="toc-number">5.1.</span> <span class="toc-text">3D Inference 发展的历史</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Depth-Prediction%EF%BC%9A%E5%9C%BA%E6%99%AF%E6%B7%B1%E5%BA%A6%E9%A2%84%E6%B5%8B%E4%BB%BB%E5%8A%A1"><span class="toc-number">5.2.</span> <span class="toc-text">Depth Prediction：场景深度预测任务</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Loss-Function"><span class="toc-number">5.2.1.</span> <span class="toc-text">Loss Function</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Architecture-Example"><span class="toc-number">5.2.2.</span> <span class="toc-text">Architecture Example</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Depth-Datasets"><span class="toc-number">5.2.3.</span> <span class="toc-text">Depth Datasets</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Scale-and-Shift-invariant-Depth-Prediction"><span class="toc-number">5.2.4.</span> <span class="toc-text">Scale and Shift-invariant Depth Prediction</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Lecture-6-Single-view-3D-Objects-amp-Scenes"><span class="toc-number">6.</span> <span class="toc-text">Lecture 6: Single-view 3D: Objects &amp; Scenes</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Learning-to-Predict"><span class="toc-number">6.1.</span> <span class="toc-text">Learning to Predict</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Predict-Volumetric-3D"><span class="toc-number">6.1.1.</span> <span class="toc-text">Predict Volumetric 3D</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Predict-Implicit-Volumetric-Representations"><span class="toc-number">6.1.2.</span> <span class="toc-text">Predict Implicit Volumetric Representations</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Predict-Point-Clouds"><span class="toc-number">6.1.3.</span> <span class="toc-text">Predict Point Clouds</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Predict-Parametric-Point-Clouds"><span class="toc-number">6.1.3.1.</span> <span class="toc-text">Predict Parametric Point Clouds</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Predict-Meshes"><span class="toc-number">6.1.4.</span> <span class="toc-text">Predict Meshes</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Training-Objective"><span class="toc-number">6.1.4.1.</span> <span class="toc-text">Training Objective</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Regularization-Smoothness"><span class="toc-number">6.1.4.2.</span> <span class="toc-text">Regularization: Smoothness</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Learning-to-Deform-Meshes"><span class="toc-number">6.1.4.3.</span> <span class="toc-text">Learning to Deform Meshes</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Evaluation-Methods"><span class="toc-number">6.2.</span> <span class="toc-text">Evaluation Methods</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Single-view-3D-Scenes"><span class="toc-number">6.3.</span> <span class="toc-text">Single-view 3D: Scenes</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3D-Prediction-without-3D-supervision"><span class="toc-number">7.</span> <span class="toc-text">3D Prediction without 3D supervision</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Reasoning-about-Relationships"><span class="toc-number">7.1.</span> <span class="toc-text">Reasoning about Relationships</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="framework-info"><span>Powered by Hexo &amp; Theme Butterfly &amp; GitHub Pages</span></div><div class="copyleft"><span>Copyright © 2020-2022 c7w. LICENSE CC BY-NC-SA 4.0.</span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      false && mermaid.init()
    })
  }
}</script></div><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>