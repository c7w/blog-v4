<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>c7w 的博客</title>
  
  <subtitle>是一只 c7w 的博客</subtitle>
  <link href="https://www.c7w.tech/atom.xml" rel="self"/>
  
  <link href="https://www.c7w.tech/"/>
  <updated>2022-01-26T09:33:26.102Z</updated>
  <id>https://www.c7w.tech/</id>
  
  <author>
    <name>c7w</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>《动手学深度学习》 Pytorch ver. 阅读摘录 Part B</title>
    <link href="https://www.c7w.tech/dive-into-dl-pytorch-B/"/>
    <id>https://www.c7w.tech/dive-into-dl-pytorch-B/</id>
    <published>2022-01-26T04:01:54.000Z</published>
    <updated>2022-01-26T09:33:26.102Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/cover.png" alt=""></p><ul><li>《动手学深度学习》原书地址：<a href="https://github.com/d2l-ai/d2l-zh">https://github.com/d2l-ai/d2l-zh</a></li><li>《动手学深度学习》(Pytorch ver.)：<a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/">https://tangshusen.me/Dive-into-DL-PyTorch/#/</a></li></ul><p>知识架构：</p><p><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/book-org.svg" alt="封面"></p><p>本文的主要作用是在阅读过程中做一些摘录。对于「机器学习」领域， c7w 虽然曾尝试从各个领域入门，也尝试训过一些模型，但是还是缺少系统性、结构性的学习。希望阅读本书能带来更多的收获吧。</p><p>与前面的一些笔记相比，本文更加侧重于「实践」。也就是说切实地提升自己的代码能力。</p><p>Part B 包含：</p><ul><li>§ 5 CNN</li><li>§ 6 RNN</li><li>§ 7 优化算法</li><li>§ 8 计算性能</li></ul><a id="more"></a><h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><h4 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h4><ul><li><strong>二维互相关运算</strong></li></ul><p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.1_correlation.svg" alt="img"></p><p>如图所示，输入是一个高和宽均为3的二维数组。我们将该数组的形状记为 $3 \times 3$ 或$（3，3）$。</p><p>核数组的高和宽分别为 2。该数组在卷积计算中又称卷积核或过滤器（filter）。</p><p>卷积核窗口（又称卷积窗口）的形状取决于卷积核的高和宽，即 $2 \times 2$。</p><p>图中的阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：</p><p>$0\times0+1\times1+3\times2+4\times3=19$​​​​。</p><p>在二维互相关运算中，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，按照特定的步长，依次在输入数组上滑动。</p><p>当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素。</p><ul><li><strong>从互相关运算到卷积运算</strong></li></ul><p>实际上，卷积运算与互相关运算类似。<strong>为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算</strong>。</p><ul><li><strong>Feature Map 与 Receptive Field</strong></li></ul><p>二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。</p><p>影响元素 $x$ 的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做的 $x$ 感受野（receptive field）。</p><p>以图为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。</p><p>我们将图中形状为 $2 \times 2$​​​ 的输出记为 $Y$​​，并考虑一个更深的卷积神经网络：将 $Y$ 与另一个形状为 $2 \times 2$ 的核数组做互相关运算，输出单个元素 $z$。那么，$z$ 在 $Y$ 上的 Receptive Field 为 $Y$ 的全部四个元素，在 $x$ 上的感受野包括其中全部 9 个元素。</p><p>可见，我们可以<strong>通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征</strong>。</p><h4 id="Padding-amp-Stride"><a href="#Padding-amp-Stride" class="headerlink" title="Padding &amp; Stride"></a>Padding &amp; Stride</h4><p>本节我们将介绍卷积层的两个超参数，即填充和步幅。它们可以对给定形状的输入和卷积核改变输出形状。</p><ul><li><strong>Padding</strong></li></ul><p>填充（padding）是指在输入高和宽的两侧填充元素（通常是 0 元素）。</p><p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.2_conv_pad.svg" alt="img"></p><p>图中我们在原输入高和宽的两侧分别添加了值为 0 的元素，使得输入高和宽从 3 变成了 5 ，并导致输出高和宽由 2 增加到 4。</p><p>一般来说，如果在高的两侧一共填充 $p_h$ 行，在宽的两侧一共填充 $p_w$ 列，在很多情况下，我们会设置 $p_h=k_h-1$ 和 $p_w=k_w-1$ 来使输入和输出具有相同的高和宽，其中 $k_h\times k_w$ 是卷积核窗口形状。这样会方便在构造网络时推测每个层的输出形状。</p><p>假设这里 $k_h$ 是奇数，我们会在高的两侧分别填充 $p_h/2$ 行。如果 $k_h$ 是偶数，一种可能是在输入的顶端一侧填充 $\lceil p_h/2\rceil$ 行，而在底端一侧填充 $\lfloor p_h/2\rfloor$​ 行。在宽的两侧填充同理。卷积神经网络经常使用<strong>奇数高宽的卷积核</strong> $k_h \times k_w$，如 1、3、5 和 7，所以两端上的填充个数相等。</p><p>对任意的二维数组 <code>X</code>，设它的第 <code>i</code> 行第 <code>j</code> 列的元素为 <code>X[i,j]</code>。当两端上的填充个数相等，并使输入和输出具有相同的高和宽时，我们就知道输出 <code>Y[i,j]</code> 是由输入以 <code>X[i,j]</code> 为中心的窗口同卷积核进行互相关计算得到的。</p><ul><li><strong>Stride</strong></li></ul><p>在上一节里我们介绍了二维互相关运算。卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。我们将每次滑动的行数和列数称为步幅（stride）。</p><p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.2_conv_stride.svg" alt="img"></p><p>目前我们看到的例子里，在高和宽两个方向上步幅均为1。我们也可以使用更大步幅。</p><p>图中展示了在高上步幅为 3、在宽上步幅为 2 的二维互相关运算。可以看到，输出第一列第二个元素时，卷积窗口向下滑动了 3 行，而在输出第一行第二个元素时卷积窗口向右滑动了 2 列。当卷积窗口在输入上再向右滑动 2 列时，由于输入元素无法填满窗口，无结果输出。</p><h4 id="多通道"><a href="#多通道" class="headerlink" title="多通道"></a>多通道</h4><p>前面两节里我们用到的输入和输出都是二维数组，但真实数据的维度经常更高。</p><p>例如，彩色图像在高和宽 2 个维度外还有 RGB（红、绿、蓝）3 个颜色通道。</p><p>假设彩色图像的高和宽分别是 $h$ 和 $w$（像素），那么它可以表示为一个 $3\times h\times w$ 的多维数组。</p><p>我们将大小为 3 的这一维称为通道（channel）维。</p><p>本节我们将介绍含多个输入通道或多个输出通道的卷积核。</p><ul><li><strong>多输入通道</strong></li></ul><p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.3_conv_multi_in.svg" alt="img"></p><p>当输入数据含多个通道时，我们需要构造一个输入通道数与输入数据的通道数相同的卷积核，从而能够与含多通道的输入数据做互相关运算。</p><p>含多个通道的输入数据与多输入通道的卷积核做二维互相关运算的输出：在各个通道上对输入的二维数组和卷积核的二维核数组做互相关运算，再将这些互相关运算的输出相加。</p><ul><li><strong>多输出通道</strong></li></ul><p>当输入通道有多个时，因为我们对各个通道的结果做了累加，所以不论输入通道数是多少，输出通道数总是为 1。设卷积核输入通道数和输出通道数分别为 $c_i$ 和 $c_o$，高和宽分别为 $k_h$ 和 $k_w$。</p><p>如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为 $c_i\times k_h\times k_w$ 的核数组。将它们在输出通道维上连结，卷积核的形状即 $c_o\times c_i\times k_h\times k_w$。</p><ul><li><strong>1 x 1 卷积层</strong></li></ul><p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.3_conv_1x1.svg" alt="img"></p><p>因为使用了最小窗口，$1\times 1$ 卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。实际上，$1\times 1$ 卷积的主要计算发生在通道维上。</p><p>值得注意的是，输入和输出具有相同的高和宽。输出中的每个元素来自输入中在高和宽上相同位置的元素<strong>在不同通道之间的按权重累加</strong>。</p><p>假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，<strong>那么 $1\times 1$​ 卷积层的作用与全连接层等价</strong>。</p><h4 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h4><p>设任意二维数组 <code>X</code> 的 <code>i</code>行 <code>j</code> 列的元素为 <code>X[i, j]</code>。如果我们构造的 $1 \times 2$卷积核 $[1, -1]$ 输出 <code>Y[i, j]=1</code>，那么说明输入中 <code>X[i, j]</code> 和 <code>X[i, j+1]</code> 数值不一样。这可能意味着物体边缘通过这两个元素之间。</p><p>实际图像里，我们感兴趣的物体不会总出现在固定位置：即使我们连续拍摄同一个物体也极有可能出现像素位置上的偏移。这会导致同一个边缘对应的输出可能出现在卷积输出 <code>Y</code> 中的不同位置，进而对后面的模式识别造成不便。</p><p>在本节中我们介绍池化（pooling）层，它的提出是<strong>为了缓解卷积层对位置的过度敏感性</strong>。</p><ul><li><strong>2D-MaxPooling &amp; Mean Pooling</strong></li></ul><p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.4_pooling.svg" alt="img"></p><p>同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出。</p><p>不同于卷积层里计算输入和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也分别叫做最大池化或平均池化。</p><p>让我们再次回到本节开始提到的物体边缘检测的例子。现在我们将卷积层的输出作为 $2\times 2$ 最大池化的输入。设该卷积层输入是 <code>X</code>、池化层输出为 <code>Y</code>。无论是 <code>X[i, j]</code> 和 <code>X[i, j+1]</code> 值不同，还是 <code>X[i, j+1]</code> 和 <code>X[i, j+2]</code> 不同，池化层输出均有 <code>Y[i, j]=1</code>。也就是说，使用 $2\times 2$ 最大池化层时，只要卷积层识别的模式在高和宽上移动不超过一个元素，我们依然可以将它检测出来。</p><ul><li><strong>Padding &amp; Stride</strong></li></ul><p>同卷积层一样，池化层也可以在输入的高和宽两侧的填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。我们将通过 <code>nn</code> 模块里的二维最大池化层 <code>MaxPool2d</code> 来演示池化层填充和步幅的工作机制。</p><pre class="language-python" data-language="python"><code class="language-python">X <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># C_o * C_i * K_h * K_w</span><span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span></code></pre><p>输出：<br><pre class="language-none"><code class="language-none">tensor([[[[ 0.,  1.,  2.,  3.],          [ 4.,  5.,  6.,  7.],          [ 8.,  9., 10., 11.],          [12., 13., 14., 15.]]]])</code></pre></p><p>默认情况下，<code>MaxPool2d</code> 实例里步幅和池化窗口形状相同。下面使用形状为 $(3, 3)$ 的池化窗口，默认获得形状为 $(3, 3)$ 的步幅。</p><pre class="language-python" data-language="python"><code class="language-python">pool2d <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>pool2d<span class="token punctuation">(</span>X<span class="token punctuation">)</span> </code></pre><p>输出：<br><pre class="language-none"><code class="language-none">tensor([[[[10.]]]])</code></pre></p><p>我们可以手动指定步幅和填充。</p><pre class="language-python" data-language="python"><code class="language-python">pool2d <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>pool2d<span class="token punctuation">(</span>X<span class="token punctuation">)</span></code></pre><p>输出：<br><pre class="language-none"><code class="language-none">tensor([[[[ 5.,  7.],          [13., 15.]]]])</code></pre></p><p>当然，我们也可以指定非正方形的池化窗口，并分别指定高和宽上的填充和步幅。</p><pre class="language-python" data-language="python"><code class="language-python">pool2d <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>pool2d<span class="token punctuation">(</span>X<span class="token punctuation">)</span></code></pre><p>输出：<br><pre class="language-none"><code class="language-none">tensor([[[[ 1.,  3.],          [ 9., 11.],          [13., 15.]]]])</code></pre></p><ul><li><strong>多通道</strong></li></ul><p>在处理多通道输入数据时，<strong>池化层对每个输入通道分别池化，而不是像卷积层那样将各通道的输入按通道相加</strong>。</p><p>这意味着池化层的输出通道数与输入通道数相等。下面将数组 <code>X</code> 和 <code>X+1</code> 在通道维上连结来构造通道数为 2 的输入。</p><pre class="language-python" data-language="python"><code class="language-python">X <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> X <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>X</code></pre><p>输出：<br><pre class="language-none"><code class="language-none">tensor([[[[ 0.,  1.,  2.,  3.],          [ 4.,  5.,  6.,  7.],          [ 8.,  9., 10., 11.],          [12., 13., 14., 15.]],         [[ 1.,  2.,  3.,  4.],          [ 5.,  6.,  7.,  8.],          [ 9., 10., 11., 12.],          [13., 14., 15., 16.]]]])</code></pre></p><p>池化后，我们发现输出通道数仍然是2。</p><pre class="language-python" data-language="python"><code class="language-python">pool2d <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>pool2d<span class="token punctuation">(</span>X<span class="token punctuation">)</span></code></pre><p>输出：<br><pre class="language-none"><code class="language-none">tensor([[[[ 5.,  7.],          [13., 15.]],         [[ 6.,  8.],          [14., 16.]]]])</code></pre></p><h4 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h4><p>本节我们介绍批量归一化（batch normalization）层，它能让较深的神经网络的训练变得更加容易。</p><ul><li>为什么要有 Batch Normalization?</li></ul><p>在预测回归问题里，我们对输入数据做了标准化处理：处理后的任意一个特征在数据集中所有样本上的均值为 0、标准差为 1。标准化处理输入数据使各个特征的分布相近：这往往更容易训练出有效的模型。</p><p>通常来说，数据标准化预处理对于浅层模型就足够有效了。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。但对深层神经网络来说，即使输入数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。</p><p>批量归一化的提出正是为了应对深度模型训练的挑战。在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。<strong>批量归一化和下一节将要介绍的残差网络为训练和设计深度模型提供了两类重要思路</strong>。</p><ul><li>怎么做 Batch Normalization?</li></ul><p>对全连接层和卷积层做批量归一化的方法稍有不同。下面我们将分别介绍这两种情况下的批量归一化。</p><p><strong>对 Fully Connected Layer 的 Batch Normalization</strong></p><p>我们先考虑如何对全连接层做批量归一化。通常，我们将批量归一化层置于全连接层中的仿射变换和激活函数之间。设全连接层的输入为 $\boldsymbol{u}$​​，权重参数和偏差参数分别为 $\boldsymbol{W}$​ ​和 $\boldsymbol{b}$​​，激活函数为 $\phi$​​。设批量归一化的运算符为 $\text{BN}$​​。那么，使用批量归一化的全连接层的输出为 $\phi(\text{BN}(\boldsymbol{Wu+b}))$​。</p><p>下面我们解释 $\text{BN}$​ 算符是什么。</p><p>考虑一个由 $m$​​ 个样本组成的 Mini-batch，仿射变换的输出为一个新的 Mini-batch $\mathcal{B} = {\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(m)} }$​​。它们正是批量归一化层的输入。对于小批量 $\mathcal{B}$​ ​中任意样本 $\boldsymbol{x}^{(i)} \in \mathbb{R}^d, 1 \leq i \leq m$​​，批量归一化层的输出同样是 $d$​ ​维向量$\boldsymbol{y}^{(i)} = \text{BN}(\boldsymbol{x}^{(i)})$​，并由以下几步求得。</p><script type="math/tex; mode=display">\boldsymbol{\mu}_\mathcal{B} \leftarrow \frac{1}{m}\sum_{i = 1}^{m} \boldsymbol{x}^{(i)} \\\boldsymbol{\sigma}_\mathcal{B}^2 \leftarrow \frac{1}{m} \sum_{i=1}^{m}(\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_\mathcal{B})^2 \\ \hat{\boldsymbol{x}}^{(i)} \leftarrow \frac{\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_\mathcal{B}}{\sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon}}</script><p>这里 $\epsilon &gt; 0$ 是一个很小的常数，是为了保证分母大于 0。在上面标准化的基础上，批量归一化层引入了两个可以学习的模型参数，拉伸（scale）参数 $\boldsymbol{\gamma}$ 和偏移（shift）参数 $\boldsymbol{\beta}$。这两个参数和 $\boldsymbol{x}^{(i)}$ 形状相同，皆为 $d$ 维向量。它们与 $\boldsymbol{x}^{(i)}$ 分别做 Hadamard Product（符号$\odot$​）和加法计算：</p><script type="math/tex; mode=display">{\boldsymbol{y}}^{(i)} \leftarrow \boldsymbol{\gamma} \odot \hat{\boldsymbol{x}}^{(i)} + \boldsymbol{\beta}</script><p>至此，我们得到了 $\boldsymbol{x}^{(i)}$​ ​的批量归一化的输出 $\boldsymbol{y}^{(i)}$​​。值得注意的是，可学习的拉伸和偏移参数保留了不对 $\hat{\boldsymbol{x}}^{(i)}$ ​​做批量归一化的可能：此时只需学出 $\boldsymbol{\gamma} = \sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon}$ ​​和 $\boldsymbol{\beta} = \boldsymbol{\mu}_\mathcal{B}$​​。我们可以对此这样理解：如果批量归一化无益，理论上，学出的模型可以不使用批量归一化。</p><p><strong>对 Conv. Layer 的 Batch Normalization</strong></p><p>对卷积层来说，批量归一化发生在卷积计算之后、应用激活函数之前。</p><p>如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且<strong>每个通道都拥有独立的拉伸和偏移参数，并均为标量</strong>。</p><p>设小批量中有 $m$ 个样本，在单个通道上，假设卷积计算输出的高和宽分别为 $p$ 和 $q$。我们需要对该通道中 $m \times p \times q$ 个元素同时做批量归一化。对这些元素做标准化计算时，我们使用相同的均值和方差，即该通道中 $m \times p \times q$​ 个元素的均值和方差。</p><p><strong>预测时的 Batch Normalization</strong></p><p>使用批量归一化训练时，我们可以将批量大小设得大一点，从而使批量内样本的均值和方差的计算都较为准确。将训练好的模型用于预测时，我们希望模型对于任意输入都有确定的输出。因此，<strong>单个样本的输出不应取决于批量归一化所需要的随机小批量中的均值和方差</strong>。一种常用的方法是通过移动平均<strong>估算整个训练数据集的样本均值和方差</strong>，并在预测时使用它们得到确定的输出。可见，和丢弃层一样，批量归一化层在训练模式和预测模式下的计算结果也是不一样的。</p><ul><li>实现（Simple ver.）</li></ul><p>与我们刚刚自己定义的 <code>BatchNorm</code> 类相比，Pytorch 中 <code>nn</code> 模块定义的 <code>BatchNorm1d</code> 和 <code>BatchNorm2d</code> 类使用起来更加简单，二者分别用于全连接层和卷积层，都需要指定输入的 <code>num_features</code> 参数值。下面我们用 PyTorch 实现使用批量归一化的 LeNet。</p><pre class="language-python" data-language="python"><code class="language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># in_channels, out_channels, kernel_size</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># kernel_size, stride</span>                nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                d2l<span class="token punctuation">.</span>FlattenLayer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token operator">*</span><span class="token number">4</span><span class="token operator">*</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span></code></pre><h3 id="CNN-的例子"><a href="#CNN-的例子" class="headerlink" title="CNN 的例子"></a>CNN 的例子</h3><h4 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h4><p>之前我们曾使用 MLP 对 Fashion-MNIST 数据集中的图像进行分类。每张图像高和宽均是 28 像素。我们将图像中的像素逐行展开，得到长度为 784 的向量，并输入进全连接层中。然而，这种分类方法有一定的局限性。</p><ol><li>图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。</li><li>对于大尺寸的输入图像，使用全连接层容易造成模型过大。假设输入是高和宽均为 1000 像素的彩色照片（含 3 个通道）。即使全连接层输出个数仍是 256，该层权重参数的形状是$ 3,000,000\times 256$​：它占用了大约 3 GB 的内存或显存。这带来过复杂的模型和过高的存储开销。</li></ol><p>卷积层尝试解决这两个问题。一方面，卷积层保留输入形状，使图像的像素在高和宽两个方向上的相关性均可能被有效识别；另一方面，卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。</p><p>卷积神经网络就是含卷积层的网络。本节里我们将介绍一个早期用来识别手写数字图像的卷积神经网络：LeNet。</p><p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.5_lenet.png" alt="img"></p><p>LeNet分为卷积层块和全连接层块两个部分。下面我们分别介绍这两个模块。</p><p>卷积层块里的基本单位是<strong>卷积层后接最大池化层</strong>：</p><ul><li>卷积层用来识别图像里的空间模式，如线条和物体局部</li><li>之后的最大池化层则用来降低卷积层对位置的敏感性</li></ul><p>卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用 $5\times 5$​ 的窗口，并在输出上使用 sigmoid 激活函数。第一个卷积层输出通道数为 6，第二个卷积层输出通道数则增加到 16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层的窗口形状均为 $2\times 2$​​，且步幅为 2。由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。</p><p>卷积层块的输出形状为 (批量大小, 通道, 高, 宽)。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含 3 个全连接层。它们的输出个数分别是 120、84 和 10，其中 10 为输出的类别个数。</p><p>下面我们通过 <code>Sequential</code> 类来实现 LeNet 模型。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LeNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>LeNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># in_channels, out_channels, kernel_size</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># kernel_size, stride</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token operator">*</span><span class="token number">4</span><span class="token operator">*</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img<span class="token punctuation">)</span><span class="token punctuation">:</span>        feature <span class="token operator">=</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>img<span class="token punctuation">)</span>        output <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>feature<span class="token punctuation">.</span>view<span class="token punctuation">(</span>img<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> output</code></pre><p>这里使用 GPU 进行计算，对相关函数的修改如下：</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 本函数已保存在d2lzh_pytorch包中方便以后使用</span><span class="token keyword">def</span> <span class="token function">train_ch5</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> device<span class="token punctuation">,</span> num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>    net <span class="token operator">=</span> net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"training on "</span><span class="token punctuation">,</span> device<span class="token punctuation">)</span>    loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>        train_l_sum<span class="token punctuation">,</span> train_acc_sum<span class="token punctuation">,</span> n<span class="token punctuation">,</span> batch_count<span class="token punctuation">,</span> start <span class="token operator">=</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span>            X <span class="token operator">=</span> X<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>            y <span class="token operator">=</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>            y_hat <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span>            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>            l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>            train_l_sum <span class="token operator">+=</span> l<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            train_acc_sum <span class="token operator">+=</span> <span class="token punctuation">(</span>y_hat<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>            n <span class="token operator">+=</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>            batch_count <span class="token operator">+=</span> <span class="token number">1</span>        test_acc <span class="token operator">=</span> evaluate_accuracy<span class="token punctuation">(</span>test_iter<span class="token punctuation">,</span> net<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'</span>              <span class="token operator">%</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> train_l_sum <span class="token operator">/</span> batch_count<span class="token punctuation">,</span> train_acc_sum <span class="token operator">/</span> n<span class="token punctuation">,</span> test_acc<span class="token punctuation">,</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span> start<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 本函数已保存在d2lzh_pytorch包中方便以后使用。该函数将被逐步改进。</span><span class="token keyword">def</span> <span class="token function">evaluate_accuracy</span><span class="token punctuation">(</span>data_iter<span class="token punctuation">,</span> net<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> device <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">and</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 如果没指定device就使用net的device</span>        device <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>device    acc_sum<span class="token punctuation">,</span> n <span class="token operator">=</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0</span>    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> data_iter<span class="token punctuation">:</span>            <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>                net<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 评估模式, 这会关闭 dropout</span>                acc_sum <span class="token operator">+=</span> <span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>                net<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 改回训练模式</span>            <span class="token keyword">else</span><span class="token punctuation">:</span> <span class="token comment"># 自定义的模型, 3.13节之后不会用到, 不考虑GPU</span>                <span class="token keyword">if</span><span class="token punctuation">(</span><span class="token string">'is_training'</span> <span class="token keyword">in</span> net<span class="token punctuation">.</span>__code__<span class="token punctuation">.</span>co_varnames<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 如果有 is_training 这个参数</span>                    <span class="token comment"># 将 is_training 设置成 False</span>                    acc_sum <span class="token operator">+=</span> <span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">,</span> is_training<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>                 <span class="token keyword">else</span><span class="token punctuation">:</span>                    acc_sum <span class="token operator">+=</span> <span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>             n <span class="token operator">+=</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> acc_sum <span class="token operator">/</span> n</code></pre><h4 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h4><p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.6_alexnet.png" alt="img"></p><ul><li>Larger parameter size</li><li>Use ReLU instead of sigmoid</li><li>Introducing Dropout</li><li>Data augmentation</li></ul><h4 id="其他模型"><a href="#其他模型" class="headerlink" title="其他模型"></a>其他模型</h4><ul><li>VGG</li><li>NiN</li><li>GoogLeNet</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://tangshusen.me/Dive-into-DL-PyTorch/img/cover.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;《动手学深度学习》原书地址：&lt;a href=&quot;https://github.com/d2l-ai/d2l-zh&quot;&gt;https://github.com/d2l-ai/d2l-zh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;《动手学深度学习》(Pytorch ver.)：&lt;a href=&quot;https://tangshusen.me/Dive-into-DL-PyTorch/#/&quot;&gt;https://tangshusen.me/Dive-into-DL-PyTorch/#/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;知识架构：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tangshusen.me/Dive-into-DL-PyTorch/img/book-org.svg&quot; alt=&quot;封面&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文的主要作用是在阅读过程中做一些摘录。对于「机器学习」领域， c7w 虽然曾尝试从各个领域入门，也尝试训过一些模型，但是还是缺少系统性、结构性的学习。希望阅读本书能带来更多的收获吧。&lt;/p&gt;
&lt;p&gt;与前面的一些笔记相比，本文更加侧重于「实践」。也就是说切实地提升自己的代码能力。&lt;/p&gt;
&lt;p&gt;Part B 包含：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;§ 5 CNN&lt;/li&gt;
&lt;li&gt;§ 6 RNN&lt;/li&gt;
&lt;li&gt;§ 7 优化算法&lt;/li&gt;
&lt;li&gt;§ 8 计算性能&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="理论" scheme="https://www.c7w.tech/categories/%E7%90%86%E8%AE%BA/"/>
    
    <category term="理论/机器学习" scheme="https://www.c7w.tech/categories/%E7%90%86%E8%AE%BA/%E7%90%86%E8%AE%BA-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://www.c7w.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>《动手学深度学习》 Pytorch ver. 阅读摘录 Part A</title>
    <link href="https://www.c7w.tech/dive-into-dl-pytorch-A/"/>
    <id>https://www.c7w.tech/dive-into-dl-pytorch-A/</id>
    <published>2022-01-24T09:22:22.000Z</published>
    <updated>2022-01-26T04:01:23.653Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/cover.png" alt=""></p><ul><li>《动手学深度学习》原书地址：<a href="https://github.com/d2l-ai/d2l-zh">https://github.com/d2l-ai/d2l-zh</a></li><li>《动手学深度学习》(Pytorch ver.)：<a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/">https://tangshusen.me/Dive-into-DL-PyTorch/#/</a></li></ul><p>知识架构：</p><p><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/book-org.svg" alt="封面"></p><p>本文的主要作用是在阅读过程中做一些摘录。对于「机器学习」领域， c7w 虽然曾尝试从各个领域入门，也尝试训过一些模型，但是还是缺少系统性、结构性的学习。希望阅读本书能带来更多的收获吧。</p><p>与前面的一些笔记相比，本文更加侧重于「实践」。也就是说切实地提升自己的代码能力。</p><p>Part A 包含：</p><ul><li>§ 1 深度学习简介</li><li>§ 2 预备知识：Pytorch</li><li>§ 3 深度学习基础<ul><li>线性回归，Softmax 回归，多层感知机三类基本模型</li><li>权重衰减和 Dropout 两类应对过拟合的方法</li></ul></li><li>§ 4 深度学习计算<ul><li>构造 Pytorch 模型的方式</li><li>模型参数的访问、初始化与共享</li><li>自定义 Layer</li><li>读取与存储</li><li>GPU 计算</li></ul></li></ul><a id="more"></a><h2 id="深度学习简介"><a href="#深度学习简介" class="headerlink" title="深度学习简介"></a>深度学习简介</h2><ul><li><strong>机器学习与深度学习的关系</strong></li></ul><p><strong>机器学习</strong>研究如何使计算机系统利用经验改善性能。它是人工智能领域的分支，也是实现人工智能的一种手段。</p><p>在机器学习的众多研究方向中，<strong>表征学习关注如何自动找出表示数据的合适方式</strong>，以便更好地将输入变换为正确的输出。</p><p>而本书要重点探讨的<strong>深度学习是具有多级表示的表征学习方法</strong>。</p><p>在每一级（从原始数据开始），深度学习通过简单的函数将该级的表示变换为更高级的表示。因此，深度学习模型也可以看作是由许多简单函数复合而成的函数。当这些复合的函数足够多时，深度学习模型就可以表达非常复杂的变换。</p><ul><li><strong>深度学习的一个外在特点：End-to-end</strong></li></ul><p>深度学习的一个外在特点是<strong>端到端的训练</strong>。也就是说，并不是将单独调试的部分拼凑起来组成一个系统，而是将整个系统组建好之后一起训练。</p><p>比如说，计算机视觉科学家之前曾一度将特征抽取与机器学习模型的构建分开处理，像是Canny边缘探测 [20] 和SIFT特征提取 [21] 曾占据统治性地位达10年以上，但这也就是人类能找到的最好方法了。</p><p>当深度学习进入这个领域后，这些特征提取方法就被性能更强的自动优化的逐级过滤器替代了。</p><h2 id="预备知识-Pytorch"><a href="#预备知识-Pytorch" class="headerlink" title="预备知识: Pytorch"></a>预备知识: Pytorch</h2><h3 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h3><ul><li><strong>对 Tensor 的操作：Tensor 的创建</strong></li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center"><code>Tensor(*sizes)</code></td><td style="text-align:center">基础构造函数</td></tr><tr><td style="text-align:center"><code>tensor(data,)</code></td><td style="text-align:center">类似 <code>np.array</code> 的构造函数</td></tr><tr><td style="text-align:center"><code>ones(*sizes)</code></td><td style="text-align:center">全 1 Tensor</td></tr><tr><td style="text-align:center"><code>zeros(*sizes)</code></td><td style="text-align:center">全 0 Tensor</td></tr><tr><td style="text-align:center"><code>eye(*sizes)</code></td><td style="text-align:center">对角线为 1，其他为 0</td></tr><tr><td style="text-align:center"><code>arange(s,e,step)</code></td><td style="text-align:center">从 s 到 e，步长为 step</td></tr><tr><td style="text-align:center"><code>linspace(s,e,steps)</code></td><td style="text-align:center">从 s 到 e，均匀切分成 steps 份</td></tr><tr><td style="text-align:center"><code>rand/randn(*sizes)</code></td><td style="text-align:center">均匀/标准分布</td></tr><tr><td style="text-align:center"><code>normal(mean,std)/uniform(from,to)</code></td><td style="text-align:center">正态分布/均匀分布</td></tr><tr><td style="text-align:center"><code>randperm(m)</code></td><td style="text-align:center">随机排列</td></tr></tbody></table></div><ul><li><strong>对 Tensor 进行操作时注意其可能的数据共享</strong></li></ul><p>如使用 <code>view()</code> 改变 Tensor 的形状的时候，注意返回的新 Tensor 与源 Tensor 虽然可能有不同的 size，但是是共享 data 的。</p><p>所以如果我们想返回一个真正新的副本（即不共享 data 内存）该怎么办呢？Pytorch 还提供了一个 <code>reshape()</code> 可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。推荐先用 <code>clone</code> 创造一个副本然后再使用 <code>view</code>。</p><p>注：虽然 <code>view</code> 返回的 <code>Tensor</code> 与源 <code>Tensor</code> 是共享 <code>data</code> 的，但是依然是一个新的 <code>Tensor</code>（因为 <code>Tensor</code> 除了包含 <code>data</code> 外还有一些其他属性），二者 <code>id</code>（内存地址）并不一致。</p><p>另外一个常用的函数就是 <code>item()</code>, 它可以将一个标量 <code>Tensor</code> 转换成一个 Python Number。</p><ul><li><strong>广播机制</strong> Broadcasting</li></ul><p>当对两个形状不同的 <code>Tensor</code> 按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个 <code>Tensor</code> 形状相同后再按元素运算。例如：</p><pre class="language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>y <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x <span class="token operator">+</span> y<span class="token punctuation">)</span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># x</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># y</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># x+y</span></code></pre><ul><li><strong><code>Tensor</code> 与 <code>ndarray</code></strong> 的转换</li></ul><p>很容易用 <code>numpy()</code> 和 <code>from_numpy()</code> 将 <code>Tensor</code> 和 NumPy 中的数组相互转换。但是需要注意的一点是：两个函数所产生的 <code>Tensor</code> 和 NumPy 中的数组共享相同的内存（所以它们之间的转换很快），改变其中一个时另一个也会改变。</p><p>与之相对比，还有一个常用的将 NumPy 中的 array 转换成 <code>Tensor</code> 的方法就是 <code>torch.tensor()</code>, 需要注意的是，此方法总是会进行数据拷贝（就会消耗更多的时间和空间），所以返回的 <code>Tensor</code>和原来的数据不再共享内存。</p><p>所有在CPU上的 <code>Tensor</code>（除了 <code>CharTensor</code>）都支持与 NumPy 数组相互转换。</p><ul><li><strong><code>Tensor</code> on GPU</strong></li></ul><p>用方法 <code>to()</code> 可以将 <code>Tensor</code> 在 CPU 和 GPU（需要硬件支持）之间相互移动。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 以下代码只有在PyTorch GPU版本上才会执行</span><span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>          <span class="token comment"># GPU</span>    y <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>x<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>  <span class="token comment"># 直接创建一个在GPU上的Tensor</span>    x <span class="token operator">=</span> x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>                       <span class="token comment"># 等价于 .to("cuda")</span>    z <span class="token operator">=</span> x <span class="token operator">+</span> y    <span class="token keyword">print</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>z<span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">"cpu"</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>double<span class="token punctuation">)</span><span class="token punctuation">)</span>       <span class="token comment"># to()还可以同时更改数据类型</span></code></pre><h3 id="Autograd"><a href="#Autograd" class="headerlink" title="Autograd"></a>Autograd</h3><ul><li><strong><code>Tensor</code> 的 <code>requires_grad</code> 与 <code>Function</code></strong></li></ul><p>上一节介绍的 <code>Tensor</code> 是这个包的核心类，如果将其属性 <code>requires_grad</code> 设置为 <code>True</code>，它将开始追踪（track）在其上的所有操作（这样就可以利用链式法则进行梯度传播了）。</p><p>完成计算后，可以调用 <code>backward()</code> 来完成所有梯度计算。此 <code>Tensor</code> 的梯度将累积到 <code>.grad</code> 属性中。</p><p>如果不想要被继续追踪，可以调用 <code>.detach()</code> 将其从追踪记录中分离出来，这样就可以防止将来的计算被追踪，这样梯度就传不过去了。此外，还可以用 <code>with torch.no_grad()</code> 将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用，因为在评估模型时，我们并不需要计算可训练参数（<code>requires_grad=True</code>）的梯度。</p><p><code>Function</code> 是另外一个很重要的类。<code>Tensor</code> 和 <code>Function</code> 互相结合就可以构建一个记录有整个计算过程的有向无环图。每个 <code>Tensor</code> 都有一个 <code>grad_fn</code> 属性，该属性即创建该 <code>Tensor</code> 的 <code>Function</code> , 就是说该 <code>Tensor</code> 是不是通过某些运算得到的，若是，则 <code>grad_fn</code> 返回一个与这些运算相关的对象，否则是 None。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> x <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad_fn<span class="token punctuation">)</span><span class="token boolean">None</span><span class="token operator">>></span><span class="token operator">></span> y <span class="token operator">=</span> x <span class="token operator">+</span> <span class="token number">2</span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>AddBackward0<span class="token operator">></span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">.</span>grad_fn<span class="token punctuation">)</span><span class="token operator">&lt;</span>AddBackward0 <span class="token builtin">object</span> at <span class="token number">0x7fbb003b4250</span><span class="token operator">></span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>is_leaf<span class="token punctuation">,</span> y<span class="token punctuation">.</span>is_leaf<span class="token punctuation">)</span><span class="token boolean">True</span> <span class="token boolean">False</span><span class="token operator">>></span><span class="token operator">></span> z <span class="token operator">=</span> y <span class="token operator">*</span> y <span class="token operator">*</span> <span class="token number">3</span><span class="token operator">>></span><span class="token operator">></span> out <span class="token operator">=</span> z<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>z<span class="token punctuation">,</span> out<span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">27</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">27</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">27</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">27</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>MulBackward0<span class="token operator">></span><span class="token punctuation">)</span> tensor<span class="token punctuation">(</span><span class="token number">27</span><span class="token punctuation">.</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>MeanBackward0<span class="token operator">></span><span class="token punctuation">)</span></code></pre><p>注意 x 是直接创建的，所以它没有 <code>grad_fn</code>, 而 y 是通过一个加法操作创建的，所以它有一个为 <code>&lt;AddBackward&gt;</code> 的 <code>grad_fn</code>。像 x 这种直接创建的称为叶子节点，叶子节点对应的 <code>grad_fn</code> 是 <code>None</code>。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment"># 缺失情况下默认 requires_grad = False</span><span class="token operator">>></span><span class="token operator">></span> a <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>a <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>a <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span> <span class="token comment"># False</span><span class="token boolean">False</span><span class="token comment"># 通过 .requires_grad_() 来用 in-place 的方式改变 requires_grad 属性</span><span class="token operator">>></span><span class="token operator">></span> a<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.0261</span><span class="token punctuation">,</span>  <span class="token number">0.6281</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">1.1572</span><span class="token punctuation">,</span>  <span class="token number">6.8756</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span> <span class="token comment"># True</span><span class="token boolean">True</span><span class="token operator">>></span><span class="token operator">></span> b <span class="token operator">=</span> <span class="token punctuation">(</span>a <span class="token operator">*</span> a<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span>grad_fn<span class="token punctuation">)</span><span class="token operator">&lt;</span>SumBackward0 <span class="token builtin">object</span> at <span class="token number">0x7fba80387730</span><span class="token operator">></span></code></pre><ul><li><strong><code>backward()</code></strong></li></ul><p>因为 <code>out</code> 是一个标量，所以调用 <code>backward()</code> 时不需要指定求导变量：</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 等价于 out.backward(torch.tensor(1.))</span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span> <span class="token comment"># Out 关于 x 的梯度, d(out)/dx</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">4.5000</span><span class="token punctuation">,</span> <span class="token number">4.5000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">4.5000</span><span class="token punctuation">,</span> <span class="token number">4.5000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p>本质上，反向传播的过程是在计算一系列 Jacobi 矩阵的乘积。</p><p>注意：grad 在反向传播过程中是累加的，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以<strong>一般在反向传播之前需把梯度清零</strong>。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token comment"># 再来反向传播一次，注意grad是累加的</span><span class="token operator">>></span><span class="token operator">></span> out2 <span class="token operator">=</span> x<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> out2<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5.5000</span><span class="token punctuation">,</span> <span class="token number">5.5000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">5.5000</span><span class="token punctuation">,</span> <span class="token number">5.5000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> out3 <span class="token operator">=</span> x<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> x<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> out3<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p>注：PyTorch 的 <code>backward</code> 为什么有一个 <code>grad_variables</code> 参数？</p><p>假设 x 经过一番计算得到 y，那么 <code>y.backward(w)</code> 求的不是 y 对 x 的导数，而是 <code>l = torch.sum(y*w)</code> 对 x 的导数。w 可以视为 y 的各分量的权重，也可以视为遥远的损失函数 l 对 y 的偏导数（这正是函数说明文档的含义）。特别地，若 y 为标量，w 取默认值 1.0，才是按照我们通常理解的那样，求 y 对 x 的导数。</p><ul><li><strong>中断梯度传播</strong></li></ul><pre class="language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>y1 <span class="token operator">=</span> x <span class="token operator">**</span> <span class="token number">2</span> <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    y2 <span class="token operator">=</span> x <span class="token operator">**</span> <span class="token number">3</span>y3 <span class="token operator">=</span> y1 <span class="token operator">+</span> y2<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>y1<span class="token punctuation">,</span> y1<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span> <span class="token comment"># True</span><span class="token keyword">print</span><span class="token punctuation">(</span>y2<span class="token punctuation">,</span> y2<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span> <span class="token comment"># False</span><span class="token keyword">print</span><span class="token punctuation">(</span>y3<span class="token punctuation">,</span> y3<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span> <span class="token comment"># True</span>Output<span class="token punctuation">:</span><span class="token boolean">True</span>tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>PowBackward0<span class="token operator">></span><span class="token punctuation">)</span> <span class="token boolean">True</span>tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">)</span> <span class="token boolean">False</span>tensor<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>ThAddBackward<span class="token operator">></span><span class="token punctuation">)</span> <span class="token boolean">True</span></code></pre><p>可以看到，上面的 <code>y2</code> 是没有 <code>grad_fn</code> 而且 <code>y2.requires_grad=False</code> 的，而 <code>y3</code> 是有 <code>grad_fn</code> 的。如果我们将<code>y3</code>对<code>x</code>求梯度的话：</p><pre class="language-python" data-language="python"><code class="language-python">y3<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">)</span></code></pre><p>正如我们所理解的，$y_3 = y_1 + y_2 = x^2 + x^3$，其中 $y_2$ 的梯度不被回传，因此 $\dfrac {d y_3} {dx} = 2x$.</p><p>此外，如果我们想要修改 <code>Tensor</code> 的数值，但是又不希望被 <code>autograd</code> 记录（即不会影响反向传播），那么我么可以对 <code>tensor.data</code> 进行操作。</p><pre class="language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>data<span class="token punctuation">)</span> <span class="token comment"># 还是一个tensor</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>data<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span> <span class="token comment"># 但是已经是独立于计算图之外</span>y <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> xx<span class="token punctuation">.</span>data <span class="token operator">*=</span> <span class="token number">100</span> <span class="token comment"># 只改变了值，不会记录在计算图，所以不会影响梯度传播</span>y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment"># 更改data的值也会影响tensor的值</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token boolean">False</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">100</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><h2 id="深度学习基础"><a href="#深度学习基础" class="headerlink" title="深度学习基础"></a>深度学习基础</h2><h3 id="基础模型"><a href="#基础模型" class="headerlink" title="基础模型"></a>基础模型</h3><h4 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h4><p>首先，回归问题的输出是连续值，而分类问题的输出是离散值，这是二者的区别。</p><ul><li><strong>模型定义</strong>：假设我们采集的样本数为 $n$​，索引为 $i$​ 的样本的特征为 $x_1^{(i)}$​ 和 $x_2^{(i)}$​，标签为 $y^{(i)}$​。对于索引为 $i$​ 的房屋，线性回归模型的房屋价格预测表达式为 <script type="math/tex">\hat{y}^{(i)} = x_1^{(i)} w_1 + x_2^{(i)} w_2 + b</script>​</li><li><strong>损失函数</strong>：$\ell^{(i)}(w_1, w_2, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2$​, $ \ell(w_1, w_2, b) =\frac{1}{n} \sum_{i=1}^n \ell^{(i)}(w_1, w_2, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right)^2$​</li></ul><p>在模型训练中，我们希望找出一组模型参数，记为 $w_1^<em>, w_2^</em>, b^<em>$，来使训练样本平均损失最小：$ w_1^</em>, w_2^<em>, b^</em> = \underset{w_1, w_2, b}{\arg\min} \ell(w_1, w_2, b) $</p><ul><li><strong>优化算法</strong></li></ul><p>当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作<strong>解析解</strong>。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作<strong>数值解</strong>。</p><p>在求数值解的优化算法中，<strong>小批量随机梯度下降</strong>（Mini-batch SGD, mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）$\mathcal{B}$​，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数 learning_rate $\eta$ 的乘积作为模型参数在本次迭代的减小量。</p><p>在训练本节讨论的线性回归模型的过程中，模型的每个参数将作如下迭代：</p><script type="math/tex; mode=display">\begin{aligned} w_1 &\leftarrow w_1 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{ \partial \ell^{(i)}(w_1, w_2, b) }{\partial w_1} = w_1 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right), \\w_2 &\leftarrow w_2 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{ \partial \ell^{(i)}(w_1, w_2, b) }{\partial w_2} = w_2 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right), \\b &\leftarrow b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{ \partial \ell^{(i)}(w_1, w_2, b) }{\partial b} = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right). \end{aligned}</script><p>在上式中，$|\mathcal{B}|$ 代表每个小批量中的样本个数（批量大小，batch size），$\eta$ 称作学习率（learning rate）并取正数。需要强调的是，这里的批量大小和学习率的值是人为设定的，并不是通过模型训练学出的，因此叫作超参数（hyperparameter）。我们通常所说的“调参”指的正是调节超参数，例如通过反复试错来找到超参数合适的值。</p><h5 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h5><ul><li>生成数据集</li></ul><pre class="language-python" data-language="python"><code class="language-python">num_inputs <span class="token operator">=</span> <span class="token number">2</span>num_examples <span class="token operator">=</span> <span class="token number">1000</span>true_w <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3.4</span><span class="token punctuation">]</span>true_b <span class="token operator">=</span> <span class="token number">4.2</span>features <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>num_examples<span class="token punctuation">,</span> num_inputs<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>labels <span class="token operator">=</span> true_w<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> features<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> true_w<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> features<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> true_blabels <span class="token operator">+=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">,</span> size<span class="token operator">=</span>labels<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span></code></pre><ul><li>读取数据</li></ul><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">as</span> Databatch_size <span class="token operator">=</span> <span class="token number">10</span><span class="token comment"># 将训练数据的特征和标签组合</span>dataset <span class="token operator">=</span> Data<span class="token punctuation">.</span>TensorDataset<span class="token punctuation">(</span>features<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token comment"># 随机读取小批量</span>data_iter <span class="token operator">=</span> Data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></code></pre><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> data_iter<span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span>    <span class="token keyword">break</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2.7723</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6627</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.1058</span><span class="token punctuation">,</span>  <span class="token number">0.7688</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">0.4901</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.2260</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.7227</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.2664</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.3390</span><span class="token punctuation">,</span>  <span class="token number">0.1162</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">1.6705</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.7930</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">0.2576</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.2928</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">2.0475</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.7440</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">1.0685</span><span class="token punctuation">,</span>  <span class="token number">1.1920</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">1.0996</span><span class="token punctuation">,</span>  <span class="token number">0.5106</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">0.9066</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6247</span><span class="token punctuation">,</span>  <span class="token number">9.3383</span><span class="token punctuation">,</span>  <span class="token number">3.6537</span><span class="token punctuation">,</span>  <span class="token number">3.1283</span><span class="token punctuation">,</span> <span class="token number">17.0213</span><span class="token punctuation">,</span>  <span class="token number">5.6953</span><span class="token punctuation">,</span> <span class="token number">17.6279</span><span class="token punctuation">,</span>         <span class="token number">2.2809</span><span class="token punctuation">,</span>  <span class="token number">4.6661</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># 可以看到，对迭代器进行迭代每次拿到的数据也是以 batch 的形式封装成 array</span></code></pre><ul><li>定义模型</li></ul><p>首先，导入 <code>torch.nn</code> 模块。实际上，“nn”是neural networks（神经网络）的缩写。</p><p>顾名思义，该模块定义了大量神经网络的层。之前我们已经用过了 <code>autograd</code>，而 <code>nn</code> 就是利用 <code>autograd</code> 来定义模型。</p><p><code>nn</code> 的核心数据结构是 <code>Module</code>，它是一个抽象概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。在实际使用中，最常见的做法是继承 <code>nn.Module</code>，撰写自己的网络/层。</p><p>一个 <code>nn.Module</code> 实例应该包含一些层以及返回输出的前向传播（forward）方法。下面先来看看如何用 <code>nn.Module</code> 实现一个线性回归模型。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LinearNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_feature<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>LinearNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_feature<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment"># forward 定义前向传播</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> ynet <span class="token operator">=</span> LinearNet<span class="token punctuation">(</span>num_inputs<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span> <span class="token comment"># 使用print可以打印出网络的结构</span><span class="token comment"># Output:</span>LinearNet<span class="token punctuation">(</span>  <span class="token punctuation">(</span>linear<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>事实上我们还可以用<code>nn.Sequential</code>来更加方便地搭建网络，<code>Sequential</code>是一个有序的容器，网络层将按照在传入<code>Sequential</code>的顺序依次被添加到计算图中。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 写法一</span>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    <span class="token comment"># 此处还可以传入其他层</span>    <span class="token punctuation">)</span><span class="token comment"># 写法二</span>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">'linear'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># net.add_module ......</span><span class="token comment"># 写法三</span><span class="token keyword">from</span> collections <span class="token keyword">import</span> OrderedDictnet <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>OrderedDict<span class="token punctuation">(</span><span class="token punctuation">[</span>          <span class="token punctuation">(</span><span class="token string">'linear'</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>          <span class="token comment"># ......</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>Output<span class="token punctuation">:</span>Sequential<span class="token punctuation">(</span>  <span class="token punctuation">(</span>linear<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></code></pre><p>可以通过 <code>net.parameters()</code> 来查看模型所有的可学习参数，此函数将返回一个生成器。<br><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>param<span class="token punctuation">)</span></code></pre><br>输出：<br><pre class="language-none"><code class="language-none">Parameter containing:tensor([[-0.0277,  0.2771]], requires_grad&#x3D;True)Parameter containing:tensor([0.3395], requires_grad&#x3D;True)</code></pre></p><p>注意：<code>torch.nn</code> <strong>仅支持输入一个 batch 的样本</strong>，而不支持单个样本输入，如果只有单个样本，可使用 <code>input.unsqueeze(0)</code> 来添加一维。</p><p>附：<code>unsqueeze()</code> 的使用：</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> atensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.6444</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5408</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6239</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.8880</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7358</span><span class="token punctuation">,</span>  <span class="token number">0.7287</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span> <span class="token number">1.1660</span><span class="token punctuation">,</span>  <span class="token number">1.3125</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3.4676</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.4620</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1572</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4755</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span> <span class="token number">0.6389</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.4514</span><span class="token punctuation">,</span>  <span class="token number">0.3339</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> a<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment"># 最外层加壳</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.6444</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5408</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6239</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.8880</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7358</span><span class="token punctuation">,</span>  <span class="token number">0.7287</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">1.1660</span><span class="token punctuation">,</span>  <span class="token number">1.3125</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3.4676</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.4620</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1572</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4755</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">0.6389</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.4514</span><span class="token punctuation">,</span>  <span class="token number">0.3339</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> a<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># 次外层元素</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.6444</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5408</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6239</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.8880</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7358</span><span class="token punctuation">,</span>  <span class="token number">0.7287</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">1.1660</span><span class="token punctuation">,</span>  <span class="token number">1.3125</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3.4676</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.4620</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1572</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4755</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">0.6389</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.4514</span><span class="token punctuation">,</span>  <span class="token number">0.3339</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> a<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.6444</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5408</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6239</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.8880</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7358</span><span class="token punctuation">,</span>  <span class="token number">0.7287</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1.1660</span><span class="token punctuation">,</span>  <span class="token number">1.3125</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3.4676</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.4620</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1572</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4755</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.6389</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.4514</span><span class="token punctuation">,</span>  <span class="token number">0.3339</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> a<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.6444</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.5408</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.6239</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.8880</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.7358</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">0.7287</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1.1660</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">1.3125</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">3.4676</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.4620</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.1572</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.4755</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.6389</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2.4514</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">0.3339</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> a<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span>Traceback <span class="token punctuation">(</span>most recent call last<span class="token punctuation">)</span><span class="token punctuation">:</span>  File <span class="token string">"&lt;stdin>"</span><span class="token punctuation">,</span> line <span class="token number">1</span><span class="token punctuation">,</span> <span class="token keyword">in</span> <span class="token operator">&lt;</span>module<span class="token operator">></span>IndexError<span class="token punctuation">:</span> Dimension out of <span class="token builtin">range</span> <span class="token punctuation">(</span>expected to be <span class="token keyword">in</span> <span class="token builtin">range</span> of <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> but got <span class="token number">4</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> a<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># 最内层</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.6444</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.5408</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.6239</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.8880</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.7358</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">0.7287</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1.1660</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">1.3125</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">3.4676</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.4620</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.1572</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.4755</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.6389</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2.4514</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">0.3339</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> a<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 一直到最内层</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.6444</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5408</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6239</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.8880</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7358</span><span class="token punctuation">,</span>  <span class="token number">0.7287</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">1.1660</span><span class="token punctuation">,</span>  <span class="token number">1.3125</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3.4676</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.4620</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1572</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4755</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">0.6389</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.4514</span><span class="token punctuation">,</span>  <span class="token number">0.3339</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> a<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># 某一层</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.6444</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5408</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6239</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.8880</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7358</span><span class="token punctuation">,</span>  <span class="token number">0.7287</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span> <span class="token number">1.1660</span><span class="token punctuation">,</span>  <span class="token number">1.3125</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3.4676</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.4620</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1572</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4755</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span> <span class="token number">0.6389</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.4514</span><span class="token punctuation">,</span>  <span class="token number">0.3339</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><ul><li>初始化模型参数</li></ul><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> initinit<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight<span class="token punctuation">,</span> mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>bias<span class="token punctuation">,</span> val<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># 也可以直接修改bias的data: net[0].bias.data.fill_(0)</span></code></pre><ul><li>定义损失函数</li></ul><p>PyTorch 在 <code>nn</code> 模块中提供了各种损失函数，这些损失函数可看作是一种特殊的层，PyTorch 也将这些损失函数实现为 <code>nn.Module</code> 的子类。我们现在使用它提供的均方误差损失作为模型的损失函数。</p><pre class="language-python" data-language="python"><code class="language-python">loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><ul><li>定义优化算法</li></ul><p>同样，我们也无须自己实现小批量随机梯度下降算法。<code>torch.optim</code> 模块提供了很多常用的优化算法比如 SGD、Adam 和 RMSProp 等。下面我们创建一个用于优化 <code>net</code> 所有参数的优化器实例，并指定学习率为 0.03 的小批量随机梯度下降（SGD）为优化算法。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optimoptimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.03</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>optimizer<span class="token punctuation">)</span></code></pre><p>输出：<br><pre class="language-none"><code class="language-none">SGD (Parameter Group 0    dampening: 0    lr: 0.03    momentum: 0    nesterov: False    weight_decay: 0)</code></pre></p><p>我们还可以为不同子网络设置不同的学习率，这在 fine-tune 时经常用到。例：<br><pre class="language-python" data-language="python"><code class="language-python">optimizer <span class="token operator">=</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span><span class="token punctuation">[</span>                <span class="token comment"># 如果对某个参数不指定学习率，就使用最外层的默认学习率</span>                <span class="token punctuation">&#123;</span><span class="token string">'params'</span><span class="token punctuation">:</span> net<span class="token punctuation">.</span>subnet1<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span> <span class="token comment"># lr=0.03</span>                <span class="token punctuation">&#123;</span><span class="token string">'params'</span><span class="token punctuation">:</span> net<span class="token punctuation">.</span>subnet2<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.01</span><span class="token punctuation">&#125;</span>            <span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.03</span><span class="token punctuation">)</span></code></pre></p><p>有时候我们不想让学习率固定成一个常数，那如何调整学习率呢？主要有两种做法。一种是修改 <code>optimizer.param_groups</code> 中对应的学习率，另一种是更简单也是较为推荐的做法 —— <strong>新建优化器</strong>，由于optimizer十分轻量级，构建开销很小，故而可以构建新的 optimizer。但是后者对于使用动量的优化器（如 Adam），会丢失动量等状态信息，可能会造成损失函数的收敛出现震荡等情况。<br><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 调整学习率</span><span class="token keyword">for</span> param_group <span class="token keyword">in</span> optimizer<span class="token punctuation">.</span>param_groups<span class="token punctuation">:</span>    param_group<span class="token punctuation">[</span><span class="token string">'lr'</span><span class="token punctuation">]</span> <span class="token operator">*=</span> <span class="token number">0.1</span> <span class="token comment"># 学习率为之前的0.1倍</span></code></pre></p><ul><li>训练模型</li></ul><p>通过调用 <code>optim</code> 实例的 <code>step</code> 函数来迭代模型参数。按照小批量随机梯度下降的定义，我们在 <code>step</code> 函数中指明批量大小，从而对批量中样本梯度求平均。</p><pre class="language-python" data-language="python"><code class="language-python">num_epochs <span class="token operator">=</span> <span class="token number">3</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> data_iter<span class="token punctuation">:</span>        output <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        l <span class="token operator">=</span> loss<span class="token punctuation">(</span>output<span class="token punctuation">,</span> y<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 梯度清零，等价于net.zero_grad()</span>        l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'epoch %d, loss: %f'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> l<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>输出：<br><pre class="language-none"><code class="language-none">epoch 1, loss: 0.000457epoch 2, loss: 0.000081epoch 3, loss: 0.000198</code></pre></p><h4 id="Softmax-Regression"><a href="#Softmax-Regression" class="headerlink" title="Softmax Regression"></a>Softmax Regression</h4><p>线性回归模型适用于<strong>输出为连续值</strong>的情景。</p><p>在另一类情景中，模型输出可以是一个像图像类别这样的<strong>离散值</strong>。对于这样的离散值预测问题，我们可以使用诸如 softmax 回归在内的<strong>分类模型</strong>。</p><p>和线性回归不同，softmax 回归的<strong>输出单元从一个变成了多个</strong>，且引入了 softmax 运算使输出更适合离散值的预测和训练。本节以 softmax 回归模型为例，介绍神经网络中的分类模型。</p><p>softmax 回归跟线性回归一样将输入特征与权重做线性叠加。与线性回归的一个主要不同在于，softmax 回归的<strong>输出值个数等于标签里的类别数</strong>。</p><ul><li>模型定义</li></ul><script type="math/tex; mode=display">\begin{aligned} o_1 &= x_1 w_{11} + x_2 w_{21} + x_3 w_{31} + x_4 w_{41} + b_1,\\ o_2 &= x_1 w_{12} + x_2 w_{22} + x_3 w_{32} + x_4 w_{42} + b_2,\\ o_3 &= x_1 w_{13} + x_2 w_{23} + x_3 w_{33} + x_4 w_{43} + b_3. \\\end{aligned}</script><script type="math/tex; mode=display">\hat{y}_1, \hat{y}_2, \hat{y}_3 = \text{softmax}(o_1, o_2, o_3)</script><p>其中：</p><script type="math/tex; mode=display">\hat{y_1} = \frac{ \exp(o_1)}{\sum_{i=1}^3 \exp(o_i)},\quad \hat{y_2} = \frac{ \exp(o_2)}{\sum_{i=1}^3 \exp(o_i)},\quad \hat{y_3} = \frac{ \exp(o_3)}{\sum_{i=1}^3 \exp(o_i)}.</script><ul><li>损失函数：Cross Entropy</li></ul><script type="math/tex; mode=display">H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ) = -\sum_{j=1}^q y_j^{(i)} \log \hat y_j^{(i)},</script><p>交叉熵只关心<strong>对正确类别</strong>的预测概率，因为只要其值足够大，就可以确保分类结果正确。</p><p>假设训练数据集的样本数为 $n$​，交叉熵损失函数定义为 $\ell(\boldsymbol{\Theta}) = \frac{1}{n} \sum_{i=1}^n H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ) $​. 其中$\boldsymbol{\Theta}$​​代表模型参数。</p><p>注意：这里的交叉熵是 H = - [ 实际值 * log(预测值) ] 的求和。而 log(预测值) 却又可以替换为是预测时的 logits $o_i$​​​，二者之间只差了一个系数。因此背后实现可以直接用 logits 参与简化计算。</p><blockquote><p>数据集与相关包的介绍：Fashion-MNIST</p><p>本节我们将使用 torchvision 包，它是服务于 PyTorch 深度学习框架的，主要用来构建计算机视觉模型。 torchvision 主要由以下几部分构成：</p><ol><li><code>torchvision.datasets</code>: 一些加载数据的函数及常用的数据集接口；</li><li><code>torchvision.models</code>: 包含常用的模型结构（含预训练模型），例如 AlexNet、VGG、ResNet 等；</li><li><code>torchvision.transforms</code>: 常用的图片变换，例如裁剪、旋转等；</li><li><code>torchvision.utils</code>: 其他的一些有用的方法。</li></ol></blockquote><h4 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h4><ul><li>仅添加隐藏层：即便再添加更多的隐藏层，将线性隐藏层间彼此相接依然只能与仅含输出层的单层神经网络等价。</li></ul><p>上述问题的根源在于全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。</p><p>解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。</p><p>这个非线性函数被称为激活函数（activation function）。下面我们介绍几个常用的激活函数。</p><ul><li>$Relu(x) = \max(0, x)$.</li><li>$sigmoid(x) =\sigma(x) = \dfrac 1 {1 + e^{-x}}$.</li><li>$\text{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}$.</li></ul><p>多层感知机就是含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。多层感知机的层数和各隐藏层中隐藏单元个数都是超参数。</p><h3 id="应对过拟合"><a href="#应对过拟合" class="headerlink" title="应对过拟合"></a>应对过拟合</h3><h4 id="L2-Regularization"><a href="#L2-Regularization" class="headerlink" title="L2 Regularization"></a>L2 Regularization</h4><ul><li><strong>权重衰减</strong>是一种应对过拟合的方法</li><li>权重衰减等价于 $L_2$ 范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。我们先描述 $L_2$ 范数正则化，再解释它为何又称权重衰减。</li></ul><p>$L_2$ 范数正则化在模型原损失函数基础上添加 $L_2$ 范数惩罚项，从而得到训练所需要最小化的函数。</p><p>$L_2$ 范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。即定义新的 Loss Function 为：</p><script type="math/tex; mode=display">\ell(w, b) + \frac{\lambda}{2n} \|\boldsymbol{w}\|^2</script><p>为什么 $L_2$ Regularization 能起到“权重衰减”的作用呢？我们考虑 Optimizer 的迭代方式…</p><script type="math/tex; mode=display">\begin{aligned} w_1 &\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_1 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\\ w_2 &\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_2 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right). \end{aligned}</script><p>这是因为如果我们对损失函数求梯度，就必然带有了和原有的 $w$ 有关的一项。然后我们在做优化迭代的过程中，$L_2$ 范数正则化令权重 $w_1$ 和 $w_2$ 先自乘小于 1 的数，再减去不含惩罚项的梯度。因此，$L_2$ 范数正则化又叫权重衰减。</p><p>权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。实际场景中，我们有时也在惩罚项中添加偏差元素的平方和。</p><h5 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h5><ul><li>定义 L2 范数惩罚项</li></ul><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">l2_penalty</span><span class="token punctuation">(</span>w<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> <span class="token punctuation">(</span>w<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span></code></pre><ul><li>Train (From scratch)</li></ul><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">fit_and_plot</span><span class="token punctuation">(</span>lambd<span class="token punctuation">)</span><span class="token punctuation">:</span>    w<span class="token punctuation">,</span> b <span class="token operator">=</span> init_params<span class="token punctuation">(</span><span class="token punctuation">)</span>    train_ls<span class="token punctuation">,</span> test_ls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span>            <span class="token comment"># 添加了L2范数惩罚项</span>            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">,</span> w<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token operator">+</span> lambd <span class="token operator">*</span> l2_penalty<span class="token punctuation">(</span>w<span class="token punctuation">)</span>            l <span class="token operator">=</span> l<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> w<span class="token punctuation">.</span>grad <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>                w<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>                b<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>            l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>            d2l<span class="token punctuation">.</span>sgd<span class="token punctuation">(</span><span class="token punctuation">[</span>w<span class="token punctuation">,</span> b<span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>        train_ls<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>train_features<span class="token punctuation">,</span> w<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">,</span> train_labels<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        test_ls<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>test_features<span class="token punctuation">,</span> w<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">,</span> test_labels<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    d2l<span class="token punctuation">.</span>semilogy<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> train_ls<span class="token punctuation">,</span> <span class="token string">'epochs'</span><span class="token punctuation">,</span> <span class="token string">'loss'</span><span class="token punctuation">,</span>                 <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> test_ls<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'L2 norm of w:'</span><span class="token punctuation">,</span> w<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><ul><li>Train (Simple)</li></ul><p>这里我们直接在构造优化器实例时通过 <code>weight_decay</code> 参数来指定权重衰减超参数。默认下，PyTorch 会对权重和偏差同时衰减。我们可以分别对权重和偏差构造优化器实例，从而只对权重衰减。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">fit_and_plot_pytorch</span><span class="token punctuation">(</span>wd<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 对权重参数衰减。权重名称一般是以weight结尾</span>    net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>net<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>net<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    optimizer_w <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>params<span class="token operator">=</span><span class="token punctuation">[</span>net<span class="token punctuation">.</span>weight<span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">,</span> weight_decay<span class="token operator">=</span>wd<span class="token punctuation">)</span> <span class="token comment"># 对权重参数衰减</span>    optimizer_b <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>params<span class="token operator">=</span><span class="token punctuation">[</span>net<span class="token punctuation">.</span>bias<span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>  <span class="token comment"># 不对偏差参数衰减</span>    train_ls<span class="token punctuation">,</span> test_ls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span>            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>            optimizer_w<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>            optimizer_b<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>            l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token comment"># 对两个 optimizer 实例分别调用 step 函数，从而分别更新权重和偏差</span>            optimizer_w<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>            optimizer_b<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        train_ls<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>train_features<span class="token punctuation">)</span><span class="token punctuation">,</span> train_labels<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        test_ls<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>test_features<span class="token punctuation">)</span><span class="token punctuation">,</span> test_labels<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    d2l<span class="token punctuation">.</span>semilogy<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> train_ls<span class="token punctuation">,</span> <span class="token string">'epochs'</span><span class="token punctuation">,</span> <span class="token string">'loss'</span><span class="token punctuation">,</span>                 <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> test_ls<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'L2 norm of w:'</span><span class="token punctuation">,</span> net<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p>除了前一节介绍的权重衰减以外，深度学习模型常常使用丢弃法（Dropout）来应对过拟合问题。丢弃法有一些不同的变体。本节中提到的丢弃法特指倒置丢弃法（inverted dropout）。</p><p>当对某个隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为 $p$，那么有 $p$ 的概率其输出 $h_i$ 会被清零，有 $1-p$ 的概率 $h_i$ 会除以 $1-p$ 做拉伸。</p><p>丢弃概率是丢弃法的超参数。具体来说，设随机变量 $\xi_i$ 为 0 和 1 的概率分别为 $p$ 和 $1-p$ 。使用丢弃法时我们计算新的隐藏单元 $ h_i’ = \frac{\xi_i}{1-p} h_i $，由于 $E(\xi_i) = 1-p$，因此</p><script type="math/tex; mode=display">E(h_i') = \frac{E(\xi_i)}{1-p}h_i = h_i</script><p>即<strong>丢弃法不改变其输入的期望值</strong>。</p><ul><li>实现（Simple）</li></ul><pre class="language-python" data-language="python"><code class="language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>        d2l<span class="token punctuation">.</span>FlattenLayer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> num_hiddens1<span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>drop_prob1<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># Here</span>        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens1<span class="token punctuation">,</span> num_hiddens2<span class="token punctuation">)</span><span class="token punctuation">,</span>         nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>drop_prob2<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># Here</span>        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens2<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span><span class="token keyword">for</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>param<span class="token punctuation">,</span> mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span></code></pre><blockquote><ul><li><strong>数值稳定性</strong></li></ul><p>深度模型有关数值稳定性的典型问题是衰减（vanishing）和爆炸（explosion）。</p><p>当神经网络的层数较多时，模型的数值稳定性容易变差。假设一个层数为 $L$ 的多层感知机的第 $l$ 层 $\boldsymbol{H}^{(l)}$ 的权重参数为 $\boldsymbol{W}^{(l)}$，输出层 $\boldsymbol{H}^{(L)}$ 的权重参数为 $\boldsymbol{W}^{(L)}$。</p><p>为了便于讨论，不考虑偏差参数，且设所有隐藏层的激活函数为恒等映射 $\phi(x) = x$。</p><p>给定输入 $\boldsymbol{X}$，多层感知机的第 $l$ 层的输出 $\boldsymbol{H}^{(l)} = \boldsymbol{X} \boldsymbol{W}^{(1)} \boldsymbol{W}^{(2)} \ldots \boldsymbol{W}^{(l)}$。</p><p>此时，如果层数$l$较大，$\boldsymbol{H}^{(l)}$ 的计算可能会出现衰减或爆炸。</p><p>举个例子，假设输入和所有层的权重参数都是标量，如权重参数为 0.2 和 5，多层感知机的第 30 层输出为输入 $\boldsymbol{X}$ 分别与 $0.2^{30} \approx 1 \times 10^{-21}$（衰减）和 $5^{30} \approx 9 \times 10^{20}$（爆炸）的乘积。</p><p>类似地，当层数较多时，梯度的计算也更容易出现衰减或爆炸。</p><ul><li><strong>模型初始化</strong></li></ul><p>在神经网络中，通常需要随机初始化模型参数。下面我们来解释这样做的原因。</p><p>考虑多层感知机模型。如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。</p><p>在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。</p><p>之后的迭代也是如此。在这种情况下，无论隐藏单元有多少，隐藏层本质上只有 1 个隐藏单元在发挥作用。</p><p>因此，正如在前面的实验中所做的那样，我们通常将神经网络的模型参数，特别是权重参数，进行随机初始化。</p><p>PyTorch 中 <code>nn.Module</code> 的模块参数都采取了较为合理的初始化策略。</p></blockquote><h2 id="深度学习计算"><a href="#深度学习计算" class="headerlink" title="深度学习计算"></a>深度学习计算</h2><h3 id="模型构造"><a href="#模型构造" class="headerlink" title="模型构造"></a>模型构造</h3><ul><li>可以通过继承<code>Module</code>类来构造模型，重载 <code>__init__</code> 函数和 <code>forward</code> 函数。</li></ul><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MLP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 声明带有模型参数的层，这里声明了两个全连接层</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 调用 MLP 父类 Module 的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数</span>        <span class="token comment"># 参数，如“模型参数的访问、初始化和共享”一节将介绍的模型参数 params</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>MLP<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>hidden <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span> <span class="token comment"># 隐藏层</span>        self<span class="token punctuation">.</span>act <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>output <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>  <span class="token comment"># 输出层</span>    <span class="token comment"># 定义模型的前向计算，即如何根据输入 x 计算返回所需要的模型输出</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        a <span class="token operator">=</span> self<span class="token punctuation">.</span>act<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>output<span class="token punctuation">(</span>a<span class="token punctuation">)</span></code></pre><ul><li><code>Sequential</code>、<code>ModuleList</code>、<code>ModuleDict</code>类都继承自<code>Module</code>类。</li></ul><p><strong><code>Sequential</code></strong></p><pre class="language-python" data-language="python"><code class="language-python">net <span class="token operator">=</span> MySequential<span class="token punctuation">(</span>        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span>         <span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span></code></pre><p><strong><code>ModuleList</code></strong></p><pre class="language-python" data-language="python"><code class="language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>net<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># # 类似 List 的 append 操作</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 类似 List 的索引访问</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span class="token comment"># net(torch.zeros(1, 784)) # 会报 NotImplementedError</span></code></pre><p><code>ModuleList</code>仅仅是一个储存各种模块的列表，这些模块之间没有联系也没有顺序（所以不用保证相邻层的输入输出维度匹配），而且没有实现 <code>forward</code> 功能需要自己实现，所以上面执行 <code>net(torch.zeros(1, 784))</code> 会报<code>NotImplementedError</code>；而 <code>Sequential</code> 内的模块需要按照顺序排列，要保证相邻层的输入输出大小相匹配，内部 <code>forward</code> 功能已经实现。此外，<code>ModuleList</code> 不同于一般的 Python 的 <code>list</code>，加入到 <code>ModuleList</code> 里面的所有模块的参数会被自动添加到整个网络中。</p><p><strong><code>ModuleDict</code></strong></p><p><code>ModuleDict</code> 接收一个子模块的字典作为输入, 然后也可以类似字典那样进行添加访问操作。</p><pre class="language-python" data-language="python"><code class="language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleDict<span class="token punctuation">(</span><span class="token punctuation">&#123;</span>    <span class="token string">'linear'</span><span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token string">'act'</span><span class="token punctuation">:</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>net<span class="token punctuation">[</span><span class="token string">'output'</span><span class="token punctuation">]</span> <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span> <span class="token comment"># 添加</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">[</span><span class="token string">'linear'</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># 访问</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>output<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span class="token comment"># net(torch.zeros(1, 784)) # 会报NotImplementedError</span></code></pre><p>和 <code>ModuleList</code> 一样，<code>ModuleDict</code> 实例仅仅是存放了一些模块的字典，并没有定义 <code>forward</code> 函数需要自己定义。同样，<code>ModuleDict</code> 也与Python的 <code>Dict</code> 有所不同，<code>ModuleDict</code> 里的所有模块的参数会被自动添加到整个网络中。</p><ul><li>与<code>Sequential</code>不同，<code>ModuleList</code>和<code>ModuleDict</code>并没有定义一个完整的网络，它们只是将不同的模块存放在一起，需要自己定义<code>forward</code>函数。</li><li>虽然<code>Sequential</code>等类可以使模型构造更加简单，但直接继承<code>Module</code>类可以极大地拓展模型构造的灵活性。</li></ul><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">FancyMLP</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>FancyMLP<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>rand_weight <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token comment"># 不可训练参数（常数参数）</span>        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token comment"># 使用创建的常数参数，以及 nn.functional 中的 relu 函数和 mm 函数</span>        x <span class="token operator">=</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>rand_weight<span class="token punctuation">.</span>data<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment"># 复用全连接层。等价于两个全连接层共享参数</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token comment"># 控制流，这里我们需要调用 item 函数来返回标量进行比较</span>        <span class="token keyword">while</span> x<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>            x <span class="token operator">/=</span> <span class="token number">2</span>        <span class="token keyword">if</span> x<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">0.8</span><span class="token punctuation">:</span>            x <span class="token operator">*=</span> <span class="token number">10</span>        <span class="token keyword">return</span> x<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h3 id="模型参数的访问、初始化与共享"><a href="#模型参数的访问、初始化与共享" class="headerlink" title="模型参数的访问、初始化与共享"></a>模型参数的访问、初始化与共享</h3><p>本节用到的模型：</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> initnet <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># pytorch 已进行默认初始化</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span>X <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>Y <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>Output<span class="token punctuation">:</span>Sequential<span class="token punctuation">(</span>  <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><ul><li><strong>访问模型参数</strong></li></ul><p>对于 <code>Sequential</code> 实例（派生自 <code>Module</code>）中含模型参数的层，我们可以通过 <code>Module</code> 类的 <code>parameters()</code> 或者 <code>named_parameters</code> 方法来访问所有参数（以迭代器的形式返回），后者除了返回参数 <code>Tensor</code> 外还会返回其名字。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'generator'</span><span class="token operator">></span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> param<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token number">0.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token number">0.</span>bias torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token number">2.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token number">2.</span>bias torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p>接下来访问单层的参数。对于使用 <code>Sequential</code> 类构造的神经网络，我们可以通过方括号 <code>[]</code> 来访问网络的任一层。索引 0 表示隐藏层为 <code>Sequential</code> 实例最先添加的层。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> param<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token punctuation">(</span>param<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>weight torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'torch.nn.parameter.Parameter'</span><span class="token operator">></span>bias torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'torch.nn.parameter.Parameter'</span><span class="token operator">></span></code></pre><p>返回的 <code>param</code> 的类型为 <code>torch.nn.parameter.Parameter</code>，其实这是 <code>Tensor</code> 的子类，和 <code>Tensor</code> 不同的是如果一个 <code>Tensor</code> 是 <code>Parameter</code>，那么它会自动被添加到模型的参数列表里。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MyModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>MyModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>weight1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>weight2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">pass</span>n <span class="token operator">=</span> MyModel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> n<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">)</span>    Output<span class="token punctuation">:</span>weight1</code></pre><ul><li><strong>初始化模型参数</strong></li></ul><p>PyTorch 中 <code>nn.Module</code> 的模块参数都采取了较为合理的初始化策略，但我们经常需要使用其他方法来初始化权重。</p><p>PyTorch 的 <code>init</code> 模块里提供了多种预设的初始化方法。在下面的例子中，我们将权重参数初始化成均值为 0、标准差为 0.01 的正态分布随机数，并依然将偏差参数清零。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> <span class="token string">'weight'</span> <span class="token keyword">in</span> name<span class="token punctuation">:</span>        init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>param<span class="token punctuation">,</span> mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> param<span class="token punctuation">.</span>data<span class="token punctuation">)</span>Output<span class="token punctuation">:</span><span class="token number">0.</span>weight tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.0030</span><span class="token punctuation">,</span>  <span class="token number">0.0094</span><span class="token punctuation">,</span>  <span class="token number">0.0070</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0010</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">0.0001</span><span class="token punctuation">,</span>  <span class="token number">0.0039</span><span class="token punctuation">,</span>  <span class="token number">0.0105</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0126</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">0.0105</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0135</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0047</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0006</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token number">2.</span>weight tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.0074</span><span class="token punctuation">,</span>  <span class="token number">0.0051</span><span class="token punctuation">,</span>  <span class="token number">0.0066</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p>下面使用常数来初始化权重参数。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> <span class="token string">'bias'</span> <span class="token keyword">in</span> name<span class="token punctuation">:</span>        init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>param<span class="token punctuation">,</span> val<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> param<span class="token punctuation">.</span>data<span class="token punctuation">)</span>Output<span class="token punctuation">:</span><span class="token number">0.</span>bias tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token number">2.</span>bias tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p>有时候我们需要的初始化方法并没有在 <code>init</code> 模块中提供。这时，可以实现一个初始化方法，从而能够像使用其他初始化方法那样使用它。</p><p>首先参考 normal_ 的实现：</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">normal_</span><span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> tensor<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>mean<span class="token punctuation">,</span> std<span class="token punctuation">)</span></code></pre><p>类似地，我们可以实现自定义的初始化方法：</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">init_weight_</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        tensor<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>        tensor <span class="token operator">*=</span> <span class="token punctuation">(</span>tensor<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">>=</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> <span class="token string">'weight'</span> <span class="token keyword">in</span> name<span class="token punctuation">:</span>        init_weight_<span class="token punctuation">(</span>param<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> param<span class="token punctuation">.</span>data<span class="token punctuation">)</span></code></pre><p>此外，我们还可以通过直接改变这些参数的 <code>data</code> 来达到改写模型参数值的同时不会影响梯度的效果。</p><ul><li><strong>共享模型参数</strong></li></ul><p>在有些情况下，我们希望在多个层之间共享模型参数。</p><p>共享模型参数的方法: <code>Module</code> 类的 <code>forward</code> 函数里多次调用同一个层。</p><p>此外，如果我们传入 <code>Sequential</code> 的模块是同一个 <code>Module</code> 实例的话参数也是共享的。</p><pre class="language-python" data-language="python"><code class="language-python">linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>linear<span class="token punctuation">,</span> linear<span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>param<span class="token punctuation">,</span> val<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> param<span class="token punctuation">.</span>data<span class="token punctuation">)</span>Output<span class="token punctuation">:</span>Sequential<span class="token punctuation">(</span>  <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token number">0.</span>weight tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># 这里只输出了一次参数列表，证明二者共享</span></code></pre><p>因为模型参数里包含了梯度，所以在反向传播计算时，这些共享的参数的梯度是累加的。</p><h3 id="自定义-Layer"><a href="#自定义-Layer" class="headerlink" title="自定义 Layer"></a>自定义 Layer</h3><p>本节将介绍如何使用 <code>Module</code>来自定义层，从而可以被重复调用。</p><ul><li><strong>不含模型参数的自定义层</strong></li></ul><p>事实上，自定义层和自定义模型类似，因为我们可以直接把一个 Packed 的模型视为是一个 Layer。</p><p>举个例子，下面的 <code>CenteredLayer</code> 类通过继承 <code>Module</code> 类自定义了一个将输入减掉均值后输出的层，并将层的计算定义在了 <code>forward</code> 函数里。这个层里不含模型参数。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">class</span> <span class="token class-name">CenteredLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>CenteredLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> x <span class="token operator">-</span> x<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p>然后，我们就可以实例化这个 Layer，然后做 Forward Feeding.</p><pre class="language-python" data-language="python"><code class="language-python">layer <span class="token operator">=</span> CenteredLayer<span class="token punctuation">(</span><span class="token punctuation">)</span>layer<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">)</span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><ul><li><strong>含模型参数的自定义层</strong></li></ul><p>之前我们介绍了 <code>Parameter</code> 类其实是 <code>Tensor</code> 的子类，如果一个 <code>Tensor</code> 是 <code>Parameter</code>，那么它会自动被添加到模型的参数列表里。 // 在这里可以推测这个参数列表是 <code>nn.Module</code> 的数据成员…?</p><p>所以在自定义含模型参数的层时，我们应该将参数定义成 <code>Parameter</code>。</p><p>除了直接定义成 <code>Parameter</code> 类外，还可以使用 <code>ParameterList</code> 和 <code>ParameterDict</code> 分别定义参数的列表和字典。</p><ul><li><strong><code>ParameterList</code></strong></li></ul><p><code>ParameterList</code>接收一个 <code>Parameter</code> 实例的列表作为输入然后得到一个参数列表，使用的时候可以用索引来访问某个参数，另外也可以使用 <code>append</code> 和 <code>extend</code> 在列表后面新增参数。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MyDense</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>MyDense<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>params <span class="token operator">=</span> nn<span class="token punctuation">.</span>ParameterList<span class="token punctuation">(</span><span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>params<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>params<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            x <span class="token operator">=</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> xnet <span class="token operator">=</span> MyDense<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span></code></pre><p>Output:</p><pre class="language-none"><code class="language-none">MyDense(  (params): ParameterList(      (0): Parameter containing: [torch.FloatTensor of size 4x4]      (1): Parameter containing: [torch.FloatTensor of size 4x4]      (2): Parameter containing: [torch.FloatTensor of size 4x4]      (3): Parameter containing: [torch.FloatTensor of size 4x1]  ))</code></pre><ul><li><strong><code>ParameterDict</code></strong></li></ul><p><code>ParameterDict</code> 接收一个 <code>Parameter</code> 实例的字典作为输入然后得到一个参数字典，然后可以按照字典的规则使用了。</p><p>例如使用 <code>update()</code> 新增参数，使用 <code>keys()</code> 返回所有键值，使用 <code>items()</code> 返回所有键值对等等。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MyDictDense</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>MyDictDense<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>params <span class="token operator">=</span> nn<span class="token punctuation">.</span>ParameterDict<span class="token punctuation">(</span><span class="token punctuation">&#123;</span>                <span class="token string">'linear1'</span><span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                <span class="token string">'linear2'</span><span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">&#125;</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>params<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">'linear3'</span><span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span> <span class="token comment"># 新增</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> choice<span class="token operator">=</span><span class="token string">'linear1'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>params<span class="token punctuation">[</span>choice<span class="token punctuation">]</span><span class="token punctuation">)</span>net <span class="token operator">=</span> MyDictDense<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span></code></pre><p>Output:</p><pre class="language-none"><code class="language-none">MyDictDense(  (params): ParameterDict(      (linear1): Parameter containing: [torch.FloatTensor of size 4x4]      (linear2): Parameter containing: [torch.FloatTensor of size 4x1]      (linear3): Parameter containing: [torch.FloatTensor of size 4x2]  ))</code></pre><p>于是我们可以根据不同的 key 进行不同的 forward feeding.</p><pre class="language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token string">'linear1'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token string">'linear2'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token string">'linear3'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.5082</span><span class="token punctuation">,</span> <span class="token number">1.5574</span><span class="token punctuation">,</span> <span class="token number">2.1651</span><span class="token punctuation">,</span> <span class="token number">1.2409</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>MmBackward<span class="token operator">></span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.8783</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>MmBackward<span class="token operator">></span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">2.2193</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.6539</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>MmBackward<span class="token operator">></span><span class="token punctuation">)</span></code></pre><h3 id="读取与存储"><a href="#读取与存储" class="headerlink" title="读取与存储"></a>读取与存储</h3><p>到目前为止，我们介绍了如何处理数据以及如何构建、训练和测试深度学习模型。</p><p>然而在实际中，我们有时需要把训练好的模型部署到很多不同的设备。</p><p>在这种情况下，我们可以把内存中训练好的模型参数存储在硬盘上供后续读取使用。</p><ul><li><strong>读写 <code>Tensor</code></strong></li></ul><p>我们可以直接使用 <code>save</code> 函数和 <code>load</code> 函数分别存储和读取 <code>Tensor</code>。</p><p><code>save</code> 使用 Python 的 pickle 库将对象进行序列化，然后将序列化的对象保存到硬盘。</p><p>使用 <code>save</code> 可以保存各种对象，包括 <code>nn.Module</code>, <code>Tensor</code>, <code>dict</code> 等等。</p><p>而 <code>load</code> 使用 unpickle 工具将 pickle 的对象文件反序列化为内存。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nnx <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token string">'x.pt'</span><span class="token punctuation">)</span>x2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'x.pt'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x2<span class="token punctuation">)</span> <span class="token comment"># tensor([1., 1., 1.])</span>y <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token punctuation">,</span> y<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'xy.pt'</span><span class="token punctuation">)</span>xy_list <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'xy.pt'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>xy_list<span class="token punctuation">)</span> <span class="token comment"># [tensor([1., 1., 1.]), tensor([0., 0., 0., 0.])]</span>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">'x'</span><span class="token punctuation">:</span> x<span class="token punctuation">,</span> <span class="token string">'y'</span><span class="token punctuation">:</span> y<span class="token punctuation">&#125;</span><span class="token punctuation">,</span> <span class="token string">'xy_dict.pt'</span><span class="token punctuation">)</span>xy <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'xy_dict.pt'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>xy<span class="token punctuation">)</span> <span class="token comment"># &#123;'x': tensor([1., 1., 1.]), 'y': tensor([0., 0., 0., 0.])&#125;</span></code></pre><ul><li><strong>读写模型</strong></li></ul><p>PyTorch 中保存和加载训练模型有两种常见的方法:</p><ol><li>仅保存和加载模型参数(<code>state_dict</code>)；</li><li>保存和加载整个模型。</li></ol><p><strong>保存和加载模型的 <code>state_dict()</code> 成员（Recommended）</strong></p><p>保存：</p><pre class="language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> PATH<span class="token punctuation">)</span> <span class="token comment"># 推荐的文件后缀名是 pt 或 pth</span></code></pre><p>加载：</p><pre class="language-python" data-language="python"><code class="language-python">model <span class="token operator">=</span> TheModelClass<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>PATH<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h3 id="GPU-计算"><a href="#GPU-计算" class="headerlink" title="GPU 计算"></a>GPU 计算</h3><p>到目前为止，我们一直在使用 CPU 计算。</p><p>对复杂的神经网络和大规模的数据来说，使用 CPU 来计算可能不够高效。</p><p>在本节中，我们将介绍如何使用单块 NVIDIA GPU 来计算。</p><p>可以通过 <code>nvidia-smi</code> 命令来查看显卡信息。</p><pre class="language-none"><code class="language-none">Wed Jan 26 11:48:28 2022+-----------------------------------------------------------------------------+| NVIDIA-SMI 457.49       Driver Version: 457.49       CUDA Version: 11.1     ||-------------------------------+----------------------+----------------------+| GPU  Name            TCC&#x2F;WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage&#x2F;Cap|         Memory-Usage | GPU-Util  Compute M. ||                               |                      |               MIG M. ||&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;+&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;+&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;||   0  GeForce GTX 1650   WDDM  | 00000000:01:00.0 Off |                  N&#x2F;A || N&#x2F;A   43C    P8     4W &#x2F;  N&#x2F;A |    359MiB &#x2F;  4096MiB |      8%      Default ||                               |                      |                  N&#x2F;A |+-------------------------------+----------------------+----------------------+</code></pre><ul><li><strong>计算设备</strong></li></ul><p>PyTorch 可以指定用来存储和计算的设备，如使用内存的 CPU 或者使用显存的 GPU。</p><p>默认情况下，PyTorch 会将数据创建在内存，然后利用 CPU 来计算。</p><p>用 <code>torch.cuda.is_available()</code> 查看 GPU 是否可用:<br><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nntorch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 输出 True</span></code></pre></p><p>GPU 的相关信息查询：<br><pre class="language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 查看 GPU 数量，输出 1</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>current_device<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 查看当前 GPU 索引号，输出 0</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>get_device_name<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment"># 根据索引号查看 GPU 名字，输出 'GeForce GTX 1050'</span></code></pre></p><ul><li><strong><code>Tensor</code> 的 GPU 计算</strong></li></ul><p>默认情况下，<code>Tensor</code> 会被存在内存上。因此，之前我们每次打印 <code>Tensor</code> 的时候看不到 GPU 相关标识。<br><pre class="language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment"># tensor([1, 2, 3])</span></code></pre><br>使用 <code>.cuda()</code> 可以将CPU上的 <code>Tensor</code> 转换（复制）到GPU上。</p><p>如果有多块GPU，我们用 <code>.cuda(i)</code>来表示第 $i$ 块 GPU 及相应的显存（$i$ 从 0 开始）且 <code>cuda(0)</code> 和 <code>cuda()</code> 等价。</p><pre class="language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> x<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment"># tensor([1, 2, 3], device='cuda:0')</span></code></pre><p>可以通过 <code>Tensor</code> 的 <code>device</code> 属性来查看该 <code>Tensor</code> 所在的设备。<br><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>device<span class="token punctuation">)</span> <span class="token comment"># device(type='cuda', index=0)</span></code></pre><br>可以直接在创建的时候就指定设备。<br><pre class="language-python" data-language="python"><code class="language-python">device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">'cpu'</span><span class="token punctuation">)</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token comment"># or</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment"># tensor([1, 2, 3], device='cuda:0')</span></code></pre><br>如果对在 GPU 上的数据进行运算，那么结果还是存放在 GPU 上。<br><pre class="language-python" data-language="python"><code class="language-python">y <span class="token operator">=</span> x<span class="token operator">**</span><span class="token number">2</span><span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span> <span class="token comment"># tensor([1, 4, 9], device='cuda:0')</span></code></pre><br>需要注意的是，<strong>存储在不同位置中的数据是不可以直接进行计算的</strong>。即存放在 CPU 上的数据不可以直接与存放在 GPU 上的数据进行运算，位于不同 GPU 上的数据也是不能直接进行计算的。</p><ul><li><strong>模型的 GPU 计算</strong></li></ul><p>同 <code>Tensor</code> 类似，PyTorch 模型也可以用类似的方式转移到 GPU 上。</p><ul><li><code>.cuda(i)</code></li><li><code>.cpu()</code></li><li><code>.to(device)</code></li></ul><p>我们也可以通过检查模型的参数的 <code>device</code> 属性来查看存放模型的设备。</p><p>同样的，需要保证模型输入的 <code>Tensor</code> 和模型都在同一设备上，否则会报错。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://tangshusen.me/Dive-into-DL-PyTorch/img/cover.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;《动手学深度学习》原书地址：&lt;a href=&quot;https://github.com/d2l-ai/d2l-zh&quot;&gt;https://github.com/d2l-ai/d2l-zh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;《动手学深度学习》(Pytorch ver.)：&lt;a href=&quot;https://tangshusen.me/Dive-into-DL-PyTorch/#/&quot;&gt;https://tangshusen.me/Dive-into-DL-PyTorch/#/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;知识架构：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tangshusen.me/Dive-into-DL-PyTorch/img/book-org.svg&quot; alt=&quot;封面&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文的主要作用是在阅读过程中做一些摘录。对于「机器学习」领域， c7w 虽然曾尝试从各个领域入门，也尝试训过一些模型，但是还是缺少系统性、结构性的学习。希望阅读本书能带来更多的收获吧。&lt;/p&gt;
&lt;p&gt;与前面的一些笔记相比，本文更加侧重于「实践」。也就是说切实地提升自己的代码能力。&lt;/p&gt;
&lt;p&gt;Part A 包含：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;§ 1 深度学习简介&lt;/li&gt;
&lt;li&gt;§ 2 预备知识：Pytorch&lt;/li&gt;
&lt;li&gt;§ 3 深度学习基础&lt;ul&gt;
&lt;li&gt;线性回归，Softmax 回归，多层感知机三类基本模型&lt;/li&gt;
&lt;li&gt;权重衰减和 Dropout 两类应对过拟合的方法&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;§ 4 深度学习计算&lt;ul&gt;
&lt;li&gt;构造 Pytorch 模型的方式&lt;/li&gt;
&lt;li&gt;模型参数的访问、初始化与共享&lt;/li&gt;
&lt;li&gt;自定义 Layer&lt;/li&gt;
&lt;li&gt;读取与存储&lt;/li&gt;
&lt;li&gt;GPU 计算&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="理论" scheme="https://www.c7w.tech/categories/%E7%90%86%E8%AE%BA/"/>
    
    <category term="理论/机器学习" scheme="https://www.c7w.tech/categories/%E7%90%86%E8%AE%BA/%E7%90%86%E8%AE%BA-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://www.c7w.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>GAMES101 PA 报告兼课程重点算法回顾</title>
    <link href="https://www.c7w.tech/games101-pa/"/>
    <id>https://www.c7w.tech/games101-pa/</id>
    <published>2022-01-18T13:19:54.000Z</published>
    <updated>2022-01-18T15:57:13.019Z</updated>
    
    <content type="html"><![CDATA[<p>// TODO: 这里本来应该有一张图片和文章摘要，但是现在还没写</p><a id="more"></a><h2 id="PA6"><a href="#PA6" class="headerlink" title="PA6"></a>PA6</h2><blockquote><p>在之前的编程练习中，我们实现了基础的光线追踪算法，具体而言是光线传输、光线与三角形求交。</p><p>我们采用了这样的方法寻找光线与场景的交点：<strong>遍历场景中的所有物体，判断光线是否与它相交。</strong></p><p>在场景中的物体数量不大时，该做法可以取得良好的结果，但当物体数量增多、模型变得更加复杂，该做法将会变得非常低效。</p><p>因此，我们需要加速结构来加速求交过程。</p><p>在本次练习中，我们重点关注：<strong>物体划分算法 Bounding Volume Hierarchy (BVH)</strong>。</p><p>本练习要求你实现 Ray-Bounding Volume 求交与 BVH 查找，附加内容为使用 <strong>SAH 加速</strong>算法对 BVH 查找进行改进。</p></blockquote><h3 id="算法回顾"><a href="#算法回顾" class="headerlink" title="算法回顾"></a>算法回顾</h3><h4 id="包围盒求交"><a href="#包围盒求交" class="headerlink" title="包围盒求交"></a>包围盒求交</h4><p>我们将（包围盒）长方体理解成三个对面所分划出的空间的交集。我们一般使用的包围体积便是长方体，且长方体是与坐标轴平行的，即轴对齐包围盒（AABB）。选用 AABB 式包围盒有利于加速计算。判断光线和 AABB 类包围盒的求交方法如下：</p><p><img src="https://s2.loli.net/2022/01/14/5rMlAnIw3gQBVNL.png" alt="image-20220114163501816"></p><p>核心思想：只有当光线进入所有三个对面后，光线才进入了包围盒；若光线从某一个对面出射，则光线便从包围盒中射出。于是我们可以计算三个对面分别对应的 $t_{min}$​​ 和 $t_{max}$​​，然后对其求交。当且仅当 $t_{enter} \lt t_{exit}$​​ 且 $t_{exit} \ge 0$​​，光线才可能与包围盒有交点。</p><h4 id="BVH"><a href="#BVH" class="headerlink" title="BVH"></a>BVH</h4><p>通过对含有物体的向量沿轴划分建立二叉树，并且对于二叉树的每一个结点建立其包围盒。</p><p><img src="https://s2.loli.net/2022/01/16/zbLpGw5E7SCgQAu.png" alt="image-20220116102930688"></p><p>如何进行划分？</p><ul><li>沿着最长的维度来划分</li><li>划分端点取中位数<ul><li>取三角形的重心</li></ul></li></ul><p><img src="https://s2.loli.net/2022/01/16/xtKaWO7lJ2Rm8Xb.png" alt="image-20220116103520101"></p><p>使用 BVH 算法，即使包围盒可能在空间上有相交，但是我们却解决了 kd-Tree 存在的问题。</p><h4 id="SAH-加速"><a href="#SAH-加速" class="headerlink" title="SAH 加速"></a>SAH 加速</h4><p>（BVH）构建过程中最重要的问题就是如何对图元进行划分…划分时要<strong>尽可能减少划分后两部分包围盒重叠的体积</strong>，因为<strong>重叠的体积越大，光线穿过重叠区域的可能性越大，遍历两个子树的可能性就越高，计算消耗越多</strong>。</p><p>因为我们最终的目的是要减少划分后左右子节点重叠的体积，因此一般<strong>在图元跨度最大的坐标轴上进行划分</strong>。这里，图元的跨度可以用图元的包围盒来衡量也可以用图元的质心来衡量。</p><p>两种最简单的划分方法是：</p><ul><li>取坐标轴跨度的中点 $t_{\operatorname{mid}}=\frac{t_{\max }+t_{\min }}{2}$​ ，若节点的坐标小于 $t_{mid}$ 则将其划分到左节点，否则将其划分到右节点（中点划分）。</li><li>最左边的 $\frac n 2$​ 个被划分到左节点，剩下的被划分到右节点（等量划分）。 // 作业框架中 Naive 的划分由此实现</li></ul><p>一种更为常用且效果更好的方法是<strong>基于表面积的启发式评估划分方法</strong>（Surface Area Heuristic，SAH），这种方法通过对求交代价和遍历代价进行评估，给出了每一种划分的代价（Cost），而我们的目的便是去寻找代价最小的划分。</p><p>假设当前节点的包围体中存在 $n$​ 个物体，设对每一个物体求交的代价为 $t(i)$​​ ，如果不做划分依次对其求交则总的代价为：</p><script type="math/tex; mode=display">\sum t(i)=t(1)+t(2)+\cdots+t(n)</script><p>如果这些物体划分为 2 组，这两组物体分别处于它们的包围盒 A 和 B 中。设光线击中它们的概率分别为 $p_A$​​​​​​​ 和 $p_B$​​​​​​ ，需要注意包围盒 A 和 B 之间存在重叠，且它们并不一定会填满其父节点的包围体，因此 $p_A$​​​ 和 $p_B$​​ 的和不一定为1，且它们的和越大说明 A 和 B 的重叠程度越大。综上所述，当前节点求交的代价可以写为：</p><script type="math/tex; mode=display">c(A,B) = p_A \sum_{i \in A}t(i) + p_B \sum_{i \in B}t(i) + t_{traverse}</script><p>其中 $t_{traverse}$​​​ 代表遍历树状结构的代价。一般来说，我们假设对所有图元的求交代价是相同的，可设 $t(i) \equiv 1$​​​，又遍历的代价小于求交的代价，可设 $t_{traverse} = 0.125$​​ 。设包围盒A中图元的个数为 $a$​，B中图元的个数为 $b$，则：</p><script type="math/tex; mode=display">c(A,B) = \frac {S(A)} {S(C)}a + \frac {S(B)} {S(C)}b + 0.125</script><p>SAH 考虑到了图元在空间中的分布也考虑到了子节点包围体的重叠程度，在实际应用中拥有很好的效果。</p><p>然后我们就可以借助一定的辅助空间，在 $O(n)$​ 的时间内完成这个划分过程。这里我们<s>偷懒</s>采用一种 $O(n \log n)$ 的分划方式：</p><ul><li>将物体按照质心顺序排序. $O(n \log n)$</li><li>$\forall k$​，记录 $a_i := \cup_{i \le k} \ S(i)$​. $O(n)$​​</li><li>$\forall k$​，记录 $b_i := \cup_{i \gt k} \ S(i)$​. $O(n)$​</li><li>$\forall k$​，考虑划分 $[0, k] \ || \ [k+1, n)$​，计算其代价并更新最小代价. $O(n)$​</li></ul><p>参考资料：<a href="https://zhuanlan.zhihu.com/p/50720158">https://zhuanlan.zhihu.com/p/50720158</a></p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>采用 SAH 加速前，进行 RT 用时 8 min 35 secs；使用 SAH 加速后，用时为 8 min 48 secs。</p><p>// TODO: 没错用了这 ** 优化又慢了我也不知道为什么总之明天再说吧</p><p>// TODO: 我好困啊我好困啊我好困啊我好困啊我好困啊我好困啊我好困啊我好困啊我好困啊我好困啊我好困啊</p><p><img src="https://s2.loli.net/2022/01/18/Y9Nh7QAUxfncCgV.png" alt="6-1"></p><h3 id="实验框架"><a href="#实验框架" class="headerlink" title="实验框架"></a>实验框架</h3><ul><li><code>main.cpp</code><ul><li>创建场景（1280x960）与预处理<ul><li>导入模型为 MeshTriangle 类对象，并添加入场景；</li><li>将光源添加入场景；</li><li>对场景执行 buildBVH()</li></ul></li><li>创建 <code>Renderer</code> 类对象渲染器，并开始渲染</li></ul></li><li><code>Vector.hpp, Vector.cpp</code>：向量类，提供了向量 <code>Vector3f</code>, <code>Vector2f</code> 的基本操作</li><li><code>Object.hpp</code>：抽象类，提供了以下接口：<ul><li><code>bool intersect(const Ray&amp;)</code></li><li><code>bool intersect(const Ray&amp; ray, float&amp; distance, int&amp; index) const</code></li><li>// TODO: 不想写了 明天 再说吧   </li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;// TODO: 这里本来应该有一张图片和文章摘要，但是现在还没写&lt;/p&gt;</summary>
    
    
    
    <category term="理论" scheme="https://www.c7w.tech/categories/%E7%90%86%E8%AE%BA/"/>
    
    <category term="理论/计算机图形学" scheme="https://www.c7w.tech/categories/%E7%90%86%E8%AE%BA/%E7%90%86%E8%AE%BA-%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
    
    <category term="图形学" scheme="https://www.c7w.tech/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>GAMES101 现代计算机图形学基础 笔记（下）</title>
    <link href="https://www.c7w.tech/games101-cont/"/>
    <id>https://www.c7w.tech/games101-cont/</id>
    <published>2022-01-16T14:27:32.000Z</published>
    <updated>2022-01-18T08:58:53.461Z</updated>
    
    <content type="html"><![CDATA[<p>GAMES 101 现代计算机图形学基础 笔记（下篇）.</p><ul><li>光线追踪（Whitted-style，求交算法，加速结构，辐射度量学，路径追踪）</li><li>材质与外观</li><li>动画与模拟</li></ul><a id="more"></a><h2 id="光线追踪-Ray-Tracing"><a href="#光线追踪-Ray-Tracing" class="headerlink" title="光线追踪 Ray Tracing"></a>光线追踪 Ray Tracing</h2><h3 id="从问题出发"><a href="#从问题出发" class="headerlink" title="从问题出发"></a>从问题出发</h3><p>如何用光栅化的手法解决阴影的问题？</p><p>重要思想：点不在阴影中，当且仅当点必须同时被光源和摄像机看到。</p><ol><li>从光源看向场景（用 Z-buffer’ 记录深度）</li><li>从摄像机看向场景</li><li>如果两深度相同，则无需阴影；若两深度不同，则在阴影中。</li></ol><p>这样做存在的问题：</p><ul><li>Hard shadows（阴影被 0/1 化，无法表示阴影的程度）</li></ul><p>提出光线追踪，就是为了解决光线追踪无法很好地处理<strong>全局效果</strong>的问题。</p><ul><li>Soft Shadows</li><li>Glossy reflection</li><li>Indirect illumination (光线会弹射不止一次)</li></ul><p>光线追踪生成的图片质量很高，但是是离线的算法，运作速度也很慢。</p><h3 id="基础算法"><a href="#基础算法" class="headerlink" title="基础算法"></a>基础算法</h3><h4 id="相关定义"><a href="#相关定义" class="headerlink" title="相关定义"></a>相关定义</h4><ul><li><p>光线</p><ul><li>光线沿直线传播</li><li>光线和光线并不会发生碰撞</li><li>光线总是从光源发出，终止于人的眼睛（我们需要模拟的就是这个过程）<ul><li>根据光路的可逆性，我们可以假设“感知光线”从人眼出发…</li></ul></li></ul></li><li><p>光线投射</p></li></ul><p><img src="https://s2.loli.net/2022/01/14/wJAXG9b1VIEuSLt.png" alt="image-20220114154759095"></p><p>对于我们眼前的成像平面的每一个像素点，从人眼发出一条“感知光线”经过这个像素点，可能会触碰到场景中的某个位置。如果该位置和光源的连线上无其它物体的遮挡，那么我们就可以直接对其进行着色计算。</p><p><img src="https://s2.loli.net/2022/01/14/wCFDdpmHioMkrEL.png" alt="image-20220114155221509"></p><h4 id="Whitted-style-Ray-Tracing"><a href="#Whitted-style-Ray-Tracing" class="headerlink" title="Whitted-style Ray Tracing"></a>Whitted-style Ray Tracing</h4><p><img src="https://s2.loli.net/2022/01/14/T1xAeVvrhOoGJK4.png" alt="image-20220114155905503"></p><p>思路：模拟光线不断弹射的过程，并记录光线在每一个弹射点的衰减率。如上图中，对于玻璃球，需要模拟其反射和折射光路。具体的技术实现见下。</p><h4 id="光线和物体表面交点的求法"><a href="#光线和物体表面交点的求法" class="headerlink" title="光线和物体表面交点的求法"></a>光线和物体表面交点的求法</h4><ul><li>对于隐式表面</li></ul><p>光线的定义：原点 $\vec o$​​，传播方向 $\vec d \ (||\vec d||=1)$​​​，光线定义为 $\vec r(t) = \vec o +t\vec d, \ t\ge0$​​。</p><p>球的定义：$\vec p: (\vec p - \vec c)^2 - R^2 = 0$.</p><script type="math/tex; mode=display">\begin{aligned}&(\mathbf{o}+t \mathbf{d}-\mathbf{c})^{2}-R^{2}=0 \\&a t^{2}+b t+c=0, \text { where } \\&a=\mathbf{d} \cdot \mathbf{d} \\&b=2(\mathbf{o}-\mathbf{c}) \cdot \mathbf{d} \\&c=(\mathbf{o}-\mathbf{c}) \cdot(\mathbf{o}-\mathbf{c})-R^{2} \\&t=\frac{-b \pm \sqrt{b^{2}-4 a c}}{2 a}\end{aligned}</script><ul><li>对于显式表面</li></ul><p>一个简单的想法是对模型的所有三角形表面做遍历，然后选择 $t$ 最小的那个。（可能不存在，可以优化*）</p><p>于是，接下来我们仅考虑光线如何与单个三角形求交。</p><p>定义平面为 $(p-p’) \cdot N = 0$.</p><script type="math/tex; mode=display">\begin{aligned}&\left(\mathbf{p}-\mathbf{p}^{\prime}\right) \cdot \mathbf{N}=\left(\mathbf{o}+t \mathbf{d}-\mathbf{p}^{\prime}\right) \cdot \mathbf{N}=0 \\&t=\frac{\left(\mathbf{p}^{\prime}-\mathbf{o}\right) \cdot \mathbf{N}}{\mathbf{d} \cdot \mathbf{N}} \quad \text { Check: } 0 \leq t<\infty\end{aligned}</script><p>然后判断交点是否在三角形内。</p><p>此外还有 Möller Trumbore 算法，可以直接判断交点是否在三角形内（需要用到克莱姆法则）：</p><script type="math/tex; mode=display">\begin{gathered}\overrightarrow{\mathbf{O}}+t \overrightarrow{\mathbf{D}}=\left(1-b_{1}-b_{2}\right) \overrightarrow{\mathbf{P}}_{0}+b_{1} \overrightarrow{\mathbf{P}}_{1}+b_{2} \overrightarrow{\mathbf{P}}_{2} \\\text { Where: } \\{\left[\begin{array}{c}t \\b_{1} \\b_{2}\end{array}\right]=\frac{1}{\overrightarrow{\mathbf{S}}_{1} \bullet \overrightarrow{\mathbf{E}}_{1}}\left[\begin{array}{cc}\overrightarrow{\mathbf{S}}_{2} \cdot \overrightarrow{\mathbf{E}}_{2} \\\overrightarrow{\mathbf{S}}_{1} \cdot \overrightarrow{\mathbf{S}} \\\overrightarrow{\mathbf{S}}_{2} \cdot \overrightarrow{\mathbf{D}}\end{array}\right]} \\\text { Cost = (1 div, 27 mul, 17 add) } \\\overrightarrow{\mathbf{E}}_{1}=\overrightarrow{\mathbf{P}}_{1}-\overrightarrow{\mathbf{P}}_{0} \\\overrightarrow{\mathbf{E}}_{2}=\overrightarrow{\mathbf{P}}_{2}-\overrightarrow{\mathbf{P}}_{0} \\\overrightarrow{\mathbf{S}}=\overrightarrow{\mathbf{O}}-\overrightarrow{\mathbf{P}}_{0} \\\overrightarrow{\mathbf{S}}_{1}=\overrightarrow{\mathbf{D}} \times \overrightarrow{\mathbf{E}}_{2} \\\overrightarrow{\mathbf{S}}_{2}=\overrightarrow{\mathbf{S}} \times \overrightarrow{\mathbf{E}}_{1}\end{gathered}</script><p>该怎样加速与模型求交呢(*)？</p><ul><li><p>Naive algorithm = #pixels ⨉ # traingles (⨉ #bounces) 太慢了，不能满足我们的需求</p></li><li><p>Improved Algorithm: Bounding Volume</p><ul><li>这里我们仍然引入“包围盒”的思想，这里称为 Bounding Volume</li><li>光线能够与模型有交点的必要条件：光线能够与包围模型的一个简单包围体积有交点</li></ul></li></ul><p><img src="https://s2.loli.net/2022/01/14/gtSeZD1FWrcXQCs.png" alt="image-20220114163023985"></p><p>我们将长方体理解成三个对面所分划出的空间的交集。我们一般使用的包围体积便是长方体，且长方体是与坐标轴平行的，即轴对齐包围盒（AABB）。选用 AABB 式包围盒有利于加速计算。判断光线和 AABB 类包围盒的求交方法如下：</p><p><img src="https://s2.loli.net/2022/01/14/5rMlAnIw3gQBVNL.png" alt="image-20220114163501816"></p><p>核心思想：只有当光线进入所有三个对面后，光线才进入了包围盒；若光线从某一个对面出射，则光线便从包围盒中射出。于是我们可以计算三个对面分别对应的 $t_{min}$​​ 和 $t_{max}$​​，然后对其求交。当且仅当 $t_{enter} \lt t_{exit}$​​ 且 $t_{exit} \ge 0$​​，光线才可能与包围盒有交点。</p><h4 id="使用包围盒加速光线追踪"><a href="#使用包围盒加速光线追踪" class="headerlink" title="使用包围盒加速光线追踪"></a>使用包围盒加速光线追踪</h4><h5 id="Uniform-Grids"><a href="#Uniform-Grids" class="headerlink" title="Uniform Grids"></a>Uniform Grids</h5><p>① 预处理</p><p><img src="https://s2.loli.net/2022/01/14/Ggt2BDyranQYdCh.png" alt="image-20220114165308910"></p><p>一般来说，取格子数 #cells = #objs * 27.</p><p>② 光线与场景中模型的求交</p><p><img src="https://s2.loli.net/2022/01/14/qTfEAhQo5OXLcgW.png" alt="image-20220114165353415"></p><p>存在的问题：Teapot in a Stadium，事实上物体模型在空间中的分布可能并不均匀！</p><h5 id="空间划分-Spatial-Partitions"><a href="#空间划分-Spatial-Partitions" class="headerlink" title="空间划分 Spatial Partitions"></a>空间划分 Spatial Partitions</h5><ul><li>Oct-Tree</li><li>Kd-Tree</li><li>BSP-Tree</li></ul><p>这里以 Kd-Tree 为例，Kd-Tree 的具体实现在《数据结构》课程中已详细介绍过，这里便不再赘述。我们将空间模型首先使用 Kd-Tree 预处理，然后针对不同的光线（Query），进行如下操作：</p><ul><li>首先判断光线和最外层包围盒是否有交点，如果有那么：<ul><li>如果包围盒为叶子结点，直接判断光线和包围盒中模型是否相交并求交；</li><li>如果包围盒非叶子结点，那么对其两个子节点的包围盒，分别判断其是否与光线有交，若有则递归地执行此过程。</li></ul></li></ul><p>kd-Tree 存在的问题：</p><ul><li>难以判断实际的物体和包围盒是否相交<ul><li>即使是对于物体全部为三角形的场景，也难以判断单个包围盒和单个三角形是否有交点！</li></ul></li><li>此外，一个物体可能出现在多个叶子结点中…</li></ul><h5 id="Object-Partitions"><a href="#Object-Partitions" class="headerlink" title="Object Partitions"></a>Object Partitions</h5><ul><li>Bounding volume Hierarchy (BVH)</li></ul><p>我们划分的不是空间，而是物体！</p><p><img src="https://s2.loli.net/2022/01/16/zbLpGw5E7SCgQAu.png" alt="image-20220116102930688"></p><p>如何进行划分？</p><ul><li>沿着最长的维度来划分</li><li>划分端点取中位数<ul><li>取三角形的重心</li></ul></li></ul><p><img src="https://s2.loli.net/2022/01/16/xtKaWO7lJ2Rm8Xb.png" alt="image-20220116103520101"></p><p>使用 BVH 算法，即使包围盒可能在空间上有相交，但是我们却解决了 kd-Tree 存在的问题。</p><h4 id="辐射度量学"><a href="#辐射度量学" class="headerlink" title="辐射度量学"></a>辐射度量学</h4><p>辐射度量学提供了精准地描述光这个物理量的方法。</p><ul><li>光照的度量方法和单位</li><li>精确地度量光的时空属性<ul><li>Radiant Flux 辐射通量</li><li>Radiant Intensity 辐射强度</li><li>Irradiance 辐射照度</li><li>Radiance 辐射亮度</li></ul></li><li>使用物理正确的方法来计算光照</li></ul><h5 id="物理量的定义"><a href="#物理量的定义" class="headerlink" title="物理量的定义"></a>物理量的定义</h5><ul><li><strong>Radiant Energy</strong> $Q$ (辐射能量，单位 J)</li><li><strong>Radiant Flux</strong> (又名 <strong>Power</strong>) $\Phi = \frac {dQ} {dt}$​ （辐射通量，单位 W, lm）</li></ul><p><img src="https://s2.loli.net/2022/01/16/z41TFClAOsMcS9I.png" alt="image-20220116104840613"></p><ul><li><strong>Radiant Intensity</strong> $I(\omega) = \frac {d \Phi} {d \omega}$，其中 $\omega$​​ 是立体角，单位为 lm/sr =: cd (坎德拉)，每单位立体角的功率</li></ul><blockquote><p>立体角的定义：</p><p><img src="https://s2.loli.net/2022/01/16/la1fuCOdi9mKokN.png" alt="image-20220116105159216"></p></blockquote><p>特别地，若光源均匀辐射，则我们有 $I = \frac {\Phi} {4\pi}$​。</p><ul><li><strong>Irradiance</strong> $E(x) = \frac { d \Phi(x)} {dA}$，辐照度，单位面积上的辐射通量，单位 lux.</li></ul><p><img src="https://s2.loli.net/2022/01/16/uXeymz4bKvgBPiV.png" alt="image-20220116110821261"></p><ul><li><strong>Radiance</strong> $L(p, \omega) = \frac {d^2\Phi(p, \omega)} {d \omega dA \cos \theta}$​</li></ul><p><img src="https://s2.loli.net/2022/01/16/w8AWzY3Cs1HkaZP.png" alt="image-20220116111154506"></p><p>辐射度(亮度)是每单位立体角和每单位投影面积上，由表面反射、发射或接收的能量。辐射度是光线的属性。</p><ul><li>Radiance is power per solid angle per projected unit area;</li><li>Irradiance is power per projected unit area;</li><li>Intensity is power per solid angle;</li><li>That is to say…</li><li>Radiance is intensity per projected unit area;</li><li>Radiance is irradiance per solid angle, 也就是说，irradiance 是一个表面 $dA \cos \theta$接收到的能量，radiance 是该表面朝着某个 $\omega$ 立体角方向辐射出去或接收到的能量，后者具有表明方向的能力。</li></ul><h4 id="双向反射分布函数（BRDF）"><a href="#双向反射分布函数（BRDF）" class="headerlink" title="双向反射分布函数（BRDF）"></a>双向反射分布函数（BRDF）</h4><ul><li>BRDF := Bidirectional Reflectance Distribution Function</li></ul><p>什么是反射？反射可以看做是物体表面吸收了照射到该处的所有能量，然后再辐射出去的一个过程。</p><p>BRDF 这个分布函数是用于描述，对于物体表面一个小面积 $dA$，接收到的来自于立体角 $d \omega_i$ 的 irradiance，会以怎样的方式被辐射出去，分布于各个 solid angle 中。即：$BRDF := \frac {dL_r({\omega _r)}} {L(w_i) \cos \theta \ d\omega_i} \rightarrow percentage$​. 即：朝某个方向的辐射度占总照度的比例。</p><p><img src="https://s2.loli.net/2022/01/16/QCrPA8qnHXOhu2W.png" alt="image-20220116114348861"></p><p>使用 BRDF 可以用来定义镜面反射和漫反射。此外，BRDF 还可以用来定义物体表面的材质。</p><p>于是，借助于 BRDF 的定义，我们考虑真实的光线传播。从某个点 $dA$ 向 $\omega_r$ 方向出射的 radiance，可以通过考虑所有 $w_i$ 到达这个点的照度乘以其对应的 BRDF 占比，然后求和得到。</p><p><img src="https://s2.loli.net/2022/01/16/xbeovLICaJnK7sj.png" alt="image-20220116115205963"></p><p>问题：我们考虑的入射 Radiance 可能不仅仅由光源发出，也可能是由其他物体先经过若干次反射得到…</p><h4 id="Rendering-Equation"><a href="#Rendering-Equation" class="headerlink" title="Rendering Equation"></a>Rendering Equation</h4><script type="math/tex; mode=display">L_{o}\left(p, \omega_{o}\right)=L_{e}\left(p, \omega_{o}\right)+\int_{\Omega^{+}} L_{i}\left(p, \omega_{i}\right) f_{r}\left(p, \omega_{i}, \omega_{o}\right)\left(n \cdot \omega_{i}\right) \mathrm{d} \omega_{i}</script><p>经过推导之后，我们得出：</p><p><img src="https://s2.loli.net/2022/01/16/gFjCZGfP3VmcYTs.png" alt="image-20220116151021069"></p><p>全局光照 L = 直接光照 E 和间接光照的集合，其中 K 是反射算符。</p><h4 id="路径追踪"><a href="#路径追踪" class="headerlink" title="路径追踪"></a>路径追踪</h4><blockquote><p><strong>蒙特卡洛积分</strong></p><p>目的是为了解决定积分 $\int_a^bf(x)dx$ 问题。</p><p>做法是在 $[a,b]$​ 间随机采样足够多次，然后将足够多次的结果进行平均后作为函数均值积分。</p><p>例如，我们使用均匀采样的方式：</p><p><img src="https://s2.loli.net/2022/01/16/4MltFzimgEdkbVR.png" alt="image-20220116153649942"></p><p>更一般地，我们有：</p><script type="math/tex; mode=display">\int f(x) \mathrm{d} x=\frac{1}{N} \sum_{i=1}^{N} \frac{f\left(X_{i}\right)}{p\left(X_{i}\right)} \quad X_{i} \sim p(x)</script></blockquote><p>回忆 Whitted-style ray tracing:</p><ul><li>可以做镜面反射和折射的效果</li><li>遇到漫反射平面便停止</li></ul><p>使用渲染方程，我们可以有以下推导过程：</p><p>对于非光源表面 $dA$​​​，我们有：</p><script type="math/tex; mode=display">L_{o}\left(p, \omega_{o}\right)=\int_{\Omega^{+}} L_{i}\left(p, \omega_{i}\right) f_{r}\left(p, \omega_{i}, \omega_{o}\right)\left(n \cdot \omega_{i}\right) \mathrm{d} \omega_{i}</script><p>对 $w_i$​​ 进行 Sample，且 Sample 均匀分布于 $[0, 2\pi]$​​。于是我们可以得出：</p><script type="math/tex; mode=display">L_{o}\left(p, \omega_{o}\right) \approx \frac{1}{N} \sum_{i=1}^{N} \frac{L_{i}\left(p, \omega_{i}\right) f_{r}\left(p, \omega_{i}, \omega_{o}\right)\left(n \cdot \omega_{i}\right)}{p\left(\omega_{i}\right)}</script><p>进而，我们可以提出一种着色算法：</p><p><img src="https://s2.loli.net/2022/01/16/QVLKN1BoPISCDZE.png" alt="image-20220116164125480"></p><p>然而，我们如何继续引入全局光照的概念呢？很简单，只需要考虑如果光线 $r$ 命中某个物体时的情况。也就是说，将该物体表面视为新的光源：</p><p><img src="https://s2.loli.net/2022/01/16/qEcxeKGIJSw1lLb.png" alt="image-20220116164724144"></p><p>我们只需要计算 $q$ 处，$-w_i$ 方向发射的 Radiance，便可以得到渲染结果。</p><p>但是这样做带来的问题是，光线的数目会发生指数爆炸，于是我们只能在采样时取 N=1，即在计算蒙特卡洛积分时取 N=1，然后仅追踪一根光线的情况。</p><p><img src="https://s2.loli.net/2022/01/16/rxEkeSplq6doBgQ.png" alt="image-20220116164911441"></p><p>我们便将这种 N=1 的处理方式称为“路径追踪”。</p><p>为了降低 Noise，我们可以对像平面上的每个像素都追踪多条这样的“路径”，然后将这些路径计算得到的 Radiance 取平均值即可。</p><p><img src="https://s2.loli.net/2022/01/16/gbk2BK9LGTjZ1PV.png" alt="image-20220116165143374"></p><p>此外，我们还需要设置 shade 过程的递归基。为此，我们可以设置最大光线弹射次数，但这样取得的效果不如使用 <code>Russian Roulette</code>（俄罗斯轮盘赌）。也就是说，对于某个点的着色结果，我们给定 $p$ 的概率返回 $result_{old} / p$，$1-p$ 的概率返回 $0$​，这样做我们着色结果的期望仍是 $result_{old}$！</p><p><img src="https://s2.loli.net/2022/01/16/7g4DHb8rNKmeC1M.png" alt="image-20220116165744374"></p><p>目前我们就得到了路径追踪算法的正确版本，即对于每个像平面上的像素点调用 ray_generation 过程，而 shade 过程则加入了这样的几何分布方式来确定光线是否能够继续生存。但是目前来说，我们的算法可能在效率上并不高，在低采样率（SPP, Samples per pixel）的情况下效果并不好：</p><p><img src="https://s2.loli.net/2022/01/16/1M76Bj8Q9lnav3K.png" alt="image-20220116170133804"></p><p>这是因为，如果我们的光源过小，我们需要足够多次采样，才有可能将“追踪用光线”最终命中光源。也就是说，大部分的追踪路径都是无效的：</p><p><img src="https://s2.loli.net/2022/01/16/1j7Tk6HnRyMsVD5.png" alt="image-20220116170327150"></p><p>或许我们能找到更合适的采样概率分布…比如，<strong>对光源采样</strong>！</p><p><img src="https://s2.loli.net/2022/01/16/s8yVDHI3JiXBFeU.png" alt="image-20220116190243440"></p><p>于是：</p><script type="math/tex; mode=display">\begin{aligned}L_{o}\left(x, \omega_{o}\right) &=\int_{\Omega^{+}} L_{i}\left(x, \omega_{i}\right) f_{r}\left(x, \omega_{i}, \omega_{o}\right) \cos \theta \mathrm{d} \omega_{i} \\&=\int_{A} L_{i}\left(x, \omega_{i}\right) f_{r}\left(x, \omega_{i}, \omega_{o}\right) \frac{\cos \theta \cos \theta^{\prime}}{\left\|x^{\prime}-x\right\|^{2}} \mathrm{~d} A\end{aligned}</script><p>在之前我们假设光线可以在采样半球面上均匀射出，而现在我们将点 $dA$ 处的 Radiance 分两部分来考虑：</p><ol><li>光源的直接贡献（不需要 RR 计算）</li><li>其他表面的反射光的贡献（使用 RR）</li></ol><p>这样考虑后，我们得出改进版的路径追踪伪代码如下：</p><p><img src="https://s2.loli.net/2022/01/16/zMDhcL3almKIr6t.png" alt="image-20220116190636646"></p><p>此外，我们还需要考虑光源发出的光束是否可能被障碍物遮挡…</p><p><img src="https://s2.loli.net/2022/01/16/OEXG9d2tikuHncl.png" alt="image-20220116190917657"></p><p>最后，路径追踪的伪代码整理如下：</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">ray_generation</span><span class="token punctuation">(</span>camPos<span class="token punctuation">,</span> pixel<span class="token punctuation">)</span><span class="token punctuation">:</span>Uniformly choose N samples <span class="token keyword">from</span> the pixelpixel_radiance <span class="token operator">=</span> <span class="token number">0.0</span><span class="token keyword">for</span> sample <span class="token keyword">in</span> the pixel<span class="token punctuation">:</span>Shoot a ray r<span class="token punctuation">(</span>camPos<span class="token punctuation">,</span> cam_to_sample<span class="token punctuation">)</span><span class="token keyword">if</span> ray r hit the scene at p<span class="token punctuation">:</span>pixel_radiance <span class="token operator">+=</span> <span class="token number">1</span> <span class="token operator">/</span> N <span class="token operator">*</span> shade<span class="token punctuation">(</span>p<span class="token punctuation">,</span> sample_to_cam<span class="token punctuation">)</span>    <span class="token keyword">return</span> pixel_radiance<span class="token keyword">def</span> <span class="token function">shade</span><span class="token punctuation">(</span>p<span class="token punctuation">,</span> wo<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment"># Contributions from the light source</span>L_dir <span class="token operator">=</span> <span class="token number">0.0</span>Uniformly sample the light at x' <span class="token punctuation">(</span>pdf_light <span class="token operator">=</span> <span class="token number">1</span><span class="token operator">/</span>A<span class="token punctuation">)</span>Shoot a ray <span class="token keyword">from</span> p to x'<span class="token keyword">if</span> the ray <span class="token keyword">is</span> <span class="token keyword">not</span> blocked <span class="token keyword">in</span> the middle<span class="token punctuation">:</span>L_dir <span class="token operator">=</span> L_i <span class="token operator">*</span> f_r <span class="token operator">*</span> cos \theta <span class="token operator">*</span> cos \theta<span class="token string">' / |x'</span><span class="token operator">-</span>p<span class="token operator">|</span><span class="token operator">^</span><span class="token number">2</span> <span class="token operator">/</span> pdf_light        <span class="token comment"># Contributions from other places</span>    L_indir <span class="token operator">=</span> <span class="token number">0.0</span>    If test Russian Roulette <span class="token keyword">with</span> probability P_RR <span class="token punctuation">:</span>        Uniformly sample the hemisphere toward wi <span class="token punctuation">(</span>pdf_hemi <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> 2pi<span class="token punctuation">)</span>        Trace a ray r<span class="token punctuation">(</span>p<span class="token punctuation">,</span> wi<span class="token punctuation">)</span>        If ray r hit a non<span class="token operator">-</span>emitting <span class="token builtin">object</span> at q<span class="token punctuation">:</span>            L_indir <span class="token operator">=</span> shade<span class="token punctuation">(</span>q<span class="token punctuation">,</span> <span class="token operator">-</span>wi<span class="token punctuation">)</span> <span class="token operator">*</span> f_r <span class="token operator">*</span> cos \theta <span class="token operator">/</span> pdf_hemi <span class="token operator">/</span> P_RR        Return L_dir <span class="token operator">+</span> L_indir</code></pre><h2 id="材质与外观-Materials-and-Appearance"><a href="#材质与外观-Materials-and-Appearance" class="headerlink" title="材质与外观 Materials and Appearance"></a>材质与外观 Materials and Appearance</h2><ul><li>Material == BRDF!</li></ul><h3 id="一些材质的例子"><a href="#一些材质的例子" class="headerlink" title="一些材质的例子"></a>一些材质的例子</h3><h4 id="漫反射材料-Diffuse-Lambertian-Material"><a href="#漫反射材料-Diffuse-Lambertian-Material" class="headerlink" title="漫反射材料 Diffuse/Lambertian Material"></a>漫反射材料 Diffuse/Lambertian Material</h4><p>假设入射光的 Radiance 均匀分布：</p><script type="math/tex; mode=display">\begin{aligned}L_{o}\left(\omega_{o}\right) &=\int_{H^{2}} f_{r} L_{i}\left(\omega_{i}\right) \cos \theta_{i} \mathrm{~d} \omega_{i} \\&=f_{r} L_{i} \int_{H^{2}}\left(\omega_{i}\right) \cos \theta_{i} \mathrm{~d} \omega_{i} \\&=\pi f_{r} L_{i} \\f_{r}=\frac{\rho}{\pi} &-\text { albedo (color) }\end{aligned}</script><p>这里 $\rho$ 可以是常数（单通道），也可以是针对不同通道定义了不同数值的向量。</p><h4 id="抛光的金属-Glossy-Material"><a href="#抛光的金属-Glossy-Material" class="headerlink" title="抛光的金属 Glossy Material"></a>抛光的金属 Glossy Material</h4><p><img src="https://s2.loli.net/2022/01/17/9xZHgA8u6icXKyo.png" alt="image-20220117103604496"></p><h4 id="理想反射-折射材质"><a href="#理想反射-折射材质" class="headerlink" title="理想反射/折射材质"></a>理想反射/折射材质</h4><p><img src="https://s2.loli.net/2022/01/17/RgoxIXL2qr3MaD7.png" alt="image-20220117103654766"></p><ul><li>反射：反射定律</li><li>折射：折射定律 Snell’s Law，注意全反射的情况</li><li>能量分配：菲涅尔公式 Fresnel Term</li></ul><blockquote><p>精确的菲涅尔公式：</p><script type="math/tex; mode=display">\begin{aligned}&R_{\mathrm{s}}=\left|\frac{n_{1} \cos \theta_{\mathrm{i}}-n_{2} \cos \theta_{\mathrm{t}}}{n_{1} \cos \theta_{\mathrm{i}}+n_{2} \cos \theta_{\mathrm{t}}}\right|^{2}=\left|\frac{n_{1} \cos \theta_{\mathrm{i}}-n_{2} \sqrt{1-\left(\frac{n_{1}}{n_{2}} \sin \theta_{\mathrm{i}}\right)^{2}}}{n_{1} \cos \theta_{\mathrm{i}}+n_{2} \sqrt{1-\left(\frac{n_{1}}{n_{2}} \sin \theta_{\mathrm{i}}\right)^{2}}}\right|^{2} \\&R_{\mathrm{p}}=\left|\frac{n_{1} \cos \theta_{\mathrm{t}}-n_{2} \cos \theta_{\mathrm{i}}}{n_{1} \cos \theta_{\mathrm{t}}+n_{2} \cos \theta_{\mathrm{i}}}\right|^{2}=\left|\frac{n_{1} \sqrt{1-\left(\frac{n_{1}}{n_{2}} \sin \theta_{\mathrm{i}}\right)^{2}}-n_{2} \cos \theta_{\mathrm{i}}}{n_{1} \sqrt{1-\left(\frac{n_{1}}{n_{2}} \sin \theta_{\mathrm{i}}\right)^{2}}+n_{2} \cos \theta_{\mathrm{i}}}\right|^{2} \\& R_{eff} = \frac 1 2 (R_s + R_p)\end{aligned}</script></blockquote><p>近似后：</p><script type="math/tex; mode=display">\begin{aligned}R(\theta) &=R_{0}+\left(1-R_{0}\right)(1-\cos \theta)^{5} \\R_{0} &=\left(\frac{n_{1}-n_{2}}{n_{1}+n_{2}}\right)^{2}\end{aligned}</script><h4 id="微表面模型-Microfacet-Material"><a href="#微表面模型-Microfacet-Material" class="headerlink" title="微表面模型 Microfacet Material"></a>微表面模型 Microfacet Material</h4><p>虽然物体的表面是粗糙的，但是若从远处看，则可以将物体表面视为是平的。即对于粗糙表面来说：</p><ul><li>从远处看可以视为是平的粗糙表面</li><li>从近处看看到的是高低起伏的高光表面</li></ul><p>从远处看看到的是材质，从近处看看到的是几何。</p><p><img src="https://s2.loli.net/2022/01/17/8ESojAlrTM1sptY.png" alt="image-20220117110551868"></p><p>我们考虑这些微表面的法线的分布情况。如果其分布方差较小，则表面是 Glossy 的；而如果其分布方差较大，我们可以视其表面为 Diffuse 型。</p><p><img src="https://s2.loli.net/2022/01/17/n8QueMgHco16W5R.png" alt="image-20220117111415698"></p><h4 id="各向同性与各向异性材质"><a href="#各向同性与各向异性材质" class="headerlink" title="各向同性与各向异性材质"></a>各向同性与各向异性材质</h4><p>这是一种分类材质的方式。各向同性材质的微表面的法线分布并不具有明确的方向性，而各向异性材质的法线分布却具有，因而后者具有一些特殊的性质。</p><p>从定义上来说，后者的 BRDF 与其绝对方位角 $\Phi$ 有关。</p><h3 id="BRDF-的性质与其测量"><a href="#BRDF-的性质与其测量" class="headerlink" title="BRDF 的性质与其测量"></a>BRDF 的性质与其测量</h3><blockquote><p>BRDF 的性质：</p><ul><li>Non-negativity</li><li>Linearity</li><li>Reciprocity principle： $f_r(w_r \rightarrow w_i) = f_r(w_i \rightarrow w_r)$</li><li>Energy conservation</li><li>Isotropic or anisotropic</li></ul></blockquote><p>测量方法：</p><p><img src="https://s2.loli.net/2022/01/17/cegxist9jRBoTdb.png" alt="image-20220117112601733"></p><ul><li>MERL BRDF Database</li></ul><h2 id="知识补完"><a href="#知识补完" class="headerlink" title="知识补完"></a>知识补完</h2><h3 id="相机、棱镜与光场"><a href="#相机、棱镜与光场" class="headerlink" title="相机、棱镜与光场"></a>相机、棱镜与光场</h3><ul><li>成像 Imaging = Synthesis 合成 + Capture 捕捉</li></ul><h4 id="FOV"><a href="#FOV" class="headerlink" title="FOV"></a>FOV</h4><ul><li>小孔成像：针孔相机</li></ul><p><img src="https://s2.loli.net/2022/01/17/NtqCuUidJ1kSafb.png" alt="image-20220117162312506"></p><p>视场 $FOV = 2 \arctan(\frac {h} {2f})$​，一般我们取 $h=36 * 24mm$ 时 $f$ 的大小作为 $FOV$ 的大小。</p><h4 id="Exposure"><a href="#Exposure" class="headerlink" title="Exposure"></a>Exposure</h4><ul><li>曝光 Exposure = Time * Irradiance</li></ul><p>其中 Time 由快门控制，能量由光圈的大小和焦距决定。此外，ISO 感光度可以视为是后期处理，给感光的多少进行倍增。这些因素都能影响成像的亮度。</p><p><img src="https://s2.loli.net/2022/01/17/73moA5gzpaDlFU9.png" alt="image-20220117163652644"></p><p>原理：ISO 作为后期处理，对于含有噪声的信号，同时将信号放大，噪声也随之被放大，因此会显得 Noisy；F-Number(F-Stop) 一般写作 F<strong>N</strong> 或 F/<strong>N</strong>。这里的 N 可以近似理解为光圈直径的倒数。快门速度变快也会降低 Exposure，而变慢会产生模糊。</p><h4 id="薄透镜-Thin-lens"><a href="#薄透镜-Thin-lens" class="headerlink" title="薄透镜 Thin lens"></a>薄透镜 Thin lens</h4><p>平行光入射，出射光过焦点；过焦点光入射，出射光平行；焦距可任意改变；$\frac 1 f = \frac 1 {z_i} + \frac 1 {z_o}$​。</p><p>利用薄透镜可以解释景深的问题：</p><p><img src="https://s2.loli.net/2022/01/17/r8hei9FOAUZWb7P.png" alt="image-20220117165250088"></p><h4 id="光场-Light-Field-Lumigraph"><a href="#光场-Light-Field-Lumigraph" class="headerlink" title="光场 Light Field / Lumigraph"></a>光场 Light Field / Lumigraph</h4><ul><li>全光函数：我们看到的世界是七维函数</li></ul><p><img src="https://s2.loli.net/2022/01/17/OWevpzGJ1g7kVf4.png" alt="image-20220117171128615"></p><ul><li>光场是全光函数在位置集合上的限制，给定任何一个位置和任何一个方向，输出光线的强度。<ul><li>要想描述一个发光物体对于任何位置任何方向的辐射贡献，我们可以考虑用一个包围盒罩住这个物体，只需要知道这个包围盒上任意一点对于任意方向的辐射贡献，就可以用于替代这个物体的辐射贡献。</li><li>也就是说，对于任何一个物体，我们作全光函数在其包围盒上的限制，只要弄清楚其包围盒上任意一点对于任意方向的光照辐射，就可以用于替代这个发光物体。这样做方便我们进行 Query.</li><li>U-V 平面与 S-T 平面</li><li>光场照相机</li></ul></li></ul><h3 id="颜色与感知"><a href="#颜色与感知" class="headerlink" title="颜色与感知"></a>颜色与感知</h3><ul><li>谱功率密度（SPD），具有线性可加性；</li><li><strong>颜色</strong>事实上是人的一种感知，它并不是光的一种属性；</li><li>人感光的生物基础：视网膜上的 Cone 型细胞</li><li>同色异谱现象的存在</li><li>颜色的混合与匹配<ul><li>加色系统</li></ul></li><li>颜色空间与色域，RGB, XYZ, HSV, Lab, CMYK</li></ul><h2 id="动画与模拟"><a href="#动画与模拟" class="headerlink" title="动画与模拟"></a>动画与模拟</h2><h3 id="质点弹簧系统"><a href="#质点弹簧系统" class="headerlink" title="质点弹簧系统"></a>质点弹簧系统</h3><ul><li>质点弹簧系统：一系列相互连接的质点和弹簧</li></ul><script type="math/tex; mode=display">\boldsymbol{f}_{a \rightarrow b}=k_{s} \frac{\boldsymbol{b}-\boldsymbol{a}}{\|\boldsymbol{b}-\boldsymbol{a}\|}(\|\boldsymbol{b}-\boldsymbol{a}\|-l) \\f_{b \rightarrow a} = - f_{ a \rightarrow b}</script><p>我们再引入内部损耗摩擦力以让该系统可以停下来：</p><script type="math/tex; mode=display">\boldsymbol{f}_{\boldsymbol{b}}=-k_{d} \ \left( \frac{\boldsymbol{b}-\boldsymbol{a}}{\|\boldsymbol{b}-\boldsymbol{a}\|} \cdot(\dot{\boldsymbol{b}}-\dot{\boldsymbol{a}}) \right)\  \frac{\boldsymbol{b}-\boldsymbol{a}}{\|\boldsymbol{b}-\boldsymbol{a}\|}</script><ul><li>质点弹簧系统组成的结构：<ul><li>Sheet / Block / …</li><li>下图为用质点弹簧系统对布料的模拟：</li></ul></li></ul><p><img src="https://s2.loli.net/2022/01/18/nKJWEFLij1BHOXz.png" alt="image-20220118112053335"></p><h3 id="粒子系统"><a href="#粒子系统" class="headerlink" title="粒子系统"></a>粒子系统</h3><p>把我们考虑的物体细分成粒子表示，然后在动画的每一帧中：</p><ul><li>(如果需要) 创建新粒子</li><li>计算每个粒子上的作用力</li><li>更新每个粒子的位置和速度</li><li>（如果需要）移除某些例子</li><li>渲染该帧</li></ul><h3 id="运动学"><a href="#运动学" class="headerlink" title="运动学"></a>运动学</h3><ul><li>正向运动学：通过定义各个部件之间的连接方式，给定参数（如 $l, \theta$），即可计算相应部件的位置</li><li>逆运动学：可以通过拖拽等方式改变最终目标部件的位置，中间的各个部件的情况自动计算得出</li></ul><blockquote><p>逆运动学的应用： Rigging</p><p>Rigging 是对于一个角色的控制，就像是提线木偶一样，改变角色的姿态或表情…</p><p>不同角色的 Rigging 是不同的…</p></blockquote><h3 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h3><ul><li><strong>Euler’s Method</strong>：简单的迭代法，十分不精确且不稳定</li></ul><script type="math/tex; mode=display">\begin{aligned}&\boldsymbol{x}^{t+\Delta t}=\boldsymbol{x}^{t}+\Delta t \dot{\boldsymbol{x}}^{t} \\&\dot{\boldsymbol{x}}^{t+\Delta t}=\dot{\boldsymbol{x}}^{t}+\Delta t \ddot{\boldsymbol{x}}^{t}\end{aligned}</script><p>用数值迭代的方法解 ODE 存在的问题：</p><ul><li>误差：随着迭代步数的增加，迭代结果和真实结果存在偏移</li><li>稳定性：原本收敛的结果<strong>发散</strong>，如螺旋速度场中粒子应该最终做匀速圆周运动，而实际模拟却会飞出该场</li></ul><p>于是，我们有若干方法解决这种不稳定性：</p><ul><li><strong>中点法 Midpoint Method</strong></li></ul><script type="math/tex; mode=display">\begin{aligned}x_{\mathrm{mid}} &=x(t)+\Delta t / 2 \cdot v(x(t), t) \\x(t+\Delta t) &=x(t)+\Delta t \cdot v\left(x_{\mathrm{mid}}, t\right)\end{aligned}\</script><ul><li><strong>Adaptive Step Size 自适应步长</strong></li></ul><p>按照误差项判断是否将 $\Delta t$​ 继续细分…</p><pre class="language-none"><code class="language-none">Repeat until error is below certain threshold epsilon:Compute x(T) one Euler step, size TCompute x(T&#x2F;2) two Euler steps, size T&#x2F;2Compute error &#x3D; || x(T) - x(T&#x2F;2) ||if (error &gt; threshold):T &lt;- reduced(T)else:break</code></pre><ul><li><strong>隐式欧拉方法</strong></li></ul><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{x}^{t+\Delta t} &=\boldsymbol{x}^{t}+\Delta t \dot{\boldsymbol{x}}^{t+\Delta t} \\\dot{\boldsymbol{x}}^{t+\Delta t} &=\dot{\boldsymbol{x}}^{t}+\Delta t \ddot{\boldsymbol{x}}^{t+\Delta t}\end{aligned}</script><p>这种方法提供了更好的稳定性，下面我们就进行稳定性的定义：</p><ul><li>局部截断误差 Truncation Error</li><li>累积误差 Accumulated Error</li></ul><p>我们一般研究误差就是研究这两个误差和我们所取的 $\Delta t$ 的阶数的关系。</p><p>推导可以得出，隐式欧拉方法：</p><ul><li>Local Truncation Error: $O(h^2)$</li><li>Global Truncation Error: $O(h)$</li><li>$h$ is the step size, i.e. $\Delta t$​.</li></ul><p>误差是 $O(h)$ 的意思是说，如果我们将步长缩小到原来的一半，那么误差期望也缩小到原来的一半。</p><p>Runge-Kutta Families 中的 RK4 方法广泛得到运用，是一个 4 阶的方法。</p><ul><li><strong>Position-Based / Verlet Integration</strong>，一种不是基于物理的方法‘<ul><li>这种方法快速简单，但是可能不会遵守能量守恒等物理定律</li></ul></li></ul><p>下面举一个简单的 Position-Based 方法的例子，即渲染如下图片的例子。</p><p><img src="https://s2.loli.net/2022/01/18/Z21NyICghTidwWG.png" alt="image-20220118165158591"></p><p>关键想法如下：</p><ul><li>将水视为是很多小刚体球组成的</li><li>认为水的密度是不变的，即这些小刚体球是不可以被压缩的</li><li>也就是说，一旦某处水的密度改变了，就应该通过重新分布不同区域的刚体球的数量来“矫正”这个变化<ul><li>需要知道水的密度场的梯度！</li><li>这个水的密度场是由刚体球所在位置决定的…</li></ul></li><li>如何矫正？Gradient Descent!</li></ul><blockquote><p>模拟方法的总结：</p><ul><li>（质点法）拉格朗日方法，将物体细分成一个个质点</li><li>（网格法）欧拉方法，将整个空间分为不同的网格，考虑空间网格中存在的物体随着时间的变化</li></ul></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;GAMES 101 现代计算机图形学基础 笔记（下篇）.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;光线追踪（Whitted-style，求交算法，加速结构，辐射度量学，路径追踪）&lt;/li&gt;
&lt;li&gt;材质与外观&lt;/li&gt;
&lt;li&gt;动画与模拟&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="理论" scheme="https://www.c7w.tech/categories/%E7%90%86%E8%AE%BA/"/>
    
    <category term="理论/计算机图形学" scheme="https://www.c7w.tech/categories/%E7%90%86%E8%AE%BA/%E7%90%86%E8%AE%BA-%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
    
    <category term="图形学" scheme="https://www.c7w.tech/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>Python Pandas 库使用指北</title>
    <link href="https://www.c7w.tech/python-pandas/"/>
    <id>https://www.c7w.tech/python-pandas/</id>
    <published>2022-01-11T12:26:30.000Z</published>
    <updated>2022-01-16T15:05:50.914Z</updated>
    
    <content type="html"><![CDATA[<p><code>Pandas</code> 是 Python 下的高性能的数据管理工具与数据分析工具。</p><p>本文中介绍了 <code>Pandas</code> 库中的一些常用类，然后记录了一些简单的筛选，切片等等用法。</p><a id="more"></a><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ul><li>（主要数据结构）Pandas 的主要数据结构是 <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html#pandas.Series">Series</a>（一维数据）与 <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame">DataFrame</a>（二维数据）。</li><li>（大小不变）Pandas 所有数据结构的值都是可变的，但数据结构的大小并非都是可变的，比如，Series 的长度不可改变，但 DataFrame 里就可以插入列。</li><li>（倾向于复制）Pandas 里，绝大多数方法都不改变原始的输入数据，而是复制数据，生成新的对象。 一般来说，原始输入数据<strong>不变</strong>更稳妥。</li></ul><h2 id="快速入门"><a href="#快速入门" class="headerlink" title="快速入门"></a>快速入门</h2><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd</code></pre><h3 id="生成-Series-与-DataFrame"><a href="#生成-Series-与-DataFrame" class="headerlink" title="生成 Series 与 DataFrame"></a>生成 Series 与 DataFrame</h3><ul><li>Series 的生成：默认使用整数索引</li></ul><pre class="language-none"><code class="language-none">&gt;&gt;&gt; s &#x3D; pd.Series([1,3,5,np.nan,6,8])&gt;&gt;&gt; s0    1.01    3.02    5.03    NaN4    6.05    8.0dtype: float64</code></pre><ul><li>DataFrame 的生成：① 指定数据，索引，列名与是否为拷贝<ul><li><code>class pandas.DataFrame(data=None, index=None, columns=None, dtype=None, copy=None)</code></li></ul></li></ul><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> dates <span class="token operator">=</span> pd<span class="token punctuation">.</span>date_range<span class="token punctuation">(</span><span class="token string">'20220101'</span><span class="token punctuation">,</span> periods<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> datesDatetimeIndex<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'2022-01-01'</span><span class="token punctuation">,</span> <span class="token string">'2022-01-02'</span><span class="token punctuation">,</span> <span class="token string">'2022-01-03'</span><span class="token punctuation">,</span> <span class="token string">'2022-01-04'</span><span class="token punctuation">,</span>               <span class="token string">'2022-01-05'</span><span class="token punctuation">,</span> <span class="token string">'2022-01-06'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>              dtype<span class="token operator">=</span><span class="token string">'datetime64[ns]'</span><span class="token punctuation">,</span> freq<span class="token operator">=</span><span class="token string">'D'</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> index<span class="token operator">=</span>dates<span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token string">'ABCD'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> df                   A         B         C         D<span class="token number">2022</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">01</span> <span class="token operator">-</span><span class="token number">0.165424</span> <span class="token operator">-</span><span class="token number">0.095599</span>  <span class="token number">0.940562</span> <span class="token operator">-</span><span class="token number">0.629898</span><span class="token number">2022</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">02</span>  <span class="token number">0.237563</span>  <span class="token number">0.252783</span>  <span class="token number">1.362851</span> <span class="token operator">-</span><span class="token number">1.525588</span><span class="token number">2022</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">03</span>  <span class="token number">2.222287</span> <span class="token operator">-</span><span class="token number">0.591918</span> <span class="token operator">-</span><span class="token number">1.450194</span>  <span class="token number">0.860104</span><span class="token number">2022</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">04</span> <span class="token operator">-</span><span class="token number">0.560238</span>  <span class="token number">0.548346</span> <span class="token operator">-</span><span class="token number">0.691656</span> <span class="token operator">-</span><span class="token number">0.810484</span><span class="token number">2022</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">05</span>  <span class="token number">1.600348</span>  <span class="token number">0.242665</span>  <span class="token number">0.836329</span>  <span class="token number">0.372371</span><span class="token number">2022</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">06</span> <span class="token operator">-</span><span class="token number">0.332250</span> <span class="token operator">-</span><span class="token number">0.941505</span>  <span class="token number">0.110009</span> <span class="token operator">-</span><span class="token number">1.478538</span><span class="token operator">>></span><span class="token operator">></span> a1 <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">"A"</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">"B"</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">"c"</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">&#125;</span><span class="token operator">>></span><span class="token operator">></span> a2 <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">"A"</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">"B"</span><span class="token punctuation">:</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">"C"</span><span class="token punctuation">:</span> <span class="token number">5</span><span class="token punctuation">&#125;</span><span class="token operator">>></span><span class="token operator">></span> pd1 <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">[</span>a1<span class="token punctuation">,</span> a2<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> pd1   A  B    c    C<span class="token number">0</span>  <span class="token number">1</span>  <span class="token number">2</span>  <span class="token number">3.0</span>  NaN<span class="token number">1</span>  <span class="token number">1</span>  <span class="token number">3</span>  NaN  <span class="token number">5.0</span></code></pre><ul><li>DataFrame 的生成：② read_csv</li></ul><pre class="language-python" data-language="python"><code class="language-python">task <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'./Data/</span><span class="token interpolation"><span class="token punctuation">&#123;</span>camp_name<span class="token punctuation">&#125;</span></span><span class="token string">/task.csv'</span></span><span class="token punctuation">)</span>         tid     name   score<span class="token number">0</span>  <span class="token number">0001</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span>A   任务 <span class="token number">1</span><span class="token operator">-</span>A       <span class="token number">5</span><span class="token number">1</span>  <span class="token number">0001</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span>B   任务 <span class="token number">1</span><span class="token operator">-</span>B       <span class="token number">7</span><span class="token number">2</span>  <span class="token number">0001</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span>C   任务 <span class="token number">1</span><span class="token operator">-</span>C       <span class="token number">2</span><span class="token number">3</span>  <span class="token number">0001</span><span class="token operator">-</span><span class="token number">02</span><span class="token operator">-</span>A   任务 <span class="token number">2</span><span class="token operator">-</span>A       <span class="token number">5</span><span class="token number">4</span>  <span class="token number">0001</span><span class="token operator">-</span><span class="token number">02</span><span class="token operator">-</span>B   任务 <span class="token number">2</span><span class="token operator">-</span>B       <span class="token number">5</span><span class="token number">5</span>  <span class="token number">0001</span><span class="token operator">-</span><span class="token number">99</span><span class="token operator">-</span>A    Bonus       <span class="token number">1</span></code></pre><ul><li>DataFrame 的生成：③ data 中传入 Series，利用其所有的 index。如果不同列的 index 不 match，那么报错。</li></ul><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># Example</span>df2 <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">'A'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>                     <span class="token string">'B'</span><span class="token punctuation">:</span> pd<span class="token punctuation">.</span>Timestamp<span class="token punctuation">(</span><span class="token string">'20130102'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                     <span class="token string">'C'</span><span class="token punctuation">:</span> pd<span class="token punctuation">.</span>Series<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">'float32'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                     <span class="token string">'D'</span><span class="token punctuation">:</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">'int32'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                     <span class="token string">'E'</span><span class="token punctuation">:</span> pd<span class="token punctuation">.</span>Categorical<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"test"</span><span class="token punctuation">,</span> <span class="token string">"train"</span><span class="token punctuation">,</span> <span class="token string">"test"</span><span class="token punctuation">,</span> <span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                     <span class="token string">'F'</span><span class="token punctuation">:</span> <span class="token string">'foo'</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span> <span class="token operator">>></span><span class="token operator">></span> df2     A          B    C  D      E    F<span class="token number">0</span>  <span class="token number">1.0</span> <span class="token number">2013</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">02</span>  <span class="token number">1.0</span>  <span class="token number">3</span>   test  foo<span class="token number">1</span>  <span class="token number">1.0</span> <span class="token number">2013</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">02</span>  <span class="token number">1.0</span>  <span class="token number">3</span>  train  foo<span class="token number">2</span>  <span class="token number">1.0</span> <span class="token number">2013</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">02</span>  <span class="token number">1.0</span>  <span class="token number">3</span>   test  foo<span class="token number">3</span>  <span class="token number">1.0</span> <span class="token number">2013</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">02</span>  <span class="token number">1.0</span>  <span class="token number">3</span>  train  foo<span class="token comment"># Test</span>df3 <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">'A'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>                     <span class="token string">'B'</span><span class="token punctuation">:</span> pd<span class="token punctuation">.</span>Timestamp<span class="token punctuation">(</span><span class="token string">'20130102'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                     <span class="token string">'C'</span><span class="token punctuation">:</span> pd<span class="token punctuation">.</span>Series<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">'float32'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                     <span class="token string">'D'</span><span class="token punctuation">:</span> pd<span class="token punctuation">.</span>Series<span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">'float32'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                     <span class="token string">'E'</span><span class="token punctuation">:</span> pd<span class="token punctuation">.</span>Categorical<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"test"</span><span class="token punctuation">,</span> <span class="token string">"train"</span><span class="token punctuation">,</span> <span class="token string">"test"</span><span class="token punctuation">,</span> <span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                     <span class="token string">'F'</span><span class="token punctuation">:</span> <span class="token string">'foo'</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>Traceback <span class="token punctuation">(</span>most recent call last<span class="token punctuation">)</span><span class="token punctuation">:</span>  File <span class="token string">"&lt;stdin>"</span><span class="token punctuation">,</span> line <span class="token number">1</span><span class="token punctuation">,</span> <span class="token keyword">in</span> <span class="token operator">&lt;</span>module<span class="token operator">></span>  File <span class="token string">"/home/c7w/.local/lib/python3.8/site-packages/pandas/core/frame.py"</span><span class="token punctuation">,</span> line <span class="token number">614</span><span class="token punctuation">,</span> <span class="token keyword">in</span> __init__    mgr <span class="token operator">=</span> dict_to_mgr<span class="token punctuation">(</span>data<span class="token punctuation">,</span> index<span class="token punctuation">,</span> columns<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> copy<span class="token operator">=</span>copy<span class="token punctuation">,</span> typ<span class="token operator">=</span>manager<span class="token punctuation">)</span>  File <span class="token string">"/home/c7w/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py"</span><span class="token punctuation">,</span> line <span class="token number">464</span><span class="token punctuation">,</span> <span class="token keyword">in</span> dict_to_mgr    <span class="token keyword">return</span> arrays_to_mgr<span class="token punctuation">(</span>  File <span class="token string">"/home/c7w/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py"</span><span class="token punctuation">,</span> line <span class="token number">119</span><span class="token punctuation">,</span> <span class="token keyword">in</span> arrays_to_mgr    index <span class="token operator">=</span> _extract_index<span class="token punctuation">(</span>arrays<span class="token punctuation">)</span>  File <span class="token string">"/home/c7w/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py"</span><span class="token punctuation">,</span> line <span class="token number">649</span><span class="token punctuation">,</span> <span class="token keyword">in</span> _extract_index    <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>msg<span class="token punctuation">)</span>ValueError<span class="token punctuation">:</span> array length <span class="token number">4</span> does <span class="token keyword">not</span> match index length <span class="token number">6</span>        <span class="token comment"># Another Test</span>df3 <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">'A'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>                     <span class="token string">'B'</span><span class="token punctuation">:</span> pd<span class="token punctuation">.</span>Timestamp<span class="token punctuation">(</span><span class="token string">'20130102'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                     <span class="token string">'C'</span><span class="token punctuation">:</span> pd<span class="token punctuation">.</span>Series<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">'float32'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                     <span class="token string">'D'</span><span class="token punctuation">:</span> pd<span class="token punctuation">.</span>Series<span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token string">'b'</span><span class="token punctuation">,</span> <span class="token string">'c'</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">'float32'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                     <span class="token string">'E'</span><span class="token punctuation">:</span> pd<span class="token punctuation">.</span>Categorical<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"test"</span><span class="token punctuation">,</span> <span class="token string">"train"</span><span class="token punctuation">,</span> <span class="token string">"test"</span><span class="token punctuation">,</span> <span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                     <span class="token string">'F'</span><span class="token punctuation">:</span> <span class="token string">'foo'</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span>Traceback <span class="token punctuation">(</span>most recent call last<span class="token punctuation">)</span><span class="token punctuation">:</span>  File <span class="token string">"&lt;stdin>"</span><span class="token punctuation">,</span> line <span class="token number">1</span><span class="token punctuation">,</span> <span class="token keyword">in</span> <span class="token operator">&lt;</span>module<span class="token operator">></span>  File <span class="token string">"/home/c7w/.local/lib/python3.8/site-packages/pandas/core/frame.py"</span><span class="token punctuation">,</span> line <span class="token number">614</span><span class="token punctuation">,</span> <span class="token keyword">in</span> __init__    mgr <span class="token operator">=</span> dict_to_mgr<span class="token punctuation">(</span>data<span class="token punctuation">,</span> index<span class="token punctuation">,</span> columns<span class="token punctuation">,</span> dtype<span class="token operator">=</span>dtype<span class="token punctuation">,</span> copy<span class="token operator">=</span>copy<span class="token punctuation">,</span> typ<span class="token operator">=</span>manager<span class="token punctuation">)</span>  File <span class="token string">"/home/c7w/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py"</span><span class="token punctuation">,</span> line <span class="token number">464</span><span class="token punctuation">,</span> <span class="token keyword">in</span> dict_to_mgr    <span class="token keyword">return</span> arrays_to_mgr<span class="token punctuation">(</span>  File <span class="token string">"/home/c7w/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py"</span><span class="token punctuation">,</span> line <span class="token number">119</span><span class="token punctuation">,</span> <span class="token keyword">in</span> arrays_to_mgr    index <span class="token operator">=</span> _extract_index<span class="token punctuation">(</span>arrays<span class="token punctuation">)</span>  File <span class="token string">"/home/c7w/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py"</span><span class="token punctuation">,</span> line <span class="token number">649</span><span class="token punctuation">,</span> <span class="token keyword">in</span> _extract_index    <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>msg<span class="token punctuation">)</span>ValueError<span class="token punctuation">:</span> array length <span class="token number">4</span> does <span class="token keyword">not</span> match index length <span class="token number">7</span>                <span class="token comment"># Another Test</span>df3 <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token string">'A'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>                     <span class="token string">'B'</span><span class="token punctuation">:</span> pd<span class="token punctuation">.</span>Timestamp<span class="token punctuation">(</span><span class="token string">'20130102'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                     <span class="token string">'C'</span><span class="token punctuation">:</span> pd<span class="token punctuation">.</span>Series<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">'float32'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                     <span class="token string">'D'</span><span class="token punctuation">:</span> pd<span class="token punctuation">.</span>Series<span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">'float32'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                     <span class="token string">'E'</span><span class="token punctuation">:</span> pd<span class="token punctuation">.</span>Categorical<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">"test"</span><span class="token punctuation">,</span> <span class="token string">"train"</span><span class="token punctuation">,</span> <span class="token string">"test"</span><span class="token punctuation">,</span> <span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                     <span class="token string">'F'</span><span class="token punctuation">:</span> <span class="token string">'foo'</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> df3     A          B    C     D      E    F<span class="token number">0</span>  <span class="token number">1.0</span> <span class="token number">2013</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">02</span>  <span class="token number">1.0</span>  <span class="token number">12.0</span>   test  foo<span class="token number">1</span>  <span class="token number">1.0</span> <span class="token number">2013</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">02</span>  <span class="token number">1.0</span>  <span class="token number">12.0</span>  train  foo<span class="token number">2</span>  <span class="token number">1.0</span> <span class="token number">2013</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">02</span>  <span class="token number">1.0</span>  <span class="token number">12.0</span>   test  foo<span class="token number">3</span>  <span class="token number">1.0</span> <span class="token number">2013</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">02</span>  <span class="token number">1.0</span>  <span class="token number">12.0</span>  train  foo</code></pre><h3 id="查看数据"><a href="#查看数据" class="headerlink" title="查看数据"></a>查看数据</h3><ul><li>列数据类型</li></ul><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> df2<span class="token punctuation">.</span>dtypesA           float64B    datetime64<span class="token punctuation">[</span>ns<span class="token punctuation">]</span>C           float32D             int32E          categoryF            <span class="token builtin">object</span>dtype<span class="token punctuation">:</span> <span class="token builtin">object</span></code></pre><ul><li>预览数据</li></ul><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> df2     A          B    C  D      E    F<span class="token number">0</span>  <span class="token number">1.0</span> <span class="token number">2013</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">02</span>  <span class="token number">1.0</span>  <span class="token number">3</span>   test  foo<span class="token number">1</span>  <span class="token number">1.0</span> <span class="token number">2013</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">02</span>  <span class="token number">1.0</span>  <span class="token number">3</span>  train  foo<span class="token number">2</span>  <span class="token number">1.0</span> <span class="token number">2013</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">02</span>  <span class="token number">1.0</span>  <span class="token number">3</span>   test  foo<span class="token number">3</span>  <span class="token number">1.0</span> <span class="token number">2013</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">02</span>  <span class="token number">1.0</span>  <span class="token number">3</span>  train  foo<span class="token operator">>></span><span class="token operator">></span> df2<span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 默认最多显示 5 条数据</span>     A          B    C  D      E    F<span class="token number">0</span>  <span class="token number">1.0</span> <span class="token number">2013</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">02</span>  <span class="token number">1.0</span>  <span class="token number">3</span>   test  foo<span class="token number">1</span>  <span class="token number">1.0</span> <span class="token number">2013</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">02</span>  <span class="token number">1.0</span>  <span class="token number">3</span>  train  foo<span class="token number">2</span>  <span class="token number">1.0</span> <span class="token number">2013</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">02</span>  <span class="token number">1.0</span>  <span class="token number">3</span>   test  foo<span class="token number">3</span>  <span class="token number">1.0</span> <span class="token number">2013</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">02</span>  <span class="token number">1.0</span>  <span class="token number">3</span>  train  foo<span class="token operator">>></span><span class="token operator">></span> df2<span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>     A          B    C  D      E    F<span class="token number">0</span>  <span class="token number">1.0</span> <span class="token number">2013</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">02</span>  <span class="token number">1.0</span>  <span class="token number">3</span>   test  foo<span class="token number">1</span>  <span class="token number">1.0</span> <span class="token number">2013</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">02</span>  <span class="token number">1.0</span>  <span class="token number">3</span>  train  foo<span class="token operator">>></span><span class="token operator">></span> df2<span class="token punctuation">.</span>tail<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>     A          B    C  D      E    F<span class="token number">2</span>  <span class="token number">1.0</span> <span class="token number">2013</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">02</span>  <span class="token number">1.0</span>  <span class="token number">3</span>   test  foo<span class="token number">3</span>  <span class="token number">1.0</span> <span class="token number">2013</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">02</span>  <span class="token number">1.0</span>  <span class="token number">3</span>  train  foo</code></pre><ul><li>显示索引与列</li></ul><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> df2<span class="token punctuation">.</span>indexInt64Index<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">'int64'</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> df2<span class="token punctuation">.</span>columnsIndex<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">,</span> <span class="token string">'C'</span><span class="token punctuation">,</span> <span class="token string">'D'</span><span class="token punctuation">,</span> <span class="token string">'E'</span><span class="token punctuation">,</span> <span class="token string">'F'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">'object'</span><span class="token punctuation">)</span></code></pre><ul><li>转换成 numpy 对象</li></ul><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> index<span class="token operator">=</span>dates<span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token string">'ABCD'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> df                   A         B         C         D<span class="token number">2022</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">01</span> <span class="token operator">-</span><span class="token number">0.451372</span>  <span class="token number">1.581206</span> <span class="token operator">-</span><span class="token number">0.499837</span> <span class="token operator">-</span><span class="token number">0.549320</span><span class="token number">2022</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">02</span>  <span class="token number">0.508692</span> <span class="token operator">-</span><span class="token number">0.304325</span>  <span class="token number">1.995154</span> <span class="token operator">-</span><span class="token number">0.895727</span><span class="token number">2022</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">03</span>  <span class="token number">0.681460</span>  <span class="token number">1.214341</span> <span class="token operator">-</span><span class="token number">0.608446</span> <span class="token operator">-</span><span class="token number">1.054553</span><span class="token number">2022</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">04</span> <span class="token operator">-</span><span class="token number">1.032206</span>  <span class="token number">0.237051</span>  <span class="token number">1.502665</span> <span class="token operator">-</span><span class="token number">0.048824</span><span class="token number">2022</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">05</span>  <span class="token number">1.707183</span>  <span class="token number">0.382228</span>  <span class="token number">1.335121</span> <span class="token operator">-</span><span class="token number">1.099418</span><span class="token number">2022</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">06</span>  <span class="token number">0.895205</span>  <span class="token number">0.982582</span>  <span class="token number">0.188397</span>  <span class="token number">0.023960</span><span class="token operator">>></span><span class="token operator">></span> df<span class="token punctuation">.</span>to_numpy<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 纯 float 类型，转换速度较快</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.45137221</span><span class="token punctuation">,</span>  <span class="token number">1.58120564</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.49983697</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.54931982</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0.50869221</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.30432474</span><span class="token punctuation">,</span>  <span class="token number">1.99515425</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.89572709</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0.68146026</span><span class="token punctuation">,</span>  <span class="token number">1.21434138</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.60844611</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.05455281</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.03220563</span><span class="token punctuation">,</span>  <span class="token number">0.23705134</span><span class="token punctuation">,</span>  <span class="token number">1.50266499</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.04882351</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">1.70718305</span><span class="token punctuation">,</span>  <span class="token number">0.38222751</span><span class="token punctuation">,</span>  <span class="token number">1.33512092</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.09941788</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span> <span class="token number">0.89520526</span><span class="token punctuation">,</span>  <span class="token number">0.98258218</span><span class="token punctuation">,</span>  <span class="token number">0.18839673</span><span class="token punctuation">,</span>  <span class="token number">0.02396049</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> df2<span class="token punctuation">.</span>to_numpy<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 复合数据类型，转换速度慢</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> Timestamp<span class="token punctuation">(</span><span class="token string">'2013-01-02 00:00:00'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">,</span> <span class="token string">'foo'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> Timestamp<span class="token punctuation">(</span><span class="token string">'2013-01-02 00:00:00'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'foo'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> Timestamp<span class="token punctuation">(</span><span class="token string">'2013-01-02 00:00:00'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">,</span> <span class="token string">'foo'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> Timestamp<span class="token punctuation">(</span><span class="token string">'2013-01-02 00:00:00'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'foo'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>      dtype<span class="token operator">=</span><span class="token builtin">object</span><span class="token punctuation">)</span></code></pre><ul><li>数据统计信息</li></ul><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> df<span class="token punctuation">.</span>describe<span class="token punctuation">(</span><span class="token punctuation">)</span>              A         B         C         Dcount  <span class="token number">6.000000</span>  <span class="token number">6.000000</span>  <span class="token number">6.000000</span>  <span class="token number">6.000000</span>mean   <span class="token number">0.384827</span>  <span class="token number">0.682181</span>  <span class="token number">0.652176</span> <span class="token operator">-</span><span class="token number">0.603980</span>std    <span class="token number">0.981800</span>  <span class="token number">0.698997</span>  <span class="token number">1.106773</span>  <span class="token number">0.497813</span><span class="token builtin">min</span>   <span class="token operator">-</span><span class="token number">1.032206</span> <span class="token operator">-</span><span class="token number">0.304325</span> <span class="token operator">-</span><span class="token number">0.608446</span> <span class="token operator">-</span><span class="token number">1.099418</span><span class="token number">25</span><span class="token operator">%</span>   <span class="token operator">-</span><span class="token number">0.211356</span>  <span class="token number">0.273345</span> <span class="token operator">-</span><span class="token number">0.327779</span> <span class="token operator">-</span><span class="token number">1.014846</span><span class="token number">50</span><span class="token operator">%</span>    <span class="token number">0.595076</span>  <span class="token number">0.682405</span>  <span class="token number">0.761759</span> <span class="token operator">-</span><span class="token number">0.722523</span><span class="token number">75</span><span class="token operator">%</span>    <span class="token number">0.841769</span>  <span class="token number">1.156402</span>  <span class="token number">1.460779</span> <span class="token operator">-</span><span class="token number">0.173948</span><span class="token builtin">max</span>    <span class="token number">1.707183</span>  <span class="token number">1.581206</span>  <span class="token number">1.995154</span>  <span class="token number">0.023960</span></code></pre><h3 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h3><ul><li>转置</li></ul><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> df<span class="token punctuation">.</span>T   <span class="token number">2022</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">01</span>  <span class="token number">2022</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">02</span>  <span class="token number">2022</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">03</span>  <span class="token number">2022</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">04</span>  <span class="token number">2022</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">05</span>  <span class="token number">2022</span><span class="token operator">-</span><span class="token number">01</span><span class="token operator">-</span><span class="token number">06</span>A   <span class="token operator">-</span><span class="token number">0.451372</span>    <span class="token number">0.508692</span>    <span class="token number">0.681460</span>   <span class="token operator">-</span><span class="token number">1.032206</span>    <span class="token number">1.707183</span>    <span class="token number">0.895205</span>B    <span class="token number">1.581206</span>   <span class="token operator">-</span><span class="token number">0.304325</span>    <span class="token number">1.214341</span>    <span class="token number">0.237051</span>    <span class="token number">0.382228</span>    <span class="token number">0.982582</span>C   <span class="token operator">-</span><span class="token number">0.499837</span>    <span class="token number">1.995154</span>   <span class="token operator">-</span><span class="token number">0.608446</span>    <span class="token number">1.502665</span>    <span class="token number">1.335121</span>    <span class="token number">0.188397</span>D   <span class="token operator">-</span><span class="token number">0.549320</span>   <span class="token operator">-</span><span class="token number">0.895727</span>   <span class="token operator">-</span><span class="token number">1.054553</span>   <span class="token operator">-</span><span class="token number">0.048824</span>   <span class="token operator">-</span><span class="token number">1.099418</span>    <span class="token number">0.023960</span></code></pre><ul><li>按索引排序<ul><li><code>DataFrame.sort_index(axis=0, level=None, ascending=True, inplace=False, kind=&#39;quicksort&#39;, na_position=&#39;last&#39;, sort_remaining=True, ignore_index=False, key=None)</code></li><li>kind 可以取’mergesort’，以达到稳定的目的</li><li>axis 取 0 按行排序，取 1 按列排序</li></ul></li></ul><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> df4 <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">29</span><span class="token punctuation">,</span> <span class="token number">234</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">150</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>                   columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> df4     A<span class="token number">100</span>  <span class="token number">1</span><span class="token number">29</span>   <span class="token number">2</span><span class="token number">234</span>  <span class="token number">3</span><span class="token number">1</span>    <span class="token number">4</span><span class="token number">150</span>  <span class="token number">5</span><span class="token operator">>></span><span class="token operator">></span> df4<span class="token punctuation">.</span>sort_index<span class="token punctuation">(</span><span class="token punctuation">)</span>     A<span class="token number">1</span>    <span class="token number">4</span><span class="token number">29</span>   <span class="token number">2</span><span class="token number">100</span>  <span class="token number">1</span><span class="token number">150</span>  <span class="token number">5</span><span class="token number">234</span>  <span class="token number">3</span><span class="token operator">>></span><span class="token operator">></span> df4 <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">29</span><span class="token punctuation">,</span> <span class="token number">234</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">150</span><span class="token punctuation">]</span><span class="token punctuation">,</span> columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'B'</span><span class="token punctuation">,</span> <span class="token string">'A'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> df4     B   A<span class="token number">100</span>  <span class="token number">1</span>   <span class="token number">2</span><span class="token number">29</span>   <span class="token number">3</span>   <span class="token number">4</span><span class="token number">234</span>  <span class="token number">5</span>   <span class="token number">6</span><span class="token number">1</span>    <span class="token number">7</span>   <span class="token number">8</span><span class="token number">150</span>  <span class="token number">9</span>  <span class="token number">10</span><span class="token operator">>></span><span class="token operator">></span> df4<span class="token punctuation">.</span>sort_index<span class="token punctuation">(</span><span class="token punctuation">)</span>     B   A<span class="token number">1</span>    <span class="token number">7</span>   <span class="token number">8</span><span class="token number">29</span>   <span class="token number">3</span>   <span class="token number">4</span><span class="token number">100</span>  <span class="token number">1</span>   <span class="token number">2</span><span class="token number">150</span>  <span class="token number">9</span>  <span class="token number">10</span><span class="token number">234</span>  <span class="token number">5</span>   <span class="token number">6</span><span class="token operator">>></span><span class="token operator">></span> df4<span class="token punctuation">.</span>sort_index<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>      A  B<span class="token number">100</span>   <span class="token number">2</span>  <span class="token number">1</span><span class="token number">29</span>    <span class="token number">4</span>  <span class="token number">3</span><span class="token number">234</span>   <span class="token number">6</span>  <span class="token number">5</span><span class="token number">1</span>     <span class="token number">8</span>  <span class="token number">7</span><span class="token number">150</span>  <span class="token number">10</span>  <span class="token number">9</span></code></pre><ul><li>按某列排序<ul><li><code>DataFrame.sort_values(by, axis=0, ascending=True, inplace=False, kind=&#39;quicksort&#39;, na_position=&#39;last&#39;, ignore_index=False, key=None)</code></li><li>na_position 可以是 <code>first</code> 和 <code>last</code></li><li>如果 ignore_index 为真，那么结果的索引会从 0 标号至 n-1</li><li>key: 接受一个 Series 作为输出，返回一个 shape 相同的 Series，然后使用后者进行排序操作</li></ul></li></ul><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">&#123;</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token string">'col1'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'A'</span><span class="token punctuation">,</span> <span class="token string">'A'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>nan<span class="token punctuation">,</span> <span class="token string">'D'</span><span class="token punctuation">,</span> <span class="token string">'C'</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token string">'col2'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token string">'col3'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token string">'col4'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">,</span> <span class="token string">'c'</span><span class="token punctuation">,</span> <span class="token string">'D'</span><span class="token punctuation">,</span> <span class="token string">'e'</span><span class="token punctuation">,</span> <span class="token string">'F'</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> df  col1  col2  col3 col4<span class="token number">0</span>    A     <span class="token number">2</span>     <span class="token number">0</span>    a<span class="token number">1</span>    A     <span class="token number">1</span>     <span class="token number">1</span>    B<span class="token number">2</span>    B     <span class="token number">9</span>     <span class="token number">9</span>    c<span class="token number">3</span>  NaN     <span class="token number">8</span>     <span class="token number">4</span>    D<span class="token number">4</span>    D     <span class="token number">7</span>     <span class="token number">2</span>    e<span class="token number">5</span>    C     <span class="token number">4</span>     <span class="token number">3</span>    F<span class="token operator">>></span><span class="token operator">></span> df<span class="token punctuation">.</span>sort_values<span class="token punctuation">(</span>by<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'col1'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  col1  col2  col3 col4<span class="token number">0</span>    A     <span class="token number">2</span>     <span class="token number">0</span>    a<span class="token number">1</span>    A     <span class="token number">1</span>     <span class="token number">1</span>    B<span class="token number">2</span>    B     <span class="token number">9</span>     <span class="token number">9</span>    c<span class="token number">5</span>    C     <span class="token number">4</span>     <span class="token number">3</span>    F<span class="token number">4</span>    D     <span class="token number">7</span>     <span class="token number">2</span>    e<span class="token number">3</span>  NaN     <span class="token number">8</span>     <span class="token number">4</span>    D<span class="token operator">>></span><span class="token operator">></span> df<span class="token punctuation">.</span>sort_values<span class="token punctuation">(</span>by<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'col1'</span><span class="token punctuation">,</span> <span class="token string">'col2'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  col1  col2  col3 col4<span class="token number">1</span>    A     <span class="token number">1</span>     <span class="token number">1</span>    B<span class="token number">0</span>    A     <span class="token number">2</span>     <span class="token number">0</span>    a<span class="token number">2</span>    B     <span class="token number">9</span>     <span class="token number">9</span>    c<span class="token number">5</span>    C     <span class="token number">4</span>     <span class="token number">3</span>    F<span class="token number">4</span>    D     <span class="token number">7</span>     <span class="token number">2</span>    e<span class="token number">3</span>  NaN     <span class="token number">8</span>     <span class="token number">4</span>    Ddf<span class="token punctuation">.</span>sort_values<span class="token punctuation">(</span>by<span class="token operator">=</span><span class="token string">'col4'</span><span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token keyword">lambda</span> col<span class="token punctuation">:</span> col<span class="token punctuation">.</span><span class="token builtin">str</span><span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  col1  col2  col3 col4<span class="token number">0</span>    A     <span class="token number">2</span>     <span class="token number">0</span>    a<span class="token number">1</span>    A     <span class="token number">1</span>     <span class="token number">1</span>    B<span class="token number">2</span>    B     <span class="token number">9</span>     <span class="token number">9</span>    c<span class="token number">3</span>  NaN     <span class="token number">8</span>     <span class="token number">4</span>    D<span class="token number">4</span>    D     <span class="token number">7</span>     <span class="token number">2</span>    e<span class="token number">5</span>    C     <span class="token number">4</span>     <span class="token number">3</span>    F</code></pre><h3 id="选择"><a href="#选择" class="headerlink" title="选择"></a>选择</h3><p>// To be continued…</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="http://www.pypandas.cn/docs/">http://www.pypandas.cn/docs/</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;code&gt;Pandas&lt;/code&gt; 是 Python 下的高性能的数据管理工具与数据分析工具。&lt;/p&gt;
&lt;p&gt;本文中介绍了 &lt;code&gt;Pandas&lt;/code&gt; 库中的一些常用类，然后记录了一些简单的筛选，切片等等用法。&lt;/p&gt;</summary>
    
    
    
    <category term="技术" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="技术/Python应用" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/%E6%8A%80%E6%9C%AF-Python%E5%BA%94%E7%94%A8/"/>
    
    
    <category term="数据分析" scheme="https://www.c7w.tech/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>c7w 的 2021 年度总结</title>
    <link href="https://www.c7w.tech/summary-2021/"/>
    <id>https://www.c7w.tech/summary-2021/</id>
    <published>2022-01-07T02:28:45.000Z</published>
    <updated>2022-01-10T07:29:31.701Z</updated>
    
    <content type="html"><![CDATA[<p>于 2022 年初，对 2021 的年度总结。</p><a id="more"></a><p>昨晚九点考完自动机，终于能放松一下。打游戏打到晚上快三点。本来想睡觉，上床上拿起手机玩到快四点。早上七点半就醒了，如果是考试周的话可能会继续睡过去，而考试周过了现在却没有丝毫困意。虽然还有很多活计要做，但是想了想还是姑且先休息整顿半周，然后好好拥抱这日程表稀稀疏疏的生活吧。</p><h2 id="学习情况总结"><a href="#学习情况总结" class="headerlink" title="学习情况总结"></a>学习情况总结</h2><p>姑且身份还是个大学生，还是要把课业总结放到最前面的（x</p><h3 id="2021-春季学期"><a href="#2021-春季学期" class="headerlink" title="2021 春季学期"></a>2021 春季学期</h3><p><img src="https://s2.loli.net/2022/01/07/AdQDf6OrNu7McKs.png" alt="image-20220107105303474"></p><p>这个学期的时候 c7w 对待课业应该还是蛮认真的，也没有翘过什么课，但想要在课业外做些什么的想法却是时不时地涌现。或许是还没有抓住足够多的机会的缘故吧。下面就对选修的课程进行一定程度的总结，或许在一定程度上可以作为后来人的选课指导？</p><h4 id="面向对象程序设计基础-OOP"><a href="#面向对象程序设计基础-OOP" class="headerlink" title="面向对象程序设计基础 (OOP)"></a>面向对象程序设计基础 (OOP)</h4><p>除了课件之外在尝试坚持看《C++ Primer》。之所以说在“尝试”坚持，是因为整本书就没有从头到尾完全看过一遍。但聊胜于无，虽说看的还是课件 cover 的大多数部分。周一的早八课，每次都是后排看课件。看到某个机制不太清晰的地方，可能并不是倾向于去问别人“How it works”，而是写段代码让事实验证这东西到底是怎么运作的。</p><p>对这门课印象深刻的一点在于，六次编程作业(Programming Assignment)和期末考试。一句话总结：人菜瘾大。刷完成时间榜变成了每两周固定的乐趣之一。</p><p>作业仓库：<a href="https://github.com/c7w/2021-Spring-OOP-Homework">https://github.com/c7w/2021-Spring-OOP-Homework</a></p><h4 id="大学物理B-1"><a href="#大学物理B-1" class="headerlink" title="大学物理B(1)"></a>大学物理B(1)</h4><p>选课掉了，最后补选了 jsh 老师的课。80 人的课容量，选课人数刚过 30，上到后期教室里一共坐了 8 个人，发现雨课堂签到的竟然有 11 个。期末考试的时候见到很多之前从未见到的面孔。</p><p>讲得也不难，作业也不多，考试难度也是跟作业难度类似。可能大家还是倾向于选更加容易拿高分，更加被钻研透套路的老师吧。（后情提要：这个 c7w 到大二的秋季学期就真香了）</p><h4 id="体育-2"><a href="#体育-2" class="headerlink" title="体育(2)"></a>体育(2)</h4><p>专项是篮球，考察方式是 4 个三步上篮，丢一个扣 5 分。体育老师人真的特别好。引体向上的话，姑且因为寒假心态转变了许多，其中之一就是完全摒弃掉了鸵鸟心态，所以有在练习。虽然这个 20 分的专项大家的得分分布是，要么爆零，要么接近满分；我却算是拿了一个中间偏上的分数。虽然说有训练，但毕竟高中三年还是养的过于肥了，怎么可能半个学期就恢复比较好的体质呢（x</p><p>总评最后 89，然后被老师捞了一把（是的我都不相信我体育能 A-），可以预测会变成这几门体育唯一能过 80 的一次了（x</p><h4 id="中国近现代史纲要"><a href="#中国近现代史纲要" class="headerlink" title="中国近现代史纲要"></a>中国近现代史纲要</h4><p>秋季学期选课，all in 夏清。开学第一节课，发现夏清不开课了，两个课堂一个给了 sw，一个给了 lj。而我就在后面那个课堂…因为要组队（这意味着要接触陌生人…），所以果断删课，补选了 whk，仅后八周上课。没想到课堂参与是签到+回答不定项选择题，最佳记录是一节课六道题只对了一道（笑</p><p>确实对这种带有政治教育任务的历史课不是很感冒。不过最后的期末论文找了一个感兴趣的话题，去图书馆借了些参考书认真做了两天。总体来说，一个学期，一次 pre 准备半天，一篇论文写了两天，课上没听，在看当周五要讲的高代：两天半的时间水掉 3 学分似乎并不亏（</p><h4 id="医学与人类文明的进步"><a href="#医学与人类文明的进步" class="headerlink" title="医学与人类文明的进步"></a>医学与人类文明的进步</h4><p>人文课组，P/F。签到就占 70 分。课上基本在看高代。</p><p>有一次去长庚医院实习参观的机会。感觉见识到不少有意思的东西。</p><h4 id="光电子-太长了不敲了"><a href="#光电子-太长了不敲了" class="headerlink" title="光电子(太长了不敲了)"></a>光电子(太长了不敲了)</h4><p>艺术课组，P/F。动手做项目的课程，虽然我不懂什么叫做艺术，但创作的过程可能就叫做艺术吧（点头</p><p><img src="https://s2.loli.net/2022/01/07/egMo8463SfpWY2h.png" alt="image-20220107112159826"></p><p>最后摸了个这东西（原谅图片高糊），想法是通过光带和发光球来模拟某个星系。事实上这东西是可以转起来的。因为是和舍友一起选的课，所以在组队这件事上没有遇到什么困难…</p><h4 id="微积分A-2"><a href="#微积分A-2" class="headerlink" title="微积分A(2)"></a>微积分A(2)</h4><p>和大一的秋季学期一样，选的是晏姐姐的微积分。风格和模式都和秋季学期类似。虽说坊间传言晏姐姐作业明显多于其他老师，可我大概没什么感觉。</p><p>每周两节微积分课是有去上的。虽然不听课，上课就先自己看课件，看完了写作业。值得一提的是，期末考试之前有人在某个数学讨论群里发了一道有关调和函数的场论证明题，当时和舍友讨论了半个小时把那题推广了一下，最后完美压中期末考试附加题前的最后一题…</p><p>附加题写了快一个小时还是寄了（</p><p>（这时的 c7w 并没有想到，自己被微积分和线性代数强化后，毫无犹豫地决定要辅修数学会在秋季学期给他带来（）的痛苦x</p><h4 id="写作与沟通"><a href="#写作与沟通" class="headerlink" title="写作与沟通"></a>写作与沟通</h4><p>大一的秋季学期隔壁寝的茶园大佬选了唯一的英文写沟。很快啊，我们寝的某位工头同志在这个学期决定也是如此。抱着“dalao带带我”的想法咱也跟着冲了英文写沟。发现课上有接近半数是留学生，而我这种英语水平简直是二哈混进了狼群（x</p><p>最初几节课因为是英文授课所以只能说很痛苦很痛苦，后来就逐渐习惯了（安详</p><p>不过这课确实我认为很有收获，至少让我明白了一篇 paper 所应该有的 architecture，以及各个部分至少应该怎么写。虽说这门课是在研究人文学科，但在之后迁移到理工科的研究上应该也是大同小异的。</p><p>最后 3k words 的长文因为留够了时间所以不是很痛苦，虽然说改了三四版吧，不过一次就被 accepted 确实很开心。五分钟的 pre 准备了快一周，终于最终才脱稿（x（还得靠着演讲者备注苟活</p><p>总结一下就是：跟着工头选课，工头在课上复习微积分，写 OOP，跟 npy 贴贴，拿 A+；咱费尽全力听讲，能听到 lecturer 讲的 80%，认真读 paper，做综述，背 pre 的稿子，拿 A（x</p><h4 id="英汉互译实践与技巧"><a href="#英汉互译实践与技巧" class="headerlink" title="英汉互译实践与技巧"></a>英汉互译实践与技巧</h4><p>作为曾任的某论坛的翻译官，当时选这课还是想好好修炼下自己的翻译水平和英文水平。但是这东西确实不应是一夕之功，更不用说每周只上一个半小时的课，留一份必须要想办法搞好分数的作业了。</p><p>期末考试就是限时英译汉和汉译英，总评分数占 50，可以用笔记本电脑，用网络学堂提交。翻着翻着感觉一个一个不认识的词查太慢，用 deepL 发现翻得比我自己还好，遂直接“润色”摆烂。</p><h4 id="离散数学-2"><a href="#离散数学-2" class="headerlink" title="离散数学(2)"></a>离散数学(2)</h4><p>如果能重来——我一定再加倍努力学离散——而不是在那死磕微积分——</p><p>回过头来想一想，当时离散数学应该是和微积分的学习模式相同，课上先搞课件，搞完课件还有剩下的时间就写作业。作业写完就高枕无忧，束之高阁。这就导致最后期末考试寄掉的惨状（</p><p>我知道自己没有去研究算法的天赋，但是如果仅仅是因为没有天赋，就不去做，就不去尝试，就不去领会痛苦背后带来的快乐，我认为这大概也是一种鸵鸟心态，是我应该摒弃掉的想法。如果自己感觉收获丰富的话，即使没有分数的认可（这里没有 cue 任何课程的意思），我认为这也是有收获的，有价值的。毕竟我们是为了学习知识，领会思想，而不是为了去 fit 那些无聊的期末考试。</p><h4 id="高等线性代数选讲"><a href="#高等线性代数选讲" class="headerlink" title="高等线性代数选讲"></a>高等线性代数选讲</h4><p>掉了，补选的 lph。基本每周课上到这个时候，DDL 清得差不多了，当周要讲的高代连带讲义和作业都搞完了。应该有大部分时间在打舟游。期中考试全班满分人数巨多。期末考试让默写张量的定义，没背会。估计学的那点东西现在也差不多忘干净了。</p><h3 id="2021-夏季学期"><a href="#2021-夏季学期" class="headerlink" title="2021 夏季学期"></a>2021 夏季学期</h3><h4 id="军事技能"><a href="#军事技能" class="headerlink" title="军事技能"></a>军事技能</h4><p>只能说这种管理模式与我生性不和。如果你没有能力将事务组织好，那请不要用无效的感化、伪装或是强制力来实现这一点。对于大多数时间只讲道理的我来说，这些只不过是掩藏能力不足的面具罢了。</p><p>倒也趁这个机会重新认识了不少人，包括在我心里的评价值上升的和下降的。</p><p>当时可能还以为这是能够增强团体凝聚力的契机（事实上它当时在某种程度上确实做到了这一点），但现在看来这种凝聚力也只是临时的罢了。</p><h4 id="毛概-2"><a href="#毛概-2" class="headerlink" title="毛概(2)"></a>毛概(2)</h4><p>实践课，协助班级策划并组织去上海展开实践调研。过程这里就不详述了，但我认为这段是十分珍贵，十分美好的回忆。尤其是刚经历过上述两周的折磨，简直起到了心灵疗伤的作用。</p><p>参与了行程制定与后期视频剪辑的宣传工作。</p><p>还记得暑假某天被工头勒令剪视频剪到半夜三点，工头第二天去和 npy 约会的事。（当然后半句只是玩笑，不过可以看到大家都为这个实践付出了很多，也收获了很多</p><h4 id="暑培"><a href="#暑培" class="headerlink" title="暑培"></a>暑培</h4><p>之所以将暑培作为一门课…是我觉得它本来就应该是一门课。</p><p>一方面是软无自那边的，只参加了前一半；另一方面是贵系自己的。</p><p><img src="https://s2.loli.net/2022/01/07/vKW6ZDFdSYcNsxR.png" alt="image-20220107120816500"></p><center>三系暑培的内容</center><p><img src="https://s2.loli.net/2022/01/07/ySp2MFXAWj67mxu.png" alt="image-20220107122326416"></p><center>科协暑培的内容</center><p>感觉就是，增长了好多见识，我认为这才是有效的教学方式：将你领到门前，告诉你门在哪，里面大概是什么。这门进不进由你，要进的话研究多少也由你自己决定。</p><p>一些认真写的作业：</p><ul><li><a href="https://github.com/c7w/React-demo-0723">React</a></li><li><a href="https://github.com/c7w/Nuxt-handout-0724">Nuxt</a> 博客生成器 &amp; 其对应的<a href="https://github.com/c7w/Nuxt-handout-0724-comment">后端</a></li><li><a href="https://github.com/c7w/Express-hasura-0730">Express &amp; Hasura</a></li><li><a href="https://github.com/c7w/SastSearch">SastSearch (Django)</a></li></ul><h4 id="程序设计训练"><a href="#程序设计训练" class="headerlink" title="程序设计训练"></a>程序设计训练</h4><p>选的是 Python 课程。他们的 Java 开发是直接和当时大二小学期一起搞 Android 开发。反正都是造轮子，抄轮子，找不到轮子自己再造轮子。</p><p>四周，前两周 Qt，后两周 Python。Qt 作业写了四天，Python 作业写了三天。</p><p>规定如果不上课的话，要机房上机，随机签到。xs，我课都不去上，还怕机房签到？八节课一共去了两节，机房去了前两天，然后除了工程验收的时候就没去过。</p><ul><li><a href="https://github.com/c7w/MilitaryChess">MilitaryChess</a></li><li><a href="https://github.com/c7w/BiliSearchCST">BiliSearchCST</a></li></ul><p>神奇的是签到分就没被扣。看起来助教随机签到比我还摸。</p><h3 id="2021-秋季学期"><a href="#2021-秋季学期" class="headerlink" title="2021 秋季学期"></a>2021 秋季学期</h3><p><img src="https://s2.loli.net/2022/01/08/Uw4o5sSPElIbz9C.png" alt="image-20220107122112101"></p><p>新鲜出炉的课程，这不得好好吐槽吐槽。虽然只有 24 学分，但是感觉到这学期课业压力明显。可能是少了不少水课的缘故。此外，可能是因为还需要考虑到除了课业之外的其他因素，比如社工，比如志愿等等。半期后更是接近以宿舍为家。</p><h4 id="大学物理B-2"><a href="#大学物理B-2" class="headerlink" title="大学物理B(2)"></a>大学物理B(2)</h4><p>因为 jsh 选课时间冲突（1-2, 3-1），所以选了 dst。半期后就没去线下上过课。在宿舍用雨课堂听课，一共边听边睡过去过两次。据去上课的舍友说，即使是最后一节课，到课人数也坐不满教室的前三排。</p><p>期末考完试也没弄懂什么是线性厄米算符。// dst 呀 dst，你说你拿着给要修《基础物理学》的学生用的课件来上《大学物理B》，这到底合适嘛（x</p><p>考试就是纯拟合。拟合作业。拟合课件。课件上的例题不会，问题不大，只要背住，考试的时候照着写就行。（没错这里就是在 cue 你，薛定谔方程）至于什么玻尔理论的定态推导过程，什么定态能量公式与半径公式，考完试就扔到脑后了。</p><p>有大学生物理竞赛白给分。到场我只把会做的题都做了，然后跑路。</p><h4 id="汇编语言程序设计"><a href="#汇编语言程序设计" class="headerlink" title="汇编语言程序设计"></a>汇编语言程序设计</h4><p>拿我们这一届当课改的小白鼠，据说下一届就完全按照《CSAPP》的授课模式来讲课了。</p><p>可以感受到的是，张老师确实是十分热心的老师，只不过会发生上课的时候讲到激动之处，会把麦克风放到桌子上然后坐在第三排都听不见的局面。至于 PPT 全面照抄 CMU 的课件，最初还把英文翻成中文，后来大规模摆烂直接放大段大段的英文，我认为倒是现在这个局面的最优解。毕竟 CMU 的 15-213 也是比较成熟的教学体系了，花费无效劳动力再做一遍也没有啥必要。</p><p>实验的话只有 Attack Lab 和一个 MIPS 汇编编程实验，作业一共六次，都是白送分。期末考试应该说考的比较全面吧，如果做题比较慢或者分析不清的话就会大寄特寄。</p><blockquote><p>考试后的 pyq：</p><p>Cheatsheet 是 TLB，</p><p>CSAPP 是 Page Table，</p><p>考试就是 Page Fault。</p></blockquote><p><img src="https://s2.loli.net/2022/01/08/W3isBlYAtPUhHyX.png" alt="image-20220108001638080"></p><center>《深入理解计算机系统》</center><h4 id="概率论与数理统计"><a href="#概率论与数理统计" class="headerlink" title="概率论与数理统计"></a>概率论与数理统计</h4><p>这门课在培养方案中是大二下必修，这学期提前上了，因为感觉没啥必要放到之后再上。上的是 hjx 老师的概统，这位老师在春季学期并不开课，这学期主要开课是面向大二及大三生命学院及相关书院和院系（？）的同学。</p><p>感觉单就知识传授这一点来说，这门课还是起到了它应该起的作用。但是就另外一些方面而言，这门课给人一种很无情的感觉。作业占总评 35 分，前期作业自己认真做，有错题就直接给 31 或者 32. 后期自己先想个思路，然后直接抄书上的参考答案，每次都是 34 或者 35. 而且同学们在群里问问题助教和教师根本不管答疑。我在答疑坊小程序里看到了好多问这门课的题目的同学，如果恰在我能力范围之内还是帮他们解决了。</p><p>半期后就没去线下上过课，一直线上腾讯会议。第 13 周课的时候下发查询成绩的密码，一张纸写了所有人的学号和密码在教室里传阅，记下密码直接涂掉，感谢舍友去了帮我签到。后来，期末考完试出成绩，有同学在群里说“密码忘记了”，这下助教可是回了消息，直接往群里丢了一个《清华大学本科生学生手册》的 pdf，把“两周不上课自动退学”那一条标了高亮。只能表示很无语。</p><p><img src="https://s2.loli.net/2022/01/08/1fex5FbR2N8gz6S.png" alt="image-20220108003007385"></p><p>出分之后一堆人请求调分。期末考卷咱倒是连带附加题一起都全写了，但还是不知道哪被扣了 7 分。估计对于他们更是雪上加霜吧。希望他们复议成功。</p><p>期末的那个查成绩的可执行 exe 倒是挺有意思，找了半天没找到破解的技巧。</p><blockquote><p>考试后的笑话：</p><p>有一门祖传的概统课学生总成绩的分布满足 $N(\theta, \sigma^2)$。几年前老师进行抽样检测，发现学生平均成绩满足$N(\mu, \delta^2)$。如今期末考完，已经出分，抽取 $n$ 位同学的成绩，均值为 $\bar x$，求 $\theta$ 的贝叶斯估计值。</p></blockquote><h4 id="信息科学技术概论"><a href="#信息科学技术概论" class="headerlink" title="信息科学技术概论"></a>信息科学技术概论</h4><p>不明白为什么这课计 0 年级要在大二上。明明应该是给大一新生<s>画大饼</s>展望未来的课程。八节课一共去了五节。听各个院系的专家们表面展望未来，实际在介绍自己的实验室在做什么，拿着明显不是课程 PPT，而是某个总结汇报 PPT 讲一个半小时真的没啥意思。即使在现场，大多数时间也在学概统，写概统作业。这学期大礼堂的麦克风实在是太吵了，感觉学长们介绍的“补觉”的课程功能基本不存在了。</p><p>大一的时候参加信息学院新生知识竞赛吃了 9 分的 Bonus。小作业基本就是摆烂了，大的综述是认真写的，选题跟现在在做的实验室项目相关。等课程成绩出了之后可以把综述整理整理放到博客上。</p><h4 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h4><p>这学期投入足够多精力想要学好的课程之一。选课的时候没上邓公的课堂，一直在 yx 的课堂里面摸鱼。</p><p><img src="https://s2.loli.net/2022/01/08/9W513bvmtnVHxpr.png" alt="image-20220108214326956"></p><p>PA1 五天，LAB1 半天，PA2 三天，LAB2+LAB3一周，PA3 两天。没打过竞赛，每次 PA 出了之后就靠学堂在线把这一个 Part 的所有部分全部学完。上课就在调代码。感谢这学期有回放，能补到一些学堂在线上没有讲过的内容。虽然最后判断题还是大寄特寄，不过没有回放的话肯定寄得更惨。</p><p>在答疑坊线上线下总计《数据结构》答疑时长 34h。记忆犹新的是 LAB3 截止前的那几个星期。因为是 LAB 出了就开始写，咱还不敢直接抄代码，咱的作业完全是照着讲义上的图然后自己实现了具有相同功能的东西。不过那一个周五晚上让我彻底背熟了邓公的 AVL 和 Splay 树的代码，包括哪个地方在 g++ 上编出来有运作问题。看着他们大段 Ctrl+C/V 怀疑自己浪费了大量时间在调自己的三棵树上。红黑树没见到多少人抄，可能是太复杂了。</p><blockquote><p>我家门口有三棵树，一颗是 AVL 树，一颗是 Splay 树，另一颗是 RedBlack 树。</p></blockquote><p>期末考试是寄了，左式堆的合并上来画了一个右撇子堆上去（画着画着人就傻了），Splay 的删除折叠根本想不到竟然不是题目白给分，是我白给题目分。判断题“这都哪跟哪啊”。上来做判断题看了两个题心态快没了，写了 KMP 和 Hash。然后过了一遍判断题，填了一半多。把 Splay 送了。又过了一遍选择题，填了剩下的一半多。把左式堆又送了。又过了一遍选择题，剩下的全部 random putchar 了，然后搞最后一题去，只求对了个期望。（事实证明这不是个正确的选择，因为后面的几道大题根本就没拿到多少分数，前面判断题还直接因为 <code>putchar(rand() % 2 ? &#39;O&#39; : &#39;X&#39;);</code> 大型白给）</p><h4 id="马克思主义原理"><a href="#马克思主义原理" class="headerlink" title="马克思主义原理"></a>马克思主义原理</h4><p>wfm, yyds. 认真听了每一节课，认真做了每一节课的笔记。因为知道老师就是这个风格，所以倒也是抱着硬逼着自己听政治课的想法选了这个课堂。没想到授课内容真的十分有意思。</p><p>虽然不指望说真的把马原理学懂了，但是感觉自己还是掌握了一些基本的方法论。比起那种教条地，直接地，带有政治性的说教，政治课如果是这个样子，带有逻辑推理，层层递进的话，可能能更加吸引我吧。</p><p>期末论文反卷工作做的十分好，1k~2k 字，多写无益。因为上课听过课所以思路很快就整理好了。</p><h4 id="形式语言与自动机"><a href="#形式语言与自动机" class="headerlink" title="形式语言与自动机"></a>形式语言与自动机</h4><p>选的是 ly 的自动机。ly 和 wsy 肯定是要推荐 ly 的，因为 ly 的 PPT 中例子的数目明显地多于 wsy。感觉听上课 PPT Reader 式的讲解没啥意思，半期后就不去上课了。</p><p>但是肯定还是要自己看 PPT 和书的。不去上课是因为我自己看的话比 Reader 读得还快就是了（</p><h4 id="复变函数引论"><a href="#复变函数引论" class="headerlink" title="复变函数引论"></a>复变函数引论</h4><p>你大伯还是你大伯。讲课比较侧重例子，平时作业咱也有认真写。最后期末考试前画了题库，最后考试就只顾着拟合和完美复刻就行，训练集和测试集都一样了。不过来选大伯的课的人，估计也没几个想认真动脑子学复分析入门的吧（</p><p>时不时有点后悔没有选 ygw 的复变。但看舍友被折磨的样子还是庆幸当时自己没选。</p><h4 id="二年级男生游泳"><a href="#二年级男生游泳" class="headerlink" title="二年级男生游泳"></a>二年级男生游泳</h4><p>c7w，旱鸭子，身体就是笨（x</p><p>在第七周第八周的时候，大部分班级成员都已经会游泳了的时候，c7w 还不会收翻蹬夹。被体育老师特殊关照。</p><p>第十周末开始下定决心，因为第十二周就要考了。</p><p>连续十四天，每天都去水里泡，十二周还是翻车了，然后继续泡（</p><p>那段时间内 B 站首页的视频推荐都是游泳教程（</p><p>Anyway 十三周补考过了。虽然最后只有 2min55s，分数只有游下来的基本分数，不过自己也挺满意了。</p><h4 id="常微分方程"><a href="#常微分方程" class="headerlink" title="常微分方程"></a>常微分方程</h4><p>4 月，大一的憨憨 c7w 报了数学辅修，很勇啊。</p><p>ODE 第一次退课阶段就扔了，倒不是作业不会做了，也不是课听不懂了，只是因为其他方面的压力一股脑的涌过来，感觉没有任何喘息的机会。这时目光果断移向一周布置两次作业，占用每周时间要大于 8h 的 ODE。快刀斩乱麻，退。</p><p>估计数辅也就可能这么咕咕咕了。不过 c7w 并不后悔。因为如果将生活视作是时间有限的一个0/1背包问题，那么舍弃一些可能让自己更有成就感和获得感，但是可能耗费极大的东西至少应该不能说是错的吧（</p><h3 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h3><p>还有一些咱认为不属于上述课程，但是也应该算作是课业部分的东西，这里一并整合过来了吧。</p><h4 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h4><p>笔记在逐步整理放在博客上。虽然现在还很乱就是了。</p><ul><li>3B1B 《But what is a neural network?》</li><li>2017，李宏毅《机器学习》</li><li>2021，李宏毅《机器学习》</li><li>tensorflow/pytorch <s>的安装与卸载</s>的基于复制粘贴代码和敲命令行运行的模型训练</li></ul><h4 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h4><ul><li>三篇论文介绍原理<ul><li>LSTM</li><li>GPT</li><li>BERT</li></ul></li></ul><p>我自己都不知道算不算是入门了。总之寒假要做相关的挑战杯项目的话还是要好好补一补。</p><h4 id="图形学"><a href="#图形学" class="headerlink" title="图形学"></a>图形学</h4><p>参加了科协组织的“计算机图形学”兴趣小组。笔记在逐步完善并放到博客上。</p><ul><li>光栅化</li><li>着色</li><li><p>几何</p></li><li><p>光线追踪（本来应该学完的，奈何因为期末周停摆了，最近一定补上）</p></li></ul><h2 id="社工志愿总结"><a href="#社工志愿总结" class="headerlink" title="社工志愿总结"></a>社工志愿总结</h2><p>c7w 感觉自己是应该<strong>创造自己的价值</strong>的。如果再像高中那样，每天只是吃饭，睡觉，打游戏，刷视频和刷题的话，必定不会给自己带来什么改变。</p><h3 id="计算机系学生科协（SAST9）"><a href="#计算机系学生科协（SAST9）" class="headerlink" title="计算机系学生科协（SAST9）"></a>计算机系学生科协（SAST9）</h3><h4 id="Docs-9"><a href="#Docs-9" class="headerlink" title="Docs 9"></a>Docs 9</h4><p>P.S. 为避免将你系科协与别的系的科协混淆，这里我倾向于称呼其为 SAST9。</p><p>进了网络部，跟着 holder 搞技能引导文档。现在也算是有点眉目了吧。 <a href="https://docs.net9.org/">https://docs.net9.org/</a></p><p>最初负责整理暑培的讲义资料，整理着整理着 Python 部分自己又加了点新东西和学习感悟进去。</p><p>期待各位 dalao 能给文档带来更多的贡献，多多 PR（鞠躬</p><p><s>（碎碎念：感觉 holder 就没分给我多少锅做</s></p><h4 id="CodePlay"><a href="#CodePlay" class="headerlink" title="CodePlay"></a>CodePlay</h4><p>各位还没看到的一个项目。有一个与下个学期的学生节活动配套的网站。是舍友带头接的宣中的外包项目。</p><p>最初分工是我写后端，另外三位写前端。我后端写了两天搞完了，给他们写了个完善的接口调用文档。</p><p><img src="https://s2.loli.net/2022/01/08/BYQLOEKsFmrAzP4.png" alt="image-20220108230237366"></p><p>一周后我来查看前端完成进度，React 的数据流还是有 Bug，建议他们采用 React Redux。最后我还是参与搞了前端代码，把逻辑修的差不多了，把代码架构重构了一份。但是问题是，前端实在是丑。部件的 style 都是 hard-coding，占用的像素值都是固定的。实在是感觉改起来有心无力。“有心”是指感觉到手里这玩意确实丑的不行，“无力”是指自己确实当时也比较忙，没有时间把整个前端的设计推倒重来一次。</p><p>最后还是带着愧疚感地交了任务。虽然前端还是丑丑丑，更不用说移动端适配了。</p><p>得到的经验教训是，下学期的《软件工程》课程我一定要先给他们写好设计文档再让他们开工。</p><h4 id="一二九人物志"><a href="#一二九人物志" class="headerlink" title="一二九人物志"></a>一二九人物志</h4><p><a href="https://stu.cs.tsinghua.edu.cn/exhibition129/game/">https://stu.cs.tsinghua.edu.cn/exhibition129/game/</a></p><p>在系里的公众号上推过。本来想在一二九合唱比赛中摸鱼，进了开发组，没想到比赛停了，开发组倒成了宣发主力。</p><p><img src="https://s2.loli.net/2022/01/08/RdaY5SGLuU9P34T.png" alt="image-20220108225849152"></p><p>这次前端基本是我一个人在写，也做了不少调试工作。唯一感觉这个游戏完成度不太高的一点是，明明后端是可以应对任意多问题的一个框架，文案组那边却只给了 5 个问题。这就好比像是求数列的前 $5$​​​ 项和，却把 $S_n$​​​​ 给求出来了，然后令 $n \leftarrow 5$​​​​ 一样。美工组的同学们十分给力，<s>我直呼画师太太带带我</s>。</p><p>后续加了个后端统计访问次数。学了 Highchart 库，感觉用起来还不错。</p><p><img src="https://s2.loli.net/2022/01/08/qxVkZO57ry2oUvn.png" alt="image-20220108225927735"></p><p>（我的天呐这游戏怎么期末周访问量还在涨，怎么还在有人看.jpeg</p><h3 id="计算机系课咨委"><a href="#计算机系课咨委" class="headerlink" title="计算机系课咨委"></a>计算机系课咨委</h3><p>最初是作为志愿者，协助参与课程改革和对系内氛围的看法的相关访谈调研。也算是借此为契机和舍友以及其他好友好好谈了谈对于系内的氛围，以及某些课程的看法。</p><p>然后做的工作就越来越正规化，比如调研计组和网原本学期在修学生对这两门课的看法。不过 c7w 的性格可能还是比较内向，在做相关调研工作的时候还是倾向于找自己认识的人，由此可能会造成结果有一定程度的偏差需要矫正吧。</p><p>最后成功留任为了这个新晋社工组织的正式委员。希望在未来可以继续为推动系内的氛围变化尽一份力吧…</p><h3 id="答疑坊志愿者"><a href="#答疑坊志愿者" class="headerlink" title="答疑坊志愿者"></a>答疑坊志愿者</h3><p>国庆节假期期间面试，被问了微积分和线性代数。问了微积分一道证明，不会。改换了一道积分。暴力做出来了。线性代数，让我用定义证明单射当且仅当存在左逆。靠着脑中被覆盖刷写过无数遍的离散数学知识磕磕绊绊地讲了出来。本来以为面试肯定无了，没想到最后却被录用了。</p><p>截止到目前总答疑次数为 192 次，总志愿工时 114 + 3*10 小时。还记得第一个周五去值班的时候，学弟让我把微分中值定理那节课的所有课后习题都讲了一遍。之后还有讲用黎曼积分的定义证明命题，用早已忘了的线性代数实对称阵的性质解决问题…只有到我自己讲东西的时候，才能理解教师们授课时的感受吧。现在的我，即使是和那些被我称为 PPT Reader 的教师们相比，表达能力上肯定也是不如他们的。更不用说他们的主业是要负责他们自己的科研了。</p><p>从某次某个人找不到 DSA 答疑者，来找我调代码的时候，我的主营科目好像发生了一些变化。我发现与其让我当场讲数学，可能调代码更适合我。于是在线下值班的时候，我优先开始答疑程设和 DSA 的 PA/LAB. 当然这并不意味着我丧失了在每个周五的晚上被数学题折磨的机会（悲（这个人的大一数学课到底怎么拿的 A 和 A+ 啊，赶紧重修算了</p><p>当然，简单的微积分题，线性代数题，乃至于正在学的概统题等等，这些才是线上答疑订单的主要组成部分。比起把一件事物讲出来，可能我更擅长于用纸和笔把一件事物写明白。</p><p><img src="https://s2.loli.net/2022/01/08/twmOcCqurd9VeXP.png" alt="image-20220108232546326"></p><p>让我真正意识到我这学期的工作十分具有价值的一个瞬间。也是让我坚持要继续做下去的再一次确认。</p><h3 id="心理中心自助项目组后台及常务管理"><a href="#心理中心自助项目组后台及常务管理" class="headerlink" title="心理中心自助项目组后台及常务管理"></a>心理中心自助项目组后台及常务管理</h3><p>从 9 月份开始，在心理中心自助项目组进行勤工工作，总工作时长 160 小时。主要负责的是积极心理云训练营和正念冥想训练营。前者主要是负责后台数据统计分析，以及常务的任务发放和流程管理；后者的话因为新成员的加入，活的话轻了不少，但是前期联络成员和中期的数据统计压力还是比较大的。</p><p>寒假一定要写个自动化的程序来帮我做事.jpeg </p><p>拉舍友入坑肯定要让他帮我干我不想干的活.jpeg</p><h2 id="碎碎念"><a href="#碎碎念" class="headerlink" title="碎碎念"></a>碎碎念</h2><h3 id="关于性格与心态"><a href="#关于性格与心态" class="headerlink" title="关于性格与心态"></a>关于性格与心态</h3><p>希望自己能够，不说变得外向，至少能够变得开朗一点。要实现起来还是无比地困难呐。</p><p>奇怪的一点是，感觉自己在人前的表现力会随着和人的熟识程度成 U 型曲线。如果是陌生人或是已经打成一片的人来说，可能咱并不会感到十分惊恐。可是对于熟识程度适中的人，总是不知道该如何面对才好。</p><p>至少在网络上不再是个社恐人了。希望现实中早日也是如此吧…至少请不要在和别人正常交流的过程中感到无力感了吧。</p><p>// 这么看来一年变化还是很大的嘛，明明一年前还把自己比作孤岛呢</p><p>至于心态，应该是变得敢于去尝试新事物了。面对困难不会束手等死，而可能会想尽自己的一切办法去克服。我觉得这可能是很难能可贵的一点吧。</p><h3 id="关于-Minecraft"><a href="#关于-Minecraft" class="headerlink" title="关于 Minecraft"></a>关于 Minecraft</h3><p>我想热爱是不会褪去的，它会以另一种方式继续延续下去。</p><div style="display: flex; justify-content: center; align-items: center">    <iframe src="//player.bilibili.com/player.html?aid=804019066&bvid=BV1sy4y1K72L&cid=367317531&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="width: 600px; height: 400px"> </iframe></div><p>社团那边不管是线上还是线下活动都一直在摸，好像这一年就没有几次打开过这个游戏呢（叹气</p><p>关于 MCBBS，八月乙烯被锤，一气之下退了翻译&amp;新闻群。也许不回头才是正确的选择吧。但我仍然珍惜通过 MCBBS 认识的好友们，倒不如说，感谢 MCBBS 的存在，能够在初中和高中的时候陪我走过一段精神上十分艰难的时期吧。</p><p>对于泥潭现在的氛围，我只能说，（精力上）穷者独善其身。泥潭现在的氛围，在某种程度上何尝不是社会的一个缩影。这可能也仅仅是我这个悲观的现实主义者的看法。</p><h3 id="关于生活的-BGM"><a href="#关于生活的-BGM" class="headerlink" title="关于生活的 BGM"></a>关于生活的 BGM</h3><p>冬天的时候，听着《<a href="https://music.163.com/song?id=1404798043&amp;userid=304573076">Hello &amp; Bye, Days</a>》度过了一个寒假。暖春回归，听着《<a href="https://music.163.com/song?id=458507422&amp;userid=304573076">春待ちクローバー</a>》(Harumachi Clover)任眼前的四叶草生长，即便知道春天与四叶草终将随风而去。夏天的时候，《<a href="https://music.163.com/song?id=1835620512&amp;userid=304573076">Sing My Pleasure</a>》似乎成为了春季学期后半期和暑假的主旋律。秋天，某部番的 ED《<a href="https://music.163.com/song?id=1423038384&amp;userid=304573076">夜空</a>》是那么的令人徜徉在幻想之中，但是比起在紫操上看繁星点点，也许阴云才更适合我吧。</p><h3 id="关于未来"><a href="#关于未来" class="headerlink" title="关于未来"></a>关于未来</h3><p>首先还是希望自己能活过接下来的一年（卑微，希望身体状态不要出啥问题…连着几周没有晚上两点前睡过觉了，总之趁寒假初的几天先好好睡会儿吧。然后是锻炼…但是天好冷，flag 还没立估计就倒了（</p><p>然后，希望自己能组织好自己的生活，平衡好自己的生活。</p><blockquote><p><em>节选自《写在 0x12 的最后一晚》</em></p><p>我不再否定我自己的过去，因为恰恰是过去的我塑造了我的现在；</p><p>我会努力地在我的生活中创造出一些影响，以让我自己感受到我的价值；</p><p>我会努力地控制自己的生活，勇敢地去尝试更多未知的事物，虽然大概率可能失败；</p><p>我会尽量不让自己陷入生产队的阿米娅的境地，留出足够的时间给自己，给周围的人。</p><p>Hello, 0x13.</p><p align="right">c7w</p><p align="right">2021-11-17</p></blockquote><center><p style="font-weight: bold">朝着理想中的自己，加油。</p></center><p>Last updated by c7w, 2022-1-9 18:29:50. Revised.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;于 2022 年初，对 2021 的年度总结。&lt;/p&gt;</summary>
    
    
    
    <category term="日志" scheme="https://www.c7w.tech/categories/%E6%97%A5%E5%BF%97/"/>
    
    <category term="日志/总结" scheme="https://www.c7w.tech/categories/%E6%97%A5%E5%BF%97/%E6%97%A5%E5%BF%97-%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="总结" scheme="https://www.c7w.tech/tags/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>Continue blogging, 2022!</title>
    <link href="https://www.c7w.tech/update-2022/"/>
    <id>https://www.c7w.tech/update-2022/</id>
    <published>2022-01-03T03:50:13.000Z</published>
    <updated>2022-01-10T13:03:17.313Z</updated>
    
    <content type="html"><![CDATA[<p>写于 2022/1/3，考完 DSA 之后的一天。</p><p>既然氪金买了自己的服务器，虽然博客还是放在 GitHub Pages 上，但是想了想还是好好整整自己的博客吧。</p><p>（这两句话之前好像没有什么因果关系</p><a id="more"></a><h2 id="初步规划"><a href="#初步规划" class="headerlink" title="初步规划"></a>初步规划</h2><ul><li>魔改页面模板的源码<ul><li>修改文章的摘要摘取机制</li><li>增加背景图片</li></ul></li><li>翻一遍站点的配置文件</li><li>SEO</li><li>侧边栏及导航栏管理<ul><li>友链</li><li>社交链接</li><li>分类</li><li>关于</li></ul></li><li>更改文章的标签和分类机制，对所有文章做一遍清洗，适当拆分/合并<ul><li>分类应有一级分类 “/技术” “/日常” 和其下的二级分类 “/技术/Linux” “/技术/Python应用”</li><li>标签应侧重于体现文章的关键词</li></ul></li><li>写一些新博客<ul><li>将一些总结性的内容放到博客上</li><li>写一些新内容</li></ul></li></ul><h2 id="更新实况"><a href="#更新实况" class="headerlink" title="更新实况"></a>更新实况</h2><h3 id="2022-1-10"><a href="#2022-1-10" class="headerlink" title="2022/1/10"></a>2022/1/10</h3><ul><li>洗了一部分博客的文章，进行标签和分类更改</li><li>更改了 butterfly 有 bug 的博客页面 head 生成机制</li><li>更改了摘要生成机制</li><li>更改页面字体大小</li></ul><p>晚上：</p><ul><li>博客文章分类基本整理完成</li><li>大致做了下 SEO4</li><li>写了 About 页面</li><li>重写了 Friends 页面</li><li>应该差不多啦w</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;写于 2022/1/3，考完 DSA 之后的一天。&lt;/p&gt;
&lt;p&gt;既然氪金买了自己的服务器，虽然博客还是放在 GitHub Pages 上，但是想了想还是好好整整自己的博客吧。&lt;/p&gt;
&lt;p&gt;（这两句话之前好像没有什么因果关系&lt;/p&gt;</summary>
    
    
    
    <category term="日志" scheme="https://www.c7w.tech/categories/%E6%97%A5%E5%BF%97/"/>
    
    <category term="日志/更新记录" scheme="https://www.c7w.tech/categories/%E6%97%A5%E5%BF%97/%E6%97%A5%E5%BF%97-%E6%9B%B4%E6%96%B0%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="站点日志" scheme="https://www.c7w.tech/tags/%E7%AB%99%E7%82%B9%E6%97%A5%E5%BF%97/"/>
    
  </entry>
  
  <entry>
    <title>GAMES101 现代计算机图形学基础 笔记（上）</title>
    <link href="https://www.c7w.tech/games101/"/>
    <id>https://www.c7w.tech/games101/</id>
    <published>2021-12-09T01:48:20.000Z</published>
    <updated>2022-01-16T14:29:28.538Z</updated>
    
    <content type="html"><![CDATA[<p>GAMES 101 现代计算机图形学基础 笔记（上篇）.</p><ul><li>变换与齐次坐标</li><li>光栅化（反走样与深度缓冲）</li><li>着色（Blinn-Phong 着色模型，图形管线，纹理映射，凹凸贴图）</li><li>几何（曲线与曲面，贝塞尔曲线）</li></ul><a id="more"></a><h2 id="变换-Transformation"><a href="#变换-Transformation" class="headerlink" title="变换 Transformation"></a>变换 Transformation</h2><h3 id="为什么要学习变换？"><a href="#为什么要学习变换？" class="headerlink" title="为什么要学习变换？"></a>为什么要学习变换？</h3><p>变换分为模型变换和视图变换。</p><p>模型变换的例子：</p><ul><li>跳舞机器人的运动</li><li>PIXAR 的开场动画</li></ul><p>视图变换：</p><ul><li>3D 空间到 2D 的投影</li></ul><h3 id="二维变换"><a href="#二维变换" class="headerlink" title="二维变换"></a>二维变换</h3><h4 id="缩放变换（Scale）"><a href="#缩放变换（Scale）" class="headerlink" title="缩放变换（Scale）"></a>缩放变换（Scale）</h4><script type="math/tex; mode=display">x'=s_xx \\y'=s_yy \\\begin{bmatrix}x' \\ y' \end{bmatrix}=\begin{bmatrix} s_x & 0 \\ 0 & s_y\end{bmatrix}\begin{bmatrix}x \\ y \end{bmatrix}</script><h4 id="反射变换（Reflection）"><a href="#反射变换（Reflection）" class="headerlink" title="反射变换（Reflection）"></a>反射变换（Reflection）</h4><script type="math/tex; mode=display">x'=-x \\y'=y \\\begin{bmatrix}x' \\ y' \end{bmatrix}=\begin{bmatrix} -1 & 0 \\ 0 & 1\end{bmatrix}\begin{bmatrix}x \\ y \end{bmatrix}</script><h4 id="切变变换（Shear）"><a href="#切变变换（Shear）" class="headerlink" title="切变变换（Shear）"></a>切变变换（Shear）</h4><script type="math/tex; mode=display">x'=x+a y \\y'=y \\\begin{bmatrix}x' \\ y' \end{bmatrix}=\begin{bmatrix} 1 & a \\ 0 & 1\end{bmatrix}\begin{bmatrix}x \\ y \end{bmatrix}</script><h4 id="旋转（Rotation"><a href="#旋转（Rotation" class="headerlink" title="旋转（Rotation)"></a>旋转（Rotation)</h4><p>About (0,0), CCW(Counter clockwise) by default.</p><script type="math/tex; mode=display">R_\theta =\begin{bmatrix}\cos\theta & -\sin \theta \\\sin\theta & \cos \theta\end{bmatrix}</script><h4 id="线性变换（Linear）"><a href="#线性变换（Linear）" class="headerlink" title="线性变换（Linear）"></a>线性变换（Linear）</h4><script type="math/tex; mode=display">\begin{bmatrix}x' \\ y' \end{bmatrix}=\begin{bmatrix} a & b \\ c & d\end{bmatrix}\begin{bmatrix}x \\ y \end{bmatrix}</script><h3 id="齐次坐标-Homogeneous-Coordinates"><a href="#齐次坐标-Homogeneous-Coordinates" class="headerlink" title="齐次坐标 (Homogeneous Coordinates)"></a>齐次坐标 (Homogeneous Coordinates)</h3><p><strong>平移</strong>不能有上述的矩阵表示。</p><script type="math/tex; mode=display">\begin{bmatrix}x' \\ y' \end{bmatrix}=\begin{bmatrix} a & b \\ c & d\end{bmatrix}\begin{bmatrix}x \\ y \end{bmatrix}+\begin{bmatrix}T_x \\ T_y \end{bmatrix}</script><p>但是我们希望使用简单的表示方法。有无能将上述变换统一的表示方式？</p><p>增加坐标的第三维度！(w-coordinate)</p><ul><li>2D Point: $(x,y,1)^T$</li><li>2D Vector: $(x, y, 0)^T$</li></ul><script type="math/tex; mode=display">\begin{bmatrix}x' \\ y' \\ w' \end{bmatrix}=\begin{bmatrix} 1 & 0 & t_x \\ 0 & 1 & t_y \\0 & 0 & 1\end{bmatrix}\begin{bmatrix}x \\ y \\ 1 \end{bmatrix}=\begin{bmatrix}x+t_x \\ y+t_y \\ 1 \end{bmatrix}</script><ul><li>Valid Operations on w-coordinate<ul><li>Vec + Vec = Vec</li><li>Point - Point = Vec</li><li>Point + Vec = Point</li><li>Point + Point = ?<ul><li>Let $(x,y,w)^T := (\frac x w, \frac y w, 1), w\ne 0$</li><li>几何意义是 n 等分点</li></ul></li></ul></li></ul><p>这样我们就能将仿射变换转换成统一的矩阵表示了。</p><h3 id="逆变换（Inverse-Transform）"><a href="#逆变换（Inverse-Transform）" class="headerlink" title="逆变换（Inverse Transform）"></a>逆变换（Inverse Transform）</h3><p>$M^{-1}$ is the inverse of transform $M$ in both a matrix and geometric sense.</p><h3 id="变换的合成（Composing-Transforms）"><a href="#变换的合成（Composing-Transforms）" class="headerlink" title="变换的合成（Composing Transforms）"></a>变换的合成（Composing Transforms）</h3><p>复杂的变换可以通过简单的变换得到;在变换的复合过程中，先后次序很重要.</p><h3 id="三维变换"><a href="#三维变换" class="headerlink" title="三维变换"></a>三维变换</h3><p>可按照上述的思路，构造出 4x4 矩阵表示的三维空间中的齐次坐标。</p><p>特别的，三维空间中的旋转需要固定旋转轴。</p><p>欧拉角 $R_{xyz}(\alpha, \beta,\gamma)=R_x(\alpha)R_y(\beta)R_z(\gamma)$​</p><h3 id="观测变换（Viewing-Transformation）"><a href="#观测变换（Viewing-Transformation）" class="headerlink" title="观测变换（Viewing Transformation）"></a>观测变换（Viewing Transformation）</h3><h4 id="视图变换（View-Camera-Transformation）"><a href="#视图变换（View-Camera-Transformation）" class="headerlink" title="视图变换（View/Camera Transformation）"></a>视图变换（View/Camera Transformation）</h4><p>什么是视图变换？我们可以与拍照过程进行类比。拍照的第一步是模型变换，也就是把模型放在合适的位置上。第二步是找好角度放相机（View Transformation），也就是本节要介绍的视图变换。第三步是做投影变换，将照片定格。</p><p>如何执行视图变换？首先要定义<strong>相机</strong>的概念：</p><ul><li>位置 $\vec e$</li><li>看向 $\hat g$</li><li>相机的向上方向 $\hat t$</li></ul><p>约定俗成地，我们在视图变换时令 $\vec e = \vec 0, \hat g = -\hat z, \hat t = \hat y \ (*)$。</p><p>定义 $M_{view}$ 变换相机，使得经过该变换满足上述 $(*)$。</p><ul><li>Translates $\vec e$ to origin.</li><li>Rotate $\hat g$ to $-\hat z$.</li><li>Rotate $\hat t$ to $\hat y$.</li><li>Then naturally $\hat g \times \hat y = \hat x$.</li></ul><p>Then the calculation process:</p><script type="math/tex; mode=display">M_{view} = R_{view}T_{view} \ (**)\\T_{view} = \begin{bmatrix} 1&0&0&-x_e\\0&1&0&-y_e\\0&0&1&-z_e\\0&0&0&1\end{bmatrix}\\R_{view}^{-1} = \begin{bmatrix} x_{\hat g \times \hat t}&x_{\hat t}&x_{-\hat g}&0\\y_{\hat g \times \hat t}&y_{\hat t}&y_{-\hat g}&0\\z_{\hat g \times \hat t}&z_{\hat t}&z_{-\hat g}&0\\0&0&0&1\end{bmatrix}\\R_{view} = (R_{view}^{-1})^T =\begin{bmatrix} x_{\hat g \times \hat t}&y_{\hat g \times \hat t}&z_{\hat g \times \hat t}&0\\x_{\hat t}&y_{\hat t}&z_{\hat t}&0\\x_{-\hat g}&y_{-\hat g}&z_{-\hat g}&0\\0&0&0&1\end{bmatrix}\\</script><p>我们要做的，是把所有物体都应用这个变换，保证相机与物体的相对运动性质不变。</p><p>视图变换也被称为 ModelView Transformation.</p><h4 id="投影变换（Projection-Transformation）"><a href="#投影变换（Projection-Transformation）" class="headerlink" title="投影变换（Projection Transformation）"></a>投影变换（Projection Transformation）</h4><p>计算机图形学中的投影：是将 3D 模型形成二维图像，有两种投影方式，正交投影与透视投影。正交投影（Orthographic projection）原来平行的线仍然平行；透视投影（Perspective projection）平行线不再平行，会有近大远小的现象。</p><h5 id="正交投影"><a href="#正交投影" class="headerlink" title="正交投影"></a>正交投影</h5><p>首先我们需要将相机摆在合适的位置（具体来说是满足上述 $(*)$​ 的条件），然后我们可以直接无视掉 Z 坐标，将剩余的物体直接平移并缩放到 $[-1, 1]^2$​​。</p><p>正交变换实际上是把空间立方体 $[l, r] \times [b,t] \times [f,n]$ 映射到标准正方体 $[-1,1]^3$​ 的过程。</p><p><img src="https://i.loli.net/2021/11/06/GjCdAz8hv9PtQle.png" alt="image-20211106214250684"></p><center>正交投影的过程</center><p>具体来说，我们有：</p><script type="math/tex; mode=display">M_{\text {ortho }}=\left[\begin{array}{cccc}\frac{2}{r-l} & 0 & 0 & 0 \\0 & \frac{2}{t-b} & 0 & 0 \\0 & 0 & \frac{2}{n-f} & 0 \\0 & 0 & 0 & 1\end{array}\right]\left[\begin{array}{cccc}1 & 0 & 0 & -\frac{r+l}{2} \\0 & 1 & 0 & -\frac{t+b}{2} \\0 & 0 & 1 & -\frac{n+f}{2} \\0 & 0 & 0 & 1\end{array}\right]</script><h5 id="透视投影"><a href="#透视投影" class="headerlink" title="透视投影"></a>透视投影</h5><p>透视投影的特性是平行线相交于一点，近大远小。</p><p>回忆：$(x,y,z,1), (kx,ky,kz,k\ne0), (xz,yz,z^2,z\ne0)$​ 都代表同一点</p><ul><li>做透视投影的步骤：<ul><li>首先将视锥压成正方体 $M_{persp\rightarrow ortho}$</li><li>然后进行正交投影 $M_{ortho}$</li></ul></li></ul><p><img src="https://s2.loli.net/2022/01/16/K8cDzW3Nu4XjBLi.png" alt="image-20211016010006341"></p><center> 做透视投影的步骤 </center><p>这里我们略去透视投影矩阵的推导，直接给出透视投影转换为正交投影矩阵的表示形式。具体的推导可以通过考虑以下特殊点，代入特殊值来决定矩阵的元素。</p><ul><li>近平面的坐标不改变</li><li>远平面的 $x,y$ 坐标被压缩至与近平面相同</li><li>近平面中心点与远平面中心点的位置不变</li></ul><script type="math/tex; mode=display">M_{persp\rightarrow ortho} = \begin{bmatrix}n & 0 & 0 & 0 \\0 & n & 0 & 0 \\0 & 0 & n+f & -nf \\0 & 0 & 1 & 0\end{bmatrix} \\\\M_{persp} = M_{ortho}M_{persp \rightarrow ortho}</script><p>值得注意的是，在我们平常的应用中，我们并不是直接使用 $l, r, b,t$ 来描述一个近平面的位置，而是更倾向于使用 $fovY$​(<strong>field-of-view</strong>, 垂直视角) 和 <strong>aspect ratio</strong> 这两个量来描述一个近平面。</p><p>使用这两个量我们可以轻松地计算出 $l,r,b,t$ 四个近平面参数。</p><p><img src="https://i.loli.net/2021/11/06/NZbxWisdEH78plf.png" alt="image-20211106214646265"></p><h2 id="光栅化-Rasterization"><a href="#光栅化-Rasterization" class="headerlink" title="光栅化 Rasterization"></a>光栅化 Rasterization</h2><h3 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h3><p>在我们进行完 MVP（模型变换，视图变换，投影变换）之后，我们将三维的模型转化成了 $[-1,1]^3$​​ 上的正规立方体。在那之后，我们就要将这个立方体投影到我们的屏幕上，绘出图像了。而这图像的绘制过程，我们便将其称为<strong>光栅化</strong>。</p><p>在此之前，我们必须先给出<strong>屏幕</strong>的定义：</p><p>我们可以将屏幕视为是像素点的二维数组，其中每个像素点由 R, G, B 三种颜色构成。像素点的维数与分辨率相同。进行坐标表示时我们使用 $(x,y)$ 表示 $[x, x+1]\times[y,y+1]$ 这一个像素点，这里 x,y 是整数。</p><p>要将正规立方体绘制到屏幕上，这个过程与 z 是无关的，我们需要首先对 xOy 平面做变换 $[-1,1]^2\rightarrow [0, width] \times [0, height]$，这个过程我们称为<strong>视口变换</strong>。而这里用到的变换矩阵是：</p><script type="math/tex; mode=display">M_{\text {viewport }}=\left(\begin{array}{cccc}\frac{\text { width }}{2} & 0 & 0 & \frac{\text { width }}{2} \\0 & \frac{\text { height }}{2} & 0 & \frac{h e i g h t}{2} \\0 & 0 & 1 & 0 \\0 & 0 & 0 & 1\end{array}\right)</script><p>该怎么把 $[0, width] \times [0, height]$​ 中的内容画到屏幕上呢？这就要用到<strong>光栅化</strong>了。</p><h3 id="三角形的光栅化"><a href="#三角形的光栅化" class="headerlink" title="三角形的光栅化"></a>三角形的光栅化</h3><p>我们接下来要考虑如何将三维空间中的一个三角形光栅化成像素。</p><p>为什么选择三角形？因为三角形是最基础的多边形，任何其它多边形都可以用三角形来表示。同时，三角形的三个顶点一定在同一个平面内。三角形的内部和外部区分的十分清楚，而且可以用叉积来判断点是否在三角形中。</p><p>那么，如果我们得到了三角形的三个顶点在二维平面中的表示 $(x_1, y_1), (x_2, y_2), (x_3, y_3)$​ 之后，我们该如何估计这个三角形所包围的像素点的集合呢？也就是说，该如何判断一个像素（的中心点）和一个三角形的位置关系呢？</p><p>我们可以通过<strong>采样</strong>（Sampling）的方法来实现。采样，实际是将一个函数离散化的过程，这是图形学中的一个核心概念。</p><p>我们再明确一下我们的采样目标：判断某个像素的中心点是否在给定的三角形 $(x_1, y_1), (x_2, y_2), (x_3, y_3)$​​ 内部。即我们要尝试去实现如下的函数：</p><script type="math/tex; mode=display">inside(t, x, y) = \begin{cases} 1, & if \ Point (x,y) \ inside \ triangle \ t, \\0, & otherwise.\end{cases}</script><pre class="language-cpp" data-language="cpp"><code class="language-cpp"><span class="token comment">// Sampling</span><span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> x <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> x <span class="token operator">&lt;</span> xmax<span class="token punctuation">;</span> <span class="token operator">++</span>x<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> y <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> y <span class="token operator">&lt;</span> ymax<span class="token punctuation">;</span> <span class="token operator">++</span>y<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>        image<span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">[</span>y<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">inside</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> x<span class="token operator">+</span><span class="token number">0.5</span><span class="token punctuation">,</span> y<span class="token operator">+</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span></code></pre><p>那么该如何实现 <code>inside(t, x, y)</code> 函数呢？</p><p><img src="https://i.loli.net/2021/11/06/Qx9SjUgGACRuBta.png" alt="image-20211106223216259"></p><p>我们考虑如下的三个叉积。$\overrightarrow {P_0P_1} \times \overrightarrow {P_0Q}$, $\overrightarrow {P_1P_2} \times \overrightarrow {P_1Q}$, $\overrightarrow {P_2P_0} \times \overrightarrow {P_2Q}$ 如果 z 坐标的符号相同，那么点 Q 就一定在三角形内。如果点正好落在边界上，可自行定义解决方案。</p><p><img src="https://i.loli.net/2021/11/06/XLsJyofHK3p9zOn.png" alt="image-20211106225745932"></p><center>光栅化加速：包围盒</center> <p>但是，如果对屏幕的所有元素采样，造成了没有必要的资源浪费。我们使用<strong>包围盒</strong>（Bounding Box）的概念，取三角形边界点的 x, y 坐标分别的最小值或最大值，作为包围盒 x, y 坐标的最小值与最大值。这样我们就能得到一张带锯齿的图像了。</p><p><img src="https://i.loli.net/2021/11/06/sh3uLpPWTYroKQz.png" alt="image-20211106232712939"></p><center>带锯齿的图像</center><p>图像之所以带锯齿（Jaggies）是因为，我们的采样率并不够高，并不足以描述原来的信号，因此就发生了走样（Aliasing）的问题。</p><h3 id="反走样"><a href="#反走样" class="headerlink" title="反走样"></a>反走样</h3><p>首先我们简单介绍一下采样理论。采样瑕疵（Sampling Artifacts）包括多种，在图形学中包括锯齿，摩尔纹等等。采样瑕疵背后都是因为信号的变化太快，以至于采样的速度跟不上信号的变化。而解决办法，便是通过首先对原图进行<strong>模糊</strong>（滤波, pre-filtering then sampling），然后再对其进行采样。</p><h4 id="频域的概念"><a href="#频域的概念" class="headerlink" title="频域的概念"></a>频域的概念</h4><p>频率的概念我们已经熟知，如 $\cos 2\pi fx$ 中频率为 $f$​。用频率可以定义函数周期性变化的快慢。</p><p>我们可以使用傅里叶级数将一个函数（从它的时域）表示成正弦函数和余弦函数的加权和的形式（转换到频域），也可以使用逆傅里叶变换将其从频域转化回时域。</p><p><img src="https://i.loli.net/2021/11/06/XSeh2kvlg97x6UE.png" alt="image-20211106235002520"></p><center>傅里叶变换和逆傅里叶变换</center><p>而<strong>走样</strong>是指，使用同样的采样方式，在高频信号和低频信号两个不同的信号上，得到了相同的采样结果。如下图所示：</p><p><img src="https://i.loli.net/2021/11/08/qPJ81KrkMgR3uoG.png" alt="image-20211108153046165"></p><center>走样的例子</center><h4 id="滤波-Filtering"><a href="#滤波-Filtering" class="headerlink" title="滤波 Filtering"></a>滤波 Filtering</h4><p>滤波是指对于某个特定的信号，仅保留其特定频率的信息。以图像为例，一个图的低通滤波等价于对这个图作用模糊效果，而一个图的高通滤波等价于描绘出这个图像的细节边界。</p><p>我们接下来要说明，对一张图做滤波，等价于对这张图做卷积（分 Box 加权平均）。</p><p><img src="https://i.loli.net/2021/11/08/txp4Ta7ugkn2IZ1.png" alt="image-20211108155427029"></p><center>卷积定理</center><p><strong>卷积定理</strong>是说，对时域的卷积，等价于对频域的乘积；在时域上的乘积，等价于在频域上的卷积。也就是说，如果我们想得到一张图像的滤波，有以下两种选择：</p><ul><li>在时域中对图像做卷积</li><li>先使用傅里叶变换将图像转化到频域，将其与 Box Filter 的频域相乘之后再做逆傅里叶变换</li></ul><p>事实上，这里的 Box Function 就相当于是一个低通滤波器，可以用来将图像模糊化。而这里 Box 越宽，所接受的频率就越低，所得到的图像也就越模糊。</p><h4 id="采样：重复频域上的内容"><a href="#采样：重复频域上的内容" class="headerlink" title="采样：重复频域上的内容"></a>采样：重复频域上的内容</h4><p><img src="https://i.loli.net/2021/11/08/1AHSzyQBpvDseNY.png" alt="image-20211108160150741"></p><p>卷积定理指出，在时域上的乘积，等价于在频域上的卷积。我们将原函数 $X_a(t)$​ 与冲激函数 $P_\delta(t)$ 做乘积，也就等价于将二者都使用 FFT 化归到频域后，将二者做卷积。而这结果表明，采样就是在重复一个原始信号的频谱。</p><p><img src="https://i.loli.net/2021/11/08/MC5PEF42YIJZDmR.png" alt="image-20211108160830500"></p><center>对走样的解释</center><p>而为什么会发生走样呢？采样的不同间隔，会引起频谱以不同的间隔去移动。如果我们采样的不够快，那么信号在频域上就产生了混叠现象。</p><h4 id="反走样的方法"><a href="#反走样的方法" class="headerlink" title="反走样的方法"></a>反走样的方法</h4><p>该怎么才能减少走样错误呢？</p><ul><li>增大采样率 （But very costly &amp; may need high resolution）</li><li>进行反走样操作：让图像的频谱的频宽变窄！如何做到？在采样前去掉高频信号！</li></ul><p>回到我们的问题上来，我们要解决现在存在的锯齿问题，也就是要先对原三角形进行模糊，然后再对其采样。那么该怎么对原来的三角形进行模糊呢？我们使用一个低通滤波器对其进行卷积即可。</p><p><img src="https://i.loli.net/2021/11/08/lCz2uI3KpdNhsiX.png" alt="image-20211108161654357"></p><center>通过计算像素点落在三角形中的平均面积来做滤波</center><p>而在我们具体解决问题的时候，我们选择 Supersampling 的方式，也就是将某个像素分为 $N\times N$ 个采样点，然后对这些采样点的像素值取平均。</p><p><img src="https://i.loli.net/2021/11/10/dlZQAvIWpihb67C.png" alt="image-20211110134128999"></p><p><img src="https://i.loli.net/2021/11/10/OabvGcD1oJsdrwP.png" alt="image-20211110134145650"></p><h3 id="深度缓冲"><a href="#深度缓冲" class="headerlink" title="深度缓冲"></a>深度缓冲</h3><p>本节我们主要立足于解决可见性/遮挡问题，解决方法就是<strong>深度缓存/深度缓冲</strong>（Z-Buffering）。</p><p>只有将模型按照一定的顺序放在屏幕上，才能达到正确的效果。直观的想法是，先画远处的，再画近处的，这样才能让近处的物体覆盖远处的物体。这样的算法叫做画家算法。而如果假设有 $n$ 个物体，进行 $O(n \log n)$​ 的排序后便可将其画出。但是这种算法的缺陷是，如何定义“远近”，即定义物体离相机所在处的深度，并不容易。</p><p><img src="https://i.loli.net/2021/11/10/K6cD4pCLA5aZSXd.png" alt="image-20211110135626091"></p><center>相互遮挡的例子</center><p>为了解决这个问题，图形学引入了深度缓存的概念。想法就是对屏幕的所有像素额外记录其当前显示物体的最浅深度（深度取正值，表示距离相机的远近）。这样类似于动态规划的算法最终复杂度是 $O(n)$ 的。而且如果我们假设在同一深度处不会出现两个模型，那么不同模型的着色顺序对结果是没有影响的。</p><p><img src="https://i.loli.net/2021/11/10/vkn5R7zfhoNZTjC.png" alt="image-20211110140334554"></p><center> 深度缓存算法 </center><h2 id="Shading-着色"><a href="#Shading-着色" class="headerlink" title="Shading 着色"></a>Shading 着色</h2><h3 id="Blinn-Phong-着色模型"><a href="#Blinn-Phong-着色模型" class="headerlink" title="Blinn-Phong 着色模型"></a>Blinn-Phong 着色模型</h3><p>着色是指引入明暗和颜色不同的过程，但在这里我们定义为对物体应用不同材质的过程。</p><p>于是我们在这里介绍一个简单的着色模型，Blinn-Phong Reflectance Model，其中包括高光，漫反射以及环境光照三部分。值得注意的是这个模型是 OpenGL 和 Direct3D 的默认着色模型。具体来说，其定义的参数如下：</p><p><img src="https://i.loli.net/2021/11/10/hjsMQS1zJblKIOa.png" alt="image-20211110141545558"></p><center> 概念的定义</center><p>着色是具有局部性的，即每次只考虑单个点，不考虑其他物体的存在。这样是无法表现阴影的。</p><h4 id="漫反射-Diffuse-Reflection"><a href="#漫反射-Diffuse-Reflection" class="headerlink" title="漫反射 Diffuse Reflection"></a>漫反射 Diffuse Reflection</h4><p>光线的接收：单位面积上接收到的光照强度为 $\cos \theta = I \cdot n$。</p><p>光线的反射：$I’ = \frac I {r^2}$​</p><p>漫反射后：$L_d = k_d(I/r^2)\max(0,\vec n \cdot \vec l)$​，其中 $k_d$ 是三维的 (R,G,B)，表示漫反射系数。</p><h4 id="高光-Specular-Term"><a href="#高光-Specular-Term" class="headerlink" title="高光 Specular Term"></a>高光 Specular Term</h4><p>基于如果 $v$ 视线方向和镜面方向相近，那么半程向量就跟平面法向相近的事实，我们定义半程向量 $h := bisector(v,l) = \frac {v+l} {||v+l||}$。</p><p>然后令 $L_s = k_s (I/r^2)max(0, \cos \alpha)^p=k_s (I/r^2)max(0, n \cdot h)^p$ 即可，其中 $k_s$ 为高光反射系数。</p><p>为什么要带有 $p$ 次方呢？这是因为我们想让产生高光的夹角处于一个相对较小的范围。</p><p><img src="https://i.loli.net/2021/11/10/lL5AVnJi8DhTZPb.png" alt="image-20211110200642332"></p><h4 id="环境光照-Ambientbu-Term"><a href="#环境光照-Ambientbu-Term" class="headerlink" title="环境光照 Ambientbu Term"></a>环境光照 Ambientbu Term</h4><p>环境光照的强度不取决于物体，我们在这里大胆假设每个地方、每个方向的环境光照的大小都相同。也就是说，这一项会填充图中的黑色区域，将图像整体提升某个光照强度。$L_a := k_aI_a$。</p><h3 id="着色频率"><a href="#着色频率" class="headerlink" title="着色频率"></a>着色频率</h3><p>不同的着色单位会有不同的着色效果，具体来说，可分为以下几种：</p><ul><li><p>逐三角形着色（Flat shading）：将每个三角形视作一个平面，但着色效果不够光滑。</p></li><li><p>逐顶点着色（Gourand shading）：使用插值的方法计算三角形内部的颜色值，但问题在于如何求顶点的法向。</p></li><li>逐像素着色（Phong shading）：对每个像素点进行模型计算。</li></ul><p><img src="https://i.loli.net/2021/11/10/rkst1SbMTCq9c7O.png" alt="image-20211110202506874"></p><p>值得注意的是，着色频率越低并不一定代表着效果越差。若几何体足够复杂，则可能区别甚小。</p><p>接下来就侧重于解决上述着色频率留下的问题：</p><p><img src="https://i.loli.net/2021/11/10/AldyF6mRwpCvb5a.png" alt="image-20211110202753142"></p><center>定义逐顶点的法线</center><p><img src="https://i.loli.net/2021/11/10/GJhlmoHwv2dMWaj.png" alt="image-20211110202853817"></p><center>定义逐像素法线</center><h3 id="图形管线-Graphic-Pipeline"><a href="#图形管线-Graphic-Pipeline" class="headerlink" title="图形管线 Graphic Pipeline"></a>图形管线 Graphic Pipeline</h3><p>从图形到场景的过程描述为图形管线。</p><p><img src="https://i.loli.net/2021/11/10/wmXdbSKyiNF95PJ.png" alt="image-20211110203343148"></p><h3 id="纹理映射-Texture-Mapping"><a href="#纹理映射-Texture-Mapping" class="headerlink" title="纹理映射 Texture Mapping"></a>纹理映射 Texture Mapping</h3><p>纹理映射，事实上是为每个三角形分配一个材质平面的坐标 $(u, v)$。</p><p><img src="https://i.loli.net/2021/11/11/hLBROgEdHIyfrkC.png" alt="image-20211111231357192"></p><p>其中那种拼接起来可以无限重复的材质我们称为 <strong>tiled</strong> texture.</p><h3 id="重心坐标插值"><a href="#重心坐标插值" class="headerlink" title="重心坐标插值"></a>重心坐标插值</h3><p>当我们知道了三角形的三个顶点的属性的时候，如果我们想要实现在三角形内部属性的平滑过渡，就要引入重心坐标的概念。</p><p><img src="https://i.loli.net/2021/11/11/iGVW3jLvqsERew8.png" alt="image-20211111231532132"></p><p>对于一个三角形 ABC 来说，其中 $(x,y) = \alpha A + \beta B + \gamma C$，若 $\alpha + \beta + \gamma = 1$，则称 $(\alpha, \beta, \gamma)$ 为该三角形内的 $(x,y)$ 点的重心坐标，其中 $\alpha, \beta, \gamma \ge 0$。</p><p><img src="https://i.loli.net/2021/11/11/U4jEISXchwARgZV.png" alt="image-20211111231732485"></p><p>而由此推导下去，在 $xOy$ 平面中我们有：</p><script type="math/tex; mode=display">\begin{aligned}\alpha &=\frac{-\left(x-x_{B}\right)\left(y_{C}-y_{B}\right)+\left(y-y_{B}\right)\left(x_{C}-x_{B}\right)}{-\left(x_{A}-x_{B}\right)\left(y_{C}-y_{B}\right)+\left(y_{A}-y_{B}\right)\left(x_{C}-x_{B}\right)} \\\beta &=\frac{-\left(x-x_{C}\right)\left(y_{A}-y_{C}\right)+\left(y-y_{C}\right)\left(x_{A}-x_{C}\right)}{-\left(x_{B}-x_{C}\right)\left(y_{A}-y_{C}\right)+\left(y_{B}-y_{C}\right)\left(x_{A}-x_{C}\right)} \\\gamma &=1-\alpha-\beta\end{aligned}</script><p>使用重心坐标可以方便我们做插值。具体来说，对于三角形 ABC，对于其内部点 $(x,y)=(\alpha, \beta, \gamma)$​，有 $V = \alpha V_A + \beta V_B + \gamma V_C$​。</p><p>但是值得注意的是，重心坐标在投影操作中可能会变化。这就会导致一些插值操作只能在三维空间中计算，比如计算深度插值，应该使用原始的三角形三维坐标来计算，而不应使用投影后的平面坐标。</p><h3 id="纹理应用-Applying-Texture"><a href="#纹理应用-Applying-Texture" class="headerlink" title="纹理应用 Applying Texture"></a>纹理应用 Applying Texture</h3><p>我们可以将纹理的颜色值设置为模型对应位置的漫反射系数，从而达到应用纹理的效果。</p><p>而一个像素对应的纹理的颜色值，则可以通过插值的方式计算得出。</p><p>这里我们可能遇到问题：</p><ul><li>如果材质对应的图像过小怎么办？</li><li>如果材质对应的图像过大怎么办？</li></ul><p>对于前者，我们可以使用双线性插值的方法来缓解。</p><p><img src="https://i.loli.net/2021/11/15/HqSbWhcoGAE8rsK.png" alt="image-20211115164939643"></p><p>而对于后者，我们可以使用 Mipmap / 各向异性过滤的方法来解决。</p><p>事实上，我们可以将纹理视为是内存中的一个数据结构，提供了便捷的查询接口，而非将纹理视为是一张图片。</p><h3 id="用纹理做环境光反射效果"><a href="#用纹理做环境光反射效果" class="headerlink" title="用纹理做环境光反射效果"></a>用纹理做环境光反射效果</h3><p>纹理可以用来做环境光反射的效果。实际上，如果我们将来自环境的光照组织成纹理：</p><p><img src="https://i.loli.net/2021/11/27/8CwUR6qT5uvHSyt.png" alt="image-20211127173639430"></p><center>犹他茶壶</center><p>环境光可以通过记录在球面上，然后将球面展开，就可以得到环境光照对应的纹理。但是这样做在纹理上下会有明显的扭曲现象。</p><p><img src="https://i.loli.net/2021/11/27/xv2lRyBA1a8PDG6.png" alt="image-20211127174058860"></p><p>为了解决这个扭曲的问题，我们使用如下的方案：</p><p><img src="https://i.loli.net/2021/11/27/TmiStDkjVz3IsNA.png" alt="image-20211127174158530"></p><p>将球面上的每个点映射到恰好包围球的包围盒的表面上（沿球面法向）。然后将包围盒展开成六个表面。</p><h3 id="用纹理做凹凸贴图"><a href="#用纹理做凹凸贴图" class="headerlink" title="用纹理做凹凸贴图"></a>用纹理做凹凸贴图</h3><p>事实上，我们的纹理不只是可以用来替换 Blinn-Phong 反射模型中的 $k_d$ 值。我们可以用纹理来描述表面的“属性”，比如待渲染表面的凹凸感，便可以用纹理来记录表面的相对高度。</p><p>其实我们可以用足够多的三角形来模拟出凹凸效果，但是这样势必会导致性能上的衰减。使用纹理来做凹凸贴图可以在不改变原模型的几何复杂程度的情况之下，达到渲染出带有凹凸感的图像的目的。</p><h4 id="凹凸贴图的推导"><a href="#凹凸贴图的推导" class="headerlink" title="凹凸贴图的推导"></a>凹凸贴图的推导</h4><p>计算过程：修改高度值 -&gt; 重新计算法向量值 -&gt; 着色。</p><p><img src="https://i.loli.net/2021/12/01/FfaVTe4LkvhR9u2.png" alt="image-20211201185543817"></p><center> Local Coordinate : 换基 </center><h4 id="位移贴图"><a href="#位移贴图" class="headerlink" title="位移贴图"></a>位移贴图</h4><p>凹凸贴图并没有改变自己的几何，所以在边缘上看不出凹凸感。同时在阴影上也体现不出来凹凸感。而位移贴图实际上改变了模型几何的位置。但我们需要要求模型足够细致。</p><h2 id="Geometry-几何"><a href="#Geometry-几何" class="headerlink" title="Geometry 几何"></a>Geometry 几何</h2><h3 id="几何的表示"><a href="#几何的表示" class="headerlink" title="几何的表示"></a>几何的表示</h3><p>几何的分类：Implicit 的几何和 Explicit 的几何。</p><p>Implicit 的几何是说，满足某些特定关系的点构成的几何，即 $f(x,y,z)=0$；Implicit 对于找出集合中的所有点很困难，但是要想判断某个点在不在其中很容易。</p><p>Explicit 的表示是说，通过参数映射的方法来定义表面，即将平面上的点 $(u,v)$ 映射到三维坐标 $(x,y,z)$​​​. 想找出几何体表面集合中的所有点，只需变化 $u,v$ 的值即可。但是不容易判断某个点在不在其中。</p><p>不同的表示方法，需要根据需要来选择。</p><p>此外，隐式表示还可以用 CSG(Constructive Solid Geometry) 的方法来构造，如 $A \cup B, A\cap B, A\backslash B$.</p><p><img src="https://i.loli.net/2021/12/01/PfKUJDwvi4FC6ZQ.png" alt="image-20211201192043523"></p><center> CSG 的例子 </center><p>隐式表示还可以用距离函数、水平集、分形等等方法来定义。</p><p>显式表示可以用点云，多边形面（obj 文件）等等表示。</p><h3 id="曲线"><a href="#曲线" class="headerlink" title="曲线"></a>曲线</h3><h4 id="贝塞尔曲线"><a href="#贝塞尔曲线" class="headerlink" title="贝塞尔曲线"></a>贝塞尔曲线</h4><p>用一系列的控制点定义出满足某些性质的曲线。</p><p>曲线的起点为控制点的起点，终点为控制点的终点，起始点处与终点处的切线分别为 $\overrightarrow {P_0P_1}, \overrightarrow{P_{n-1}P_n}$.</p><p><strong>如何画贝塞尔曲线：de Casteljau Algorithm</strong></p><p>（1）先考虑三个控制点，生成二次贝塞尔曲线。</p><p><img src="https://i.loli.net/2021/12/01/fE6FrtaL2sDQIGT.png" alt="image-20211201194418429"></p><p>（2）再考虑四个控制点，生成三次贝塞尔曲线。</p><p><img src="https://i.loli.net/2021/12/01/wR7VDnSPCrLxkKY.png" alt="image-20211201194528603"></p><p>(3) 代数形式：插值！</p><p><img src="https://i.loli.net/2021/12/01/pmW5vVa61urdbxI.png" alt="image-20211201194639946"></p><p>给定 $n+1$ 个控制点，有贝塞尔曲线如下：</p><script type="math/tex; mode=display">b^n(t) = b^n_0(t) = \sum_{j=0}^nb_jB_j^n(t) \\B_{i}^{n}(t)=\left(\begin{array}{l}n\\i\end{array}\right) t^{i}(1-t)^{n-i}</script><p>贝塞尔曲线的性质：</p><script type="math/tex; mode=display">(1) \ \ b(0) = b_0; \ b(1)=b_{n}\\(2) \ \ b'(0)=3(b_1-b_0); \ b'(1) = 3(b_3-b_2) \ (For \ cubic \ cases) \\</script><p>(3) 在仿射变换下不变；</p><p>(4) 曲线包含在控制点的凸包（包含所有控制点的最小凸多边形）内。</p><p><strong>逐段形成贝塞尔曲线</strong>：每四个控制点形成一段贝塞尔曲线，并将其连接。</p><p>保证切线光滑？导数连续！保证性质(2)成立即可。</p><p>两段贝塞尔曲线的连续： $C^0$ 连续指点重合，$C^1$ 连续指导数连续，以此类推。</p><h4 id="其它曲线"><a href="#其它曲线" class="headerlink" title="其它曲线"></a>其它曲线</h4><p>样条（Spline），B-样条（basis splines），具有好的局部性。</p><h3 id="曲面"><a href="#曲面" class="headerlink" title="曲面"></a>曲面</h3><h4 id="贝塞尔曲面"><a href="#贝塞尔曲面" class="headerlink" title="贝塞尔曲面"></a>贝塞尔曲面</h4><p><img src="https://i.loli.net/2021/12/01/SbHh4tNQ7RKZWTy.png" alt="image-20211201201257424"></p><h4 id="面上的操作"><a href="#面上的操作" class="headerlink" title="面上的操作"></a>面上的操作</h4><ul><li>Subdivision</li><li>Simplification</li><li>Regularization</li></ul><h5 id="细分"><a href="#细分" class="headerlink" title="细分"></a>细分</h5><p>(1) 分出更多三角形 (2) 将新的三角形的位置改变，使得原模型更加光滑</p><p><strong>Loop Subdivision</strong></p><p><img src="https://i.loli.net/2021/12/01/5BsOmrFeHV29QaI.png" alt="image-20211201202358001"></p><p><img src="https://i.loli.net/2021/12/01/YmIofNMZ3GVULzd.png" alt="image-20211201202518340"></p><p><strong>Catmull-Clark Subdivision</strong></p><p>针对于一般的曲面，而非三角形作为图源。</p><p>定义：非四边形面，奇异点（度不为 4 的点）</p><p>细分的方式：引入点的操作十分简单，取每面的中点和该面的边的中点，并将其连接。</p><p>调整位置的方法：分为三种。</p><p><img src="https://s2.loli.net/2022/01/16/WSt491CxVjMGNHr.png" alt="image-20211201204717217"></p><h5 id="简化"><a href="#简化" class="headerlink" title="简化"></a>简化</h5><p><strong>边坍缩</strong> Edge Collapse</p><p>Quadric Error Metrics 二次误差度量 寻找去 Collapse 某条边后新顶点的位置。</p><p><img src="https://s2.loli.net/2022/01/16/qOX5RmUnkLvfhTJ.png" alt="image-20211201205342215"></p><p>将所有边按二次误差测量排序，坍缩最小值所在边，更新受影响的其他边的权重。用堆来维护！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;GAMES 101 现代计算机图形学基础 笔记（上篇）.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;变换与齐次坐标&lt;/li&gt;
&lt;li&gt;光栅化（反走样与深度缓冲）&lt;/li&gt;
&lt;li&gt;着色（Blinn-Phong 着色模型，图形管线，纹理映射，凹凸贴图）&lt;/li&gt;
&lt;li&gt;几何（曲线与曲面，贝塞尔曲线）&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="理论" scheme="https://www.c7w.tech/categories/%E7%90%86%E8%AE%BA/"/>
    
    <category term="理论/计算机图形学" scheme="https://www.c7w.tech/categories/%E7%90%86%E8%AE%BA/%E7%90%86%E8%AE%BA-%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
    
    <category term="图形学" scheme="https://www.c7w.tech/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>《机器学习》笔记</title>
    <link href="https://www.c7w.tech/ml-notes/"/>
    <id>https://www.c7w.tech/ml-notes/</id>
    <published>2021-09-07T14:33:15.000Z</published>
    <updated>2022-01-10T11:41:07.663Z</updated>
    
    <content type="html"><![CDATA[<p>正如篇名，是机器学习笔记的笔记。主要还是记录一些重要概念和思考的过程吧。</p><p>笔记的笔记中，第一个笔记的链接：</p><ul><li><a href="https://datawhalechina.github.io/leeml-notes/#/">https://datawhalechina.github.io/leeml-notes/#/</a></li></ul><p>对应视频的链接：</p><ul><li><a href="https://www.bilibili.com/video/BV1Ht411g7Ef">https://www.bilibili.com/video/BV1Ht411g7Ef</a></li></ul><p>此外，本文中还收录了部分自监督学习的模型，如 BERT 和 GPT 的运作模式。本文还记录了学习 Transformer 模型的过程。</p><a id="more"></a><h1 id="机器学习笔记"><a href="#机器学习笔记" class="headerlink" title="机器学习笔记"></a>机器学习笔记</h1><h2 id="机器学习介绍"><a href="#机器学习介绍" class="headerlink" title="机器学习介绍"></a>机器学习介绍</h2><h3 id="发展历程及基础概念"><a href="#发展历程及基础概念" class="headerlink" title="发展历程及基础概念"></a>发展历程及基础概念</h3><ul><li>在存在深度学习之前，通过 hand-crafted rules 来设定过滤规则</li><li>机器学习的过程<ul><li>Training<ul><li>Define a set of functions as <strong>Model</strong></li><li>Evaluate the goodness of these functions</li><li>Pick the best function $f^<em>$ from the <em>*Model</em></em></li></ul></li><li>Testing<ul><li>Using $f^*$</li></ul></li></ul></li></ul><p><img src="https://datawhalechina.github.io/leeml-notes/chapter1/res/chapter1-21.png" alt="img"></p><h3 id="相关技术"><a href="#相关技术" class="headerlink" title="相关技术"></a>相关技术</h3><ul><li>监督学习 Supervised learning<ul><li>Tasks<ul><li>Regression<ul><li>The output of the target function $f$ is scalar.</li></ul></li><li>Classification<ul><li>Binary classification (Output: yes/no)</li><li>Multi-class classification</li></ul></li><li>Structured Learning<ul><li>The output is well-structured.</li></ul></li></ul></li><li>How to select function set?<ul><li>Non-linear model, the most famous of which is <strong>Deep Learning</strong></li><li>Other non-linear models, like SVM…</li></ul></li></ul></li><li>半监督学习 Semi-supervised Learning<ul><li>non-labelled data</li></ul></li><li>迁移学习 Transfer Learning<ul><li>Pictures that are not related to the topic could help…?</li></ul></li><li>无监督学习 Unsupervised Learning</li><li>强化学习 Reinforcement Learning<ul><li>我们没有告诉机器正确的答案是什么，机器所拥有的只有一个分数，就是他做的好还是不好</li></ul></li></ul><h2 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h2><p>找到函数$f$，使得对于任意给定特征$x$，输出数值$scalar$.</p><h3 id="Steps"><a href="#Steps" class="headerlink" title="Steps"></a>Steps</h3><ul><li>模型假设，选择模型框架（线性模型）</li><li>模型评估，如何判断众多模型的好坏（损失函数）</li><li>模型优化，如何筛选最优的模型（梯度下降）</li></ul><h3 id="可能出现的问题"><a href="#可能出现的问题" class="headerlink" title="可能出现的问题"></a>可能出现的问题</h3><ul><li>过拟合</li><li>Customize learning rate</li></ul><h3 id="步骤优化"><a href="#步骤优化" class="headerlink" title="步骤优化"></a>步骤优化</h3><ul><li>合并多个线性模型，使用 $\delta$ 函数</li><li>给予更多参数</li><li>正则化 Regularization<ul><li>$L=\sum_n (y-(b+\sum_iw_ix_i)) + \lambda\sum w_i^2$</li><li>使得 Loss function 更加平滑</li></ul></li></ul><h2 id="分析误差"><a href="#分析误差" class="headerlink" title="分析误差"></a>分析误差</h2><p>通过分析误差的来源，以期达到改善模型时有着手点的效果.</p><h3 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h3><ul><li>Average Error = error due to “bias” + error due to variance</li><li>Notation<ul><li>$\hat f$ := the actual function</li><li>$f^*$:= the best function picked from the model trained from the training data</li><li>$f^*$ is an $estimator$ of $\hat f$​ </li></ul></li></ul><p><img src="https://i.loli.net/2021/09/08/eqEIoViJlbtsuyj.png" alt="image-20210908141646887"></p><p><img src="https://i.loli.net/2021/09/08/jmIgTdBbVQr9Al5.jpg" alt="img"></p><ul><li>Conclusion<ul><li>Simple Model<ul><li>Large Bias</li><li>Small Variance</li></ul></li><li>Complex Model<ul><li>Small Bias</li><li>Large Variance</li></ul></li></ul></li></ul><p><img src="https://i.loli.net/2021/09/08/lXDf6xgFKIP3HZs.png" alt="image-20210908143937650"></p><p><strong>How to diagnose?</strong></p><ul><li>If your model cannot fit the training data, then you have large bias. (Underfitting)<ul><li>Redesign your model.<ul><li>Add more features…</li><li>A more complex model…</li></ul></li></ul></li><li>If your model can fit the training data, but has large error on the testing data, then you have the large variance. (Overfitting)<ul><li>More data.<ul><li>Effective but not always practical.</li></ul></li><li>Regularization. (May do harm to bias)</li></ul></li></ul><h3 id="Cross-Validation"><a href="#Cross-Validation" class="headerlink" title="Cross Validation"></a>Cross Validation</h3><ul><li>In each epoch, divide your training set into <strong>training set</strong> and <strong>validation set</strong>.</li><li>Use <strong>training set</strong> to train your model, and use <strong>validation set</strong> to pick the best one.</li><li>Then by this way, the average error of <strong>testing set</strong> could represent the real error when the model is applied.</li><li>What if the <strong>validation set</strong> has its biases? <strong>N-fold Cross Validation</strong></li></ul><p><img src="https://i.loli.net/2021/09/08/B5v8uoTOKUFHEMD.png" alt="image-20210908145850518"></p><ul><li>Firstly pick the best model using the validation approach, then train it using the whole training set.</li></ul><h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><h3 id="Tuning-learning-rates"><a href="#Tuning-learning-rates" class="headerlink" title="Tuning learning rates"></a>Tuning learning rates</h3><p>Visualize the figure of <strong>the loss</strong> and <strong>the turn of parameters updated</strong></p><p><img src="https://i.loli.net/2021/09/08/j64LqduhrsWzeTa.png" alt="image-20210908153835553"></p><h4 id="Adaptive-learning-rates"><a href="#Adaptive-learning-rates" class="headerlink" title="Adaptive learning rates"></a>Adaptive learning rates</h4><ul><li>Popular &amp; Simple Idea: Reduce the learning rate by some factor every few epochs.<ul><li>(E.g.) $\frac 1 t \ Decay$: $\eta^{(t)} = \frac {\eta^{(0)}} {\sqrt {t+1}}$</li><li>But tuning learning rate cannot be one-size for all parameters. That is to say, we need to <strong>give different parameters different learning rates</strong>.</li></ul></li><li><p>Adagrad</p><ul><li>Divide the learning rate of each parameter by the root mean square of its previous derivatives.</li><li>Vanilla gradient descent: $w^{(t+1)} \leftarrow w^{(t)} - \eta^{(t)}g^{(t)}$​​​, $t\ge0$.<ul><li>$g$​​​ is partial derivatives</li></ul></li><li>Adagrad: $w^{(t+1)} \leftarrow w^{(t)} - \frac {\eta^{(t)}}{\sigma^{(t)}}g^{(t)}$, $t\ge0$.<ul><li>$\sigma^{(t)} = \sqrt{\frac 1 {t+1} \sum_{i=0}^t[(g^{(i)})^2]}$</li></ul></li><li>If we use $\frac 1 t\ Decay$ and $Adagrad$ together, we could easily have:<ul><li>$w^{(t+1)} = w^{(t)}-\frac{\eta^{(0)}}{\sqrt {\sum_{i=0}^t[(g^{(i)})^2]}}g^{(t)}$​</li></ul></li></ul></li><li><p>The best step is $\frac{|一阶偏导|}{二阶偏导}$</p></li></ul><h3 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h3><p>随机梯度下降法. Make your training faster.</p><ul><li>每处理一个例子就更新.</li></ul><h3 id="Feature-Scaling-特征缩放"><a href="#Feature-Scaling-特征缩放" class="headerlink" title="Feature Scaling 特征缩放"></a>Feature Scaling 特征缩放</h3><p><img src="https://i.loli.net/2021/09/08/dz9XaEv26imuFY3.png" alt="image-20210908161121857"></p><ul><li>两个输入的分布的范围很不一样，建议把他们的范围缩放，使得不同输入的范围是一样的.</li></ul><p><img src="https://i.loli.net/2021/09/08/hruapvB7snqGtPk.png" alt="image-20210908161952204"></p><h3 id="Possible-Problems"><a href="#Possible-Problems" class="headerlink" title="Possible Problems"></a>Possible Problems</h3><p><img src="https://datawhalechina.github.io/leeml-notes/chapter6/res/chapter6-23.png" alt="img"></p><h2 id="Classification-概率分类模型"><a href="#Classification-概率分类模型" class="headerlink" title="Classification 概率分类模型"></a>Classification 概率分类模型</h2><h3 id="回归模型与概率模型"><a href="#回归模型与概率模型" class="headerlink" title="回归模型与概率模型"></a>回归模型与概率模型</h3><ul><li>回归模型有其缺陷.</li><li>Ideal Alternatives<ul><li><img src="https://datawhalechina.github.io/leeml-notes/chapter10/res/chapter10-7.png" alt="img"></li></ul></li></ul><h3 id="Generative-Model"><a href="#Generative-Model" class="headerlink" title="Generative Model"></a>Generative Model</h3><p><img src="https://datawhalechina.github.io/leeml-notes/chapter10/res/chapter10-9.png" alt="img"></p><ul><li>如何进行问题的转化?<ul><li>两个盒子中抽一个球，抽到的是盒子1中蓝色球的概率是多少？</li><li>相当于两个类别中抽一个 x，抽到的是类别1中 x 的概率是多少？</li><li>可以转化成，随机给出一个 x，那么它属于哪一个类别（属于<strong>概率相对比较大</strong>的类别）？<ul><li>If $P(C_1|x) \ge 0.5$, then output $C_1$​.</li><li>Else output $C_2$​.</li></ul></li></ul></li></ul><ul><li>Prior<ul><li>计算 $P(C_1), P(C_2)$: $P(C_1) = N(C_1)/N(All)$</li></ul></li><li>Probability from Class?<ul><li>我们假设 Training Data 中的数据全部是从一个 Gaussian Distribution 中 sample 出来.</li><li><a href="https://zh.wikipedia.org/zh-hans/%E5%A4%9A%E5%85%83%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83">https://zh.wikipedia.org/zh-hans/%E5%A4%9A%E5%85%83%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83</a></li><li>$f_{\mu, \Sigma}(x) = \frac 1 {(2\pi)^{D/2}}\frac 1 {|\Sigma|^{1/2}} exp\{-\frac 1 2 (x-\mu)^T\Sigma^{-1}(x-\mu)\}$</li></ul></li><li>那么如何找 $\mu$ 与 $\Sigma$ ？<strong>Maximum Likelihood</strong>, 最大似然估计.<ul><li>Likelihood of a <em>Gaussian</em> with mean $\mu$ and covariance matrix $\Sigma$:</li><li>$L(\mu, \Sigma) = \Pi_{i=1}^n f_{\mu, \Sigma}(x^{(i)})$</li><li>Assume that $\mu^<em>, \Sigma^</em>$ is the argument of the Gaussian Distribution with the maximum likelihood.</li><li>And the solution…<ul><li>$\mu^* = \frac 1 n \sum_{i=1}^nx^{(i)}$</li><li>$\Sigma ^{<em>} = \frac 1 n \sum_{i=1}^n (x^{(i)}-\mu^</em>)(x^{(i)}-\mu^*)^T$​}</li></ul></li></ul></li><li><p>Modifying Model</p><ul><li>Using the same Covariant Matrix</li><li>$L(\mu^1, \mu^2, \Sigma)$ <ul><li>Where $\Sigma = \frac {N(C_1)} {N(All)}\Sigma^1 + \frac {N(C_2)} {N(All)}\Sigma^2$ ​</li></ul></li><li>经过推导，我们的 Model 可以写成 $P_{w,b}(C_1|x) = \sigma(z), z=w \cdot x + b$</li></ul></li><li><p>假设所有的feature都是相互独立产生的，这种分类叫做 <strong>Naive Bayes Classifier</strong>（朴素贝叶斯分类器）</p></li></ul><h3 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h3><ul><li>Review<ul><li>Step 1: Function Set<ul><li>We want to find $P_{w,b}(C_1|x)$​.<ul><li>If $P_{w,b}(C_1|x) \ge 0.5$ then output $C_1$, else output $C_2$.</li></ul></li><li>$f_{w,b}(x):=P_{w,b}(C_1|x) = \sigma(z)$, where $z=w\cdot x + b$​</li></ul></li><li>Step 2: Goodness of the function<ul><li><img src="https://i.loli.net/2021/09/09/dT4isEYZSD5QGnH.png" alt="image-20210909203309727"></li><li><img src="https://datawhalechina.github.io/leeml-notes/chapter11/res/chapter11-5.png" alt="img"></li><li><img src="https://datawhalechina.github.io/leeml-notes/chapter11/res/chapter11-6.png" alt="img"></li><li>Cross Entropy</li></ul></li><li>Step 3: Find the best<ul><li><img src="https://i.loli.net/2021/09/09/I2isBxThM9nrNV4.png" alt="image-20210909205827591"></li><li>The partial derivatives are the same as those in linear regression.</li><li>The logistic regression is called discriminative method.</li></ul></li></ul></li></ul><h3 id="Discriminative-v-s-Generative"><a href="#Discriminative-v-s-Generative" class="headerlink" title="Discriminative v.s. Generative"></a>Discriminative v.s. Generative</h3><ul><li>Same model. $P(C_1|x) = \sigma(w\cdot x + b)$<ul><li>Logistic Regression: Directly find $w$ and $b$</li><li>Generative Model: Find $\mu^1$, $\mu^2$, $\Sigma^{-1}$</li><li>But we won’t obtain the same set of $w$ and $b$.</li></ul></li></ul><h3 id="Multi-class-Classification"><a href="#Multi-class-Classification" class="headerlink" title="Multi-class Classification"></a>Multi-class Classification</h3><h4 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h4><p><img src="https://datawhalechina.github.io/leeml-notes/chapter11/res/chapter11-18.png" alt="img"></p><ul><li>Definition of the target</li></ul><p><img src="https://datawhalechina.github.io/leeml-notes/chapter11/res/chapter11-19.png" alt="img"></p><h3 id="Limitation-of-logistic-regression"><a href="#Limitation-of-logistic-regression" class="headerlink" title="Limitation of logistic regression"></a>Limitation of logistic regression</h3><h4 id="Feature-Transformation"><a href="#Feature-Transformation" class="headerlink" title="Feature Transformation"></a>Feature Transformation</h4><p><img src="https://datawhalechina.github.io/leeml-notes/chapter11/res/chapter11-23.png" alt="img"></p><ul><li>Middle Layer!<ul><li>可以将很多的逻辑回归接到一起，就可以进行特征转换.</li><li><img src="https://datawhalechina.github.io/leeml-notes/chapter11/res/chapter11-26.png" alt="img"></li></ul></li></ul><h2 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h2><p><strong>Step 1: Neural Network</strong></p><ul><li>Fully Connect Feedforward Network<ul><li>Why call it deep? <strong>Deep = Many Hidden Layers</strong></li><li>本质：通过 Hidden Layers 进行 Feature Transformation</li></ul></li></ul><p><strong>Step 2: Loss Function</strong></p><ul><li>Cross Entropy</li></ul><p><strong>Step 3: Find the best function</strong></p><ul><li>Gradient Descent</li></ul><h3 id="Why-deep"><a href="#Why-deep" class="headerlink" title="Why deep?"></a>Why deep?</h3><ul><li>More parameters, better performance</li><li><p>Universality Theorem</p><ul><li>Any continuous function $f:R^N \rightarrow R^M$​ can be realized by a network with one hidden layer with enough neurons.</li><li>So why <strong>Deep Learning</strong>, not <strong>Fat Learning</strong>?</li></ul><h2 id="CNN-Convolutional-Neuronal-Network）"><a href="#CNN-Convolutional-Neuronal-Network）" class="headerlink" title="CNN (Convolutional Neuronal Network）"></a>CNN (Convolutional Neuronal Network）</h2></li></ul><h3 id="Why-CNN-for-image"><a href="#Why-CNN-for-image" class="headerlink" title="Why CNN for image"></a>Why CNN for image</h3><ul><li>Some patterns are much smaller than the whole image. That is to say, a neuron only need to have connection with a small region of the image, but not the whole image.</li><li>But the same pattern may appear in different regions in different images.<ul><li>We could let these neurons share their parameters…</li></ul></li><li>Subsampling the pixels will not change the object.</li></ul><h3 id="The-whole-CNN-architecture"><a href="#The-whole-CNN-architecture" class="headerlink" title="The whole CNN architecture"></a>The whole CNN architecture</h3><p>Image -&gt; (Convolution -&gt; Max Pooling)$^{+}$ -&gt; Flatten -(as input)-&gt; Fully Connected Feedforward Network </p><h4 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h4><ul><li>Calculation approaches</li><li>Feature Map</li><li>Colorful Image</li></ul><p><img src="https://i.loli.net/2021/10/02/Y3GhrSRlVWsnofc.png" alt="image-20211002101802253"></p><ul><li><strong>Channel</strong>: 颜色通道</li><li>What does CNN learn? 使用 gradient ascent 寻找 input $x^{*} = arg \max_x{a^k}$</li><li>Deep Dream: let CNN exaggerate what it sees</li><li>以 Alpha Go 为例讲解 Architecture 的可选择性</li></ul><h2 id="RNN-Recurrent-Neural-Network"><a href="#RNN-Recurrent-Neural-Network" class="headerlink" title="RNN (Recurrent Neural Network)"></a>RNN (Recurrent Neural Network)</h2><ul><li>Example Application<ul><li>Slot filling: Nerual Network needs memory!</li><li><img src="https://i.loli.net/2021/10/02/eJHzUvSWnOcTtBD.png" alt="image-20211002120218869"></li><li>Bidirectional RNN<ul><li><img src="https://i.loli.net/2021/10/02/r4fRUpxLKGBbnyc.png" alt="image-20211002120401629"></li><li>Why? Have a broader view of context. 正向只看前面，反向只看后面.</li></ul></li><li>Long Short-term Memory (LSTM)<ul><li>Input signal and output signal are learned by the network itself.</li><li><img src="https://i.loli.net/2021/10/02/FQIxTbOJPH25DvY.png" alt="image-20211002120742395"></li><li><img src="https://i.loli.net/2021/10/02/Z1V2qWHmAg354ku.png" alt="image-20211002121407528"></li></ul></li><li>How to train RNN?<ul><li>Loss function? Sum over cross entropy.</li><li>Learning? Gradient descent. <ul><li>How to calc partial derivative? BPTT (Back propagation through time).</li><li>The error surface may be very flat or very steep. Clipping…</li><li>LSTM may deal with gradient vanishing, but not with gradient explode.</li></ul></li></ul></li><li>Applications</li></ul></li></ul><h2 id="Semi-supervised-Learning-半监督学习"><a href="#Semi-supervised-Learning-半监督学习" class="headerlink" title="Semi-supervised Learning 半监督学习"></a>Semi-supervised Learning 半监督学习</h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li>Semi-supervised learning $\{(x^r, \hat y^r)\}_{r=1}^R, \{x^u\}_{u=R}^{R+U}$<ul><li>A set of unlabeled data, usually $U\gg R$</li></ul></li><li>Classification 半监督学习的分类<ul><li>Transductive learning: unlabelled data is the testing data</li><li>Inductive learning: unlabelled data is not in the testing data</li></ul></li><li>Why semi-supervised learning?<ul><li>Collecting data is easy, but collecting “labelled” data is expensive</li><li>We do semi-supervised learning in our lives</li></ul></li></ul><h3 id="Semi-supervised-learning-for-Generative-Model"><a href="#Semi-supervised-learning-for-Generative-Model" class="headerlink" title="Semi-supervised learning for Generative Model"></a>Semi-supervised learning for Generative Model</h3><p><img src="https://i.loli.net/2021/10/03/WhAsVmULjZCRGMq.png" alt="image-20211003005516326"></p><h3 id="Low-density-separation-assumption"><a href="#Low-density-separation-assumption" class="headerlink" title="Low-density separation assumption"></a><strong>Low-density separation assumption</strong></h3><ul><li>Assumption<ul><li>两个 Class 之间非黑即白 (Black or white)</li></ul></li></ul><p><img src="https://i.loli.net/2021/10/03/nWAefx6phy41tUK.png" alt="image-20211003010157597"></p><p><img src="https://i.loli.net/2021/10/03/YEya7komzgsGPM6.png" alt="image-20211003011038595"></p><h3 id="Smoothness-assumption"><a href="#Smoothness-assumption" class="headerlink" title="Smoothness assumption"></a>Smoothness assumption</h3><ul><li>近朱者赤 近墨者黑<ul><li>“similar” x has the same $\hat y$</li><li>More precisely:<ul><li>x is not uniform.</li><li>if $x^1$ and $x^2$​ are close in a high density region (connected by a high density path)</li><li>then $y^1$ and $y^2$ are the same</li></ul></li></ul></li></ul><h1 id="自监督学习"><a href="#自监督学习" class="headerlink" title="自监督学习"></a>自监督学习</h1><p>以 BERT 和 GPT 为例分析自监督式学习的架构。</p><!--more--><h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><h3 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li>BERT is a kind of <strong>transformer encoder</strong>.</li></ul><h3 id="Basic-Steps"><a href="#Basic-Steps" class="headerlink" title="Basic Steps"></a>Basic Steps</h3><ul><li>Mask<ul><li>Randomly masking some tokens.</li><li>Randomly replacing some tokens.</li></ul></li></ul><p><img src="https://i.loli.net/2021/10/03/5qAycs8TR7zm9LM.png" alt="image-20211003222046934"></p><ul><li>Train goal:</li></ul><p><img src="https://i.loli.net/2021/10/03/Lh5lZuBDadc2gFT.png" alt="image-20211003222105101"></p><ul><li>Next sentence prediction</li></ul><p><img src="https://i.loli.net/2021/10/03/3cfvb2VBtqKrCdo.png" alt="image-20211003222412031"></p><h3 id="Fine-tuning-for-downstream-tasks"><a href="#Fine-tuning-for-downstream-tasks" class="headerlink" title="Fine-tuning for downstream-tasks"></a>Fine-tuning for downstream-tasks</h3><p><img src="https://i.loli.net/2021/10/03/EekjwNaRW1DyCt3.png" alt="image-20211003223325577"></p><h3 id="GLUE-General-Language-Understanding-Evaluation"><a href="#GLUE-General-Language-Understanding-Evaluation" class="headerlink" title="GLUE: General Language Understanding Evaluation"></a>GLUE: General Language Understanding Evaluation</h3><p><img src="https://i.loli.net/2021/10/03/gvQtEZ3ORVxUk5n.png" alt="image-20211003234413065"></p><h3 id="How-to-use-BERT"><a href="#How-to-use-BERT" class="headerlink" title="How to use BERT"></a>How to use BERT</h3><ul><li>Case 1<ul><li>Input a sequence, output a class.<ul><li>Sentimental Analysis</li></ul></li></ul></li></ul><p><img src="https://i.loli.net/2021/10/03/KUyO9mFxqHpvST5.png" alt="image-20211003234839607"></p><ul><li>Case 2<ul><li>Input a sequence and output a sequence of the same length.<ul><li>POS tagging</li></ul></li></ul></li></ul><p><img src="https://i.loli.net/2021/10/03/m2OFX7sVIWC6zd4.png" alt="image-20211003235251180"></p><ul><li><p>Case 3</p><ul><li>Input two sequences and output a class<ul><li>Natural Language Inference (NLI)</li></ul></li></ul></li><li><p>Case 4</p><ul><li>Input a sequence and output a sequence<ul><li>Extraction-based Question Answering</li></ul></li></ul></li></ul><p><img src="https://i.loli.net/2021/10/04/rLO5AnoRiYN1tF2.png" alt="image-20211004215647151"></p><p><img src="https://i.loli.net/2021/10/04/qUB2oTwpGdKNbcj.png" alt="image-20211004220019462"></p><h3 id="Why-does-BERT-work"><a href="#Why-does-BERT-work" class="headerlink" title="Why does BERT work?"></a>Why does BERT work?</h3><ul><li>The tokens with similar meanings have similar embeddings.</li><li>You shall know a word by the company it keeps.</li></ul><h2 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h2><h3 id="Predict-Next-Token"><a href="#Predict-Next-Token" class="headerlink" title="Predict Next Token"></a>Predict Next Token</h3><p><img src="https://i.loli.net/2021/10/04/akuvyBx9AD6XS8c.png" alt="image-20211004225110571"></p><p>架构像是 Transformer 的 Decoder，取消 cross attention.</p><h3 id="How-to-use-GPT"><a href="#How-to-use-GPT" class="headerlink" title="How to use GPT?"></a>How to use GPT?</h3><p><img src="https://i.loli.net/2021/10/04/mQjp7eLRnaWdM21.png" alt="image-20211004230723769"></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><p><a href="https://www.youtube.com/watch?v=gh0hewYkjgo">https://www.youtube.com/watch?v=gh0hewYkjgo</a></p></li><li><p><a href="https://www.youtube.com/watch?v=WY_E0Sd4K80">https://www.youtube.com/watch?v=WY_E0Sd4K80</a></p></li></ul><h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><h2 id="Introduction-2"><a href="#Introduction-2" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li><em>Transformer</em> is a <em>sequence to sequence</em> model. $Seq2seq$</li><li>Application…<ul><li>Speech Recognition</li><li>Machine Translation</li><li>Speech Translation</li><li>Syntactic Parsing（文法剖析）</li><li>Multi-label Classification</li></ul></li><li>Most NLP applications could be considered as <strong>Question Answering</strong></li><li>比起单纯用 seq2seq，Task-specific model 对于某些任务更合适</li><li>Architecture<ul><li><img src="https://i.loli.net/2021/10/03/QJFbND5rkBjZli6.png" alt="image-20211003151722794"></li></ul></li></ul><h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><ul><li><p>Given a set of vectors and output another set of vectors.</p><ul><li>Self-attention, CNN, RNN… All of them could realize this!</li><li><img src="https://i.loli.net/2021/10/03/fIVaFX65TWJcOl1.png" alt="image-20211003151915990"></li></ul></li><li><p><img src="https://i.loli.net/2021/10/03/rSqZC57w2jfveAk.png" alt="image-20211003152146306"></p></li><li>Residual Connection<ul><li><img src="https://i.loli.net/2021/10/03/KnYwBAroMfI5dFV.png" alt="image-20211003152616828"></li></ul></li></ul><h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><h3 id="1-Autoregressive-AT"><a href="#1-Autoregressive-AT" class="headerlink" title="(1) Autoregressive (AT)"></a>(1) Autoregressive (AT)</h3><p><img src="https://i.loli.net/2021/10/03/QTnXwExY2VjkLlR.png" alt="image-20211003153626358"></p><p><img src="https://i.loli.net/2021/10/03/zkqyt6LX2ZEcIJ3.png" alt="image-20211003153834166"></p><ul><li>Architecture</li></ul><p><img src="https://i.loli.net/2021/10/03/RimUkCW6bKHMXVe.png" alt="image-20211003153922255"></p><ul><li>Masked Multi-head Attention?<ul><li>Like RNN…</li><li><img src="https://i.loli.net/2021/10/03/eRHTsgjEmAay3WO.png" alt="image-20211003154121360"></li></ul></li><li>Adding “Stop Token”</li></ul><h3 id="2-Non-autoregressive-NAT"><a href="#2-Non-autoregressive-NAT" class="headerlink" title="(2) Non-autoregressive (NAT)"></a>(2) Non-autoregressive (NAT)</h3><p><img src="https://i.loli.net/2021/10/03/EY3eAv68mshjZVO.png" alt="image-20211003155032111"></p><h2 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h2><p><img src="https://i.loli.net/2021/10/03/Xao8lTPvudAC7j5.png" alt="image-20211003155352197"></p><p><img src="https://i.loli.net/2021/10/03/bTLuX5Ve9ExRirH.png" alt="image-20211003155455569"></p><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><ul><li>Minimize the cross entropy<ul><li>The input of the decoder is the ground truth.</li></ul></li></ul><p><img src="https://i.loli.net/2021/10/03/zQgEb4YLiA6ceV9.png" alt="image-20211003160151665"></p><h2 id="Tips-for-training"><a href="#Tips-for-training" class="headerlink" title="Tips for training"></a>Tips for training</h2><ul><li>Copy Mechanism<ul><li>Chat-bot</li></ul></li><li><p>Guided Attention</p><ul><li>Monotonic attention</li><li>Location-aware attention</li></ul></li><li><p>Beam Search</p></li></ul><p><img src="https://i.loli.net/2021/10/03/v74KjoIAMQuDT1F.png" alt="image-20211003161025399"></p><p><img src="https://i.loli.net/2021/10/03/8oCmEM2dxJeujFs.png" alt="image-20211003161307181"></p><ul><li>Scheduled Sampling</li></ul><h2 id="Reference-1"><a href="#Reference-1" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://www.youtube.com/watch?v=n9TlOhRjYoc">https://www.youtube.com/watch?v=n9TlOhRjYoc</a></li><li><a href="https://www.youtube.com/watch?v=N6aRv06iv2g">https://www.youtube.com/watch?v=N6aRv06iv2g</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;正如篇名，是机器学习笔记的笔记。主要还是记录一些重要概念和思考的过程吧。&lt;/p&gt;
&lt;p&gt;笔记的笔记中，第一个笔记的链接：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://datawhalechina.github.io/leeml-notes/#/&quot;&gt;https://datawhalechina.github.io/leeml-notes/#/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对应视频的链接：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV1Ht411g7Ef&quot;&gt;https://www.bilibili.com/video/BV1Ht411g7Ef&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此外，本文中还收录了部分自监督学习的模型，如 BERT 和 GPT 的运作模式。本文还记录了学习 Transformer 模型的过程。&lt;/p&gt;</summary>
    
    
    
    <category term="理论" scheme="https://www.c7w.tech/categories/%E7%90%86%E8%AE%BA/"/>
    
    <category term="理论/机器学习" scheme="https://www.c7w.tech/categories/%E7%90%86%E8%AE%BA/%E7%90%86%E8%AE%BA-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://www.c7w.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Python asyncio, thread 与 websocket 库初探</title>
    <link href="https://www.c7w.tech/python-asyncio-websocket/"/>
    <id>https://www.c7w.tech/python-asyncio-websocket/</id>
    <published>2021-09-06T01:50:26.000Z</published>
    <updated>2022-01-10T11:29:40.225Z</updated>
    
    <content type="html"><![CDATA[<p>在写 Nanachat-core 的时候打算写事件循环，还要使用到 websocket，于是想参考下相关的设计模式。</p><a id="more"></a><h2 id="异步-I-O"><a href="#异步-I-O" class="headerlink" title="异步 I/O"></a>异步 I/O</h2><p>异步IO模型需要一个消息循环，在消息循环中，主线程不断地重复“读取消息-处理消息”这一过程，我们称其为<strong>消息模型</strong>：</p><pre class="language-python" data-language="python"><code class="language-python">loop <span class="token operator">=</span> get_event_loop<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>    event <span class="token operator">=</span> loop<span class="token punctuation">.</span>get_event<span class="token punctuation">(</span><span class="token punctuation">)</span>    process_event<span class="token punctuation">(</span>event<span class="token punctuation">)</span></code></pre><p>消息模型是如何解决同步IO必须等待IO操作这一问题的呢？</p><ul><li><p>当遇到IO操作时，代码只负责发出IO请求，不等待IO结果，然后直接结束本轮消息处理，进入下一轮消息处理过程。</p></li><li><p>当IO操作完成后，将收到一条“IO完成”的消息，处理该消息时就可以直接获取IO操作结果。</p></li></ul><h3 id="Coroutine-协程"><a href="#Coroutine-协程" class="headerlink" title="Coroutine 协程"></a>Coroutine 协程</h3><p>协程看上去也是子程序，但执行过程中，在子程序内部可中断，然后转而执行别的子程序，在适当的时候再返回来接着执行。</p><p>注意，在一个子程序中中断，去执行其他子程序，不是函数调用，有点类似CPU的中断。比如子程序A、B：</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">A</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'1'</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'2'</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'3'</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">B</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'x'</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'y'</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'z'</span><span class="token punctuation">)</span></code></pre><p>And we may get…</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token number">1</span><span class="token number">2</span>xy<span class="token number">3</span>z</code></pre><p>协程的执行效率比线程切换高，减少了线程切换的开销；同时，我们也不需要考虑锁机制的问题。</p><h4 id="Python-generator"><a href="#Python-generator" class="headerlink" title="Python generator"></a>Python generator</h4><p>Python对协程的支持是通过generator实现的。</p><p>在generator中，我们不但可以通过<code>for</code>循环来迭代，还可以不断调用<code>next()</code>函数获取由<code>yield</code>语句返回的下一个值。</p><p>但是Python的<code>yield</code>不但可以返回一个值，它还可以接收调用者发出的参数。</p><p>我们写一个简单的程序来输出 yield 关键字的执行顺序.</p><blockquote><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">consumer</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"[CONSUMER] Entering function..."</span><span class="token punctuation">)</span>    r <span class="token operator">=</span> <span class="token string">''</span>    <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"[CONSUMER] Entering while loop..."</span><span class="token punctuation">)</span>        n <span class="token operator">=</span> <span class="token keyword">yield</span> r        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"[CONSUMER] Fetched n..."</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token keyword">not</span> n<span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"[CONSUMER] Before returning..."</span><span class="token punctuation">)</span>            <span class="token keyword">return</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"[CONSUMER] Consuming </span><span class="token interpolation"><span class="token punctuation">&#123;</span>n<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>        r <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f'Consumed </span><span class="token interpolation"><span class="token punctuation">&#123;</span>n<span class="token punctuation">&#125;</span></span><span class="token string">!'</span></span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"[MAIN] Entering main..."</span><span class="token punctuation">)</span>c <span class="token operator">=</span> consumer<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># Get the generator</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"[MAIN] Generating generator object..."</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"[MAIN] Starting first send..."</span><span class="token punctuation">)</span>r <span class="token operator">=</span> c<span class="token punctuation">.</span>send<span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"[MAIN] Ending first send..."</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"[MAIN] First send end with result '</span><span class="token interpolation"><span class="token punctuation">&#123;</span>r<span class="token punctuation">&#125;</span></span><span class="token string">'..."</span></span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"[MAIN] Starting second send..."</span><span class="token punctuation">)</span>r <span class="token operator">=</span> c<span class="token punctuation">.</span>send<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"[MAIN] Ending second send..."</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"[MAIN] Second send end with result '</span><span class="token interpolation"><span class="token punctuation">&#123;</span>r<span class="token punctuation">&#125;</span></span><span class="token string">'..."</span></span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"[MAIN] Starting last send..."</span><span class="token punctuation">)</span>r <span class="token operator">=</span> c<span class="token punctuation">.</span>send<span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"[MAIN] Ending last send..."</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"[MAIN] Last send end with result '</span><span class="token interpolation"><span class="token punctuation">&#123;</span>r<span class="token punctuation">&#125;</span></span><span class="token string">'..."</span></span><span class="token punctuation">)</span></code></pre></blockquote><p>输出结果为：</p><blockquote><pre class="language-none"><code class="language-none">[MAIN] Entering main...[MAIN] Generating generator object...       [MAIN] Starting first send...[CONSUMER] Entering function...[CONSUMER] Entering while loop...[MAIN] Ending first send...[MAIN] First send end with result &#39;&#39;...     [MAIN] Starting second send...[CONSUMER] Fetched n...[CONSUMER] Consuming 2[CONSUMER] Entering while loop...[MAIN] Ending second send...[MAIN] Second send end with result &#39;Consumed 2!&#39;...[MAIN] Starting last send...[CONSUMER] Fetched n...[CONSUMER] Before returning...Traceback (most recent call last):  File &quot;coroutine.py&quot;, line 26, in &lt;module&gt;     r &#x3D; c.send(None)StopIteration</code></pre></blockquote><p>可以看到，在第一次执行 <code>generator.send()</code> 的时候，generator 会从头开始执行.</p><p>之后每次便从 yield 处开始执行，先完成赋值操作，再继续执行到下一次 yield 或者 return.</p><p><code>yield r</code> 会把 r 作为 <code>send()</code> 函数的返回值返回给 MAIN 函数.</p><h3 id="asyncio"><a href="#asyncio" class="headerlink" title="asyncio"></a>asyncio</h3><p>An <strong>event loop</strong> runs in a thread (typically the main thread) and executes all callbacks and Tasks in its thread. </p><p>While a <strong>Task</strong> is running in the <strong>event loop</strong>, no other Tasks can run in the same thread. </p><p>When a <strong>Task</strong> executes an <code>await</code> expression, the running <strong>Task</strong> gets suspended, and the event loop executes the next Task.</p><p><code>asyncio</code>的编程模型就是一个消息循环。</p><p>我们从<code>asyncio</code>模块中直接获取一个<code>EventLoop</code>的引用，然后把需要执行的协程扔到<code>EventLoop</code>中执行，就实现了异步IO。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> datetime<span class="token keyword">import</span> asyncio<span class="token decorator annotation punctuation">@asyncio<span class="token punctuation">.</span>coroutine</span><span class="token keyword">def</span> <span class="token function">hello</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Hello world! </span><span class="token interpolation"><span class="token punctuation">&#123;</span>datetime<span class="token punctuation">.</span>datetime<span class="token punctuation">.</span>now<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>    <span class="token comment"># 异步调用asyncio.sleep(1):</span>    r <span class="token operator">=</span> <span class="token keyword">yield</span> <span class="token keyword">from</span> asyncio<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Hello again! </span><span class="token interpolation"><span class="token punctuation">&#123;</span>datetime<span class="token punctuation">.</span>datetime<span class="token punctuation">.</span>now<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token comment"># 获取EventLoop:</span>loop <span class="token operator">=</span> asyncio<span class="token punctuation">.</span>get_event_loop<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 执行coroutine</span><span class="token keyword">print</span><span class="token punctuation">(</span>hello<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>loop<span class="token punctuation">.</span>run_until_complete<span class="token punctuation">(</span>hello<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>loop<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><pre class="language-none"><code class="language-none">coroutine.py:6: DeprecationWarning: &quot;@coroutine&quot; decorator is deprecated since Python 3.8, use &quot;async def&quot; instead  def hello():&lt;generator object hello at 0x7f05bea44cf0&gt;Hello world! 2021-09-06 11:20:07.045310   Hello again! 2021-09-06 11:20:08.047006</code></pre><p><code>yield from</code>语法可以让我们方便地调用另一个<code>generator</code>.</p><h3 id="async-await"><a href="#async-await" class="headerlink" title="async/await"></a>async/await</h3><p>为了简化并更好地标识异步IO，从Python 3.5开始引入了新的语法<code>async</code>和<code>await</code>，可以让coroutine的代码更简洁易读。</p><p>请注意，<code>async</code>和<code>await</code>是针对coroutine的新语法，要使用新的语法，只需要做两步简单的替换：</p><ol><li>把<code>@asyncio.coroutine</code>替换为<code>async</code>；</li><li>把<code>yield from</code>替换为<code>await</code>。</li></ol><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> datetime<span class="token keyword">import</span> asyncio<span class="token keyword">async</span> <span class="token keyword">def</span> <span class="token function">hello</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Hello world! </span><span class="token interpolation"><span class="token punctuation">&#123;</span>datetime<span class="token punctuation">.</span>datetime<span class="token punctuation">.</span>now<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>    <span class="token comment"># 异步调用asyncio.sleep(1):</span>    r <span class="token operator">=</span> <span class="token keyword">await</span> asyncio<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Hello again! </span><span class="token interpolation"><span class="token punctuation">&#123;</span>datetime<span class="token punctuation">.</span>datetime<span class="token punctuation">.</span>now<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token comment"># 获取EventLoop:</span>loop <span class="token operator">=</span> asyncio<span class="token punctuation">.</span>get_event_loop<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 执行coroutine</span>loop<span class="token punctuation">.</span>run_until_complete<span class="token punctuation">(</span>hello<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>loop<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><pre class="language-none"><code class="language-none">Hello world! 2021-09-06 11:23:14.967038Hello again! 2021-09-06 11:23:15.968643</code></pre><h2 id="Websocket"><a href="#Websocket" class="headerlink" title="Websocket"></a>Websocket</h2><h3 id="Server"><a href="#Server" class="headerlink" title="Server"></a>Server</h3><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># WS server example</span><span class="token keyword">import</span> asyncio<span class="token keyword">import</span> websockets<span class="token keyword">async</span> <span class="token keyword">def</span> <span class="token function">hello</span><span class="token punctuation">(</span>websocket<span class="token punctuation">,</span> path<span class="token punctuation">)</span><span class="token punctuation">:</span>    name <span class="token operator">=</span> <span class="token keyword">await</span> websocket<span class="token punctuation">.</span>recv<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"&lt; </span><span class="token interpolation"><span class="token punctuation">&#123;</span>name<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>    greeting <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"Hello </span><span class="token interpolation"><span class="token punctuation">&#123;</span>name<span class="token punctuation">&#125;</span></span><span class="token string">!"</span></span>    <span class="token keyword">await</span> websocket<span class="token punctuation">.</span>send<span class="token punctuation">(</span>greeting<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"> </span><span class="token interpolation"><span class="token punctuation">&#123;</span>greeting<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>start_server <span class="token operator">=</span> websockets<span class="token punctuation">.</span>serve<span class="token punctuation">(</span>hello<span class="token punctuation">,</span> <span class="token string">"localhost"</span><span class="token punctuation">,</span> <span class="token number">8765</span><span class="token punctuation">)</span>asyncio<span class="token punctuation">.</span>get_event_loop<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>run_until_complete<span class="token punctuation">(</span>start_server<span class="token punctuation">)</span>asyncio<span class="token punctuation">.</span>get_event_loop<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>run_forever<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h3 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h3><pre class="language-Client" data-language="Client"><code class="language-Client"># WS client exampleimport asyncioimport websocketsasync def hello():    uri &#x3D; &quot;ws:&#x2F;&#x2F;localhost:8765&quot;    async with websockets.connect(uri) as websocket:        name &#x3D; input(&quot;What&#39;s your name? &quot;)        await websocket.send(name)        print(f&quot;&gt; &#123;name&#125;&quot;)        greeting &#x3D; await websocket.recv()        print(f&quot;&lt; &#123;greeting&#125;&quot;)asyncio.get_event_loop().run_until_complete(hello())</code></pre><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>我们要实现具有以下功能的 websocket 结构.</p><ul><li>Server<ul><li>每 3s 向 Client 发一条消息.</li><li>收到消息立刻 Prompt.</li></ul></li><li>Client<ul><li>每 1s 向 Server 发一条消息.</li><li>收到消息立刻 Prompt.</li></ul></li></ul><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># Server.py</span><span class="token comment">#!/usr/bin/env python</span><span class="token comment"># WS server example</span><span class="token keyword">import</span> datetime<span class="token keyword">import</span> asyncio<span class="token keyword">import</span> websockets<span class="token keyword">async</span> <span class="token keyword">def</span> <span class="token function">recv</span><span class="token punctuation">(</span>websocket<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>        name <span class="token operator">=</span> <span class="token keyword">await</span> websocket<span class="token punctuation">.</span>recv<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"&lt; </span><span class="token interpolation"><span class="token punctuation">&#123;</span>name<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token keyword">async</span> <span class="token keyword">def</span> <span class="token function">send</span><span class="token punctuation">(</span>websocket<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>        <span class="token comment"># Send</span>        msg <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"!Server Send! </span><span class="token interpolation"><span class="token punctuation">&#123;</span>datetime<span class="token punctuation">.</span>datetime<span class="token punctuation">.</span>now<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span>        <span class="token keyword">await</span> websocket<span class="token punctuation">.</span>send<span class="token punctuation">(</span>msg<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"> </span><span class="token interpolation"><span class="token punctuation">&#123;</span>msg<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>        <span class="token keyword">await</span> asyncio<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">async</span> <span class="token keyword">def</span> <span class="token function">socket</span><span class="token punctuation">(</span>websocket<span class="token punctuation">,</span> path<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">await</span> asyncio<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>recv<span class="token punctuation">(</span>websocket<span class="token punctuation">)</span><span class="token punctuation">,</span> send<span class="token punctuation">(</span>websocket<span class="token punctuation">)</span><span class="token punctuation">)</span>        start_server <span class="token operator">=</span> websockets<span class="token punctuation">.</span>serve<span class="token punctuation">(</span>socket<span class="token punctuation">,</span> <span class="token string">"localhost"</span><span class="token punctuation">,</span> <span class="token number">8765</span><span class="token punctuation">)</span>asyncio<span class="token punctuation">.</span>get_event_loop<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>run_until_complete<span class="token punctuation">(</span>start_server<span class="token punctuation">)</span>asyncio<span class="token punctuation">.</span>get_event_loop<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>run_forever<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># Client.py</span><span class="token keyword">import</span> asyncio<span class="token keyword">import</span> datetime<span class="token keyword">import</span> websockets<span class="token keyword">async</span> <span class="token keyword">def</span> <span class="token function">recv</span><span class="token punctuation">(</span>websocket<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>        name <span class="token operator">=</span> <span class="token keyword">await</span> websocket<span class="token punctuation">.</span>recv<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"&lt; </span><span class="token interpolation"><span class="token punctuation">&#123;</span>name<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span><span class="token keyword">async</span> <span class="token keyword">def</span> <span class="token function">send</span><span class="token punctuation">(</span>websocket<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>        <span class="token comment"># Send</span>        msg <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"!Client Send! </span><span class="token interpolation"><span class="token punctuation">&#123;</span>datetime<span class="token punctuation">.</span>datetime<span class="token punctuation">.</span>now<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span>        <span class="token keyword">await</span> websocket<span class="token punctuation">.</span>send<span class="token punctuation">(</span>msg<span class="token punctuation">)</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"> </span><span class="token interpolation"><span class="token punctuation">&#123;</span>msg<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>        <span class="token keyword">await</span> asyncio<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">async</span> <span class="token keyword">def</span> <span class="token function">socket</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    uri <span class="token operator">=</span> <span class="token string">"ws://localhost:8765"</span>    <span class="token keyword">async</span> <span class="token keyword">with</span> websockets<span class="token punctuation">.</span>connect<span class="token punctuation">(</span>uri<span class="token punctuation">)</span> <span class="token keyword">as</span> websocket<span class="token punctuation">:</span>        <span class="token keyword">await</span> asyncio<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>recv<span class="token punctuation">(</span>websocket<span class="token punctuation">)</span><span class="token punctuation">,</span> send<span class="token punctuation">(</span>websocket<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">async</span> <span class="token keyword">def</span> <span class="token function">sayhello</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"INTER BREAK"</span><span class="token punctuation">)</span>        <span class="token keyword">await</span> asyncio<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span>asyncio<span class="token punctuation">.</span>get_event_loop<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>run_until_complete<span class="token punctuation">(</span>asyncio<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>socket<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> sayhello<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://docs.python.org/3/library/asyncio-dev.html">https://docs.python.org/3/library/asyncio-dev.html</a></li><li><p><a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017970488768640">https://www.liaoxuefeng.com/wiki/1016959663602400/1017970488768640</a></p></li><li><p><a href="https://realpython.com/async-io-python/">https://realpython.com/async-io-python/</a></p></li><li><a href="https://websockets.readthedocs.io/en/stable/intro.html">https://websockets.readthedocs.io/en/stable/intro.html</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;在写 Nanachat-core 的时候打算写事件循环，还要使用到 websocket，于是想参考下相关的设计模式。&lt;/p&gt;</summary>
    
    
    
    <category term="技术" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="技术/Python应用" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/%E6%8A%80%E6%9C%AF-Python%E5%BA%94%E7%94%A8/"/>
    
    
    <category term="多线程" scheme="https://www.c7w.tech/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"/>
    
    <category term="Websocket" scheme="https://www.c7w.tech/tags/Websocket/"/>
    
  </entry>
  
  <entry>
    <title>实现 MNIST 鉴别手写数字过程手记</title>
    <link href="https://www.c7w.tech/implementing-MNIST-classifier/"/>
    <id>https://www.c7w.tech/implementing-MNIST-classifier/</id>
    <published>2021-09-03T15:52:50.000Z</published>
    <updated>2022-01-10T11:36:28.392Z</updated>
    
    <content type="html"><![CDATA[<p>在学习了相关的<a href="https://cc7w.cf/but-what-is-a-neural-network/">理论基础</a>之后，我们自然是要投入到实践之中，完成这项机器学习领域的 Hello World 任务.</p><p>本篇文章会用来记录 c7w 操作的全过程，以及他试图使用 python3 来拟合一篇 python2 教程的痛苦.</p><a id="more"></a><h1 id="MNIST-Classifier-Implemention"><a href="#MNIST-Classifier-Implemention" class="headerlink" title="MNIST Classifier Implemention"></a>MNIST Classifier Implemention</h1><h2 id="Fetch-dataset"><a href="#Fetch-dataset" class="headerlink" title="Fetch dataset"></a>Fetch dataset</h2><h3 id="Download"><a href="#Download" class="headerlink" title="Download"></a>Download</h3><p>下载数据包，前往 <a href="http://yann.lecun.com/exdb/mnist/">MNIST 官方网站</a>，把 Trainset 和 Testset 的两组数据全部下载。<s>然后去摸鱼</s></p><p><img src="https://i.loli.net/2021/09/03/yRuLJvqO2eEI9MX.png" alt="image-20210903235728102"></p><p><strong>数据格式</strong></p><blockquote><h3 id="TRAINING-SET-LABEL-FILE-train-labels-idx1-ubyte"><a href="#TRAINING-SET-LABEL-FILE-train-labels-idx1-ubyte" class="headerlink" title="TRAINING SET LABEL FILE (train-labels-idx1-ubyte):"></a>TRAINING SET LABEL FILE (train-labels-idx1-ubyte):</h3><pre class="language-none"><code class="language-none">&gt;[offset] [type]     [value]     [description]&#96;&gt;&#96;0000   32 bit integer 0x00000801(2049) magic number (MSB first)&#96;&gt;&#96;0004   32 bit integer 60000      number of items&#96;&gt;&#96;0008   unsigned byte  ??        label&#96;&gt;&#96;0009   unsigned byte  ??        label&#96;&gt;&#96;........&#96;&gt;&#96;xxxx   unsigned byte  ??        label&gt;The labels values are 0 to 9.</code></pre><h3 id="TRAINING-SET-IMAGE-FILE-train-images-idx3-ubyte"><a href="#TRAINING-SET-IMAGE-FILE-train-images-idx3-ubyte" class="headerlink" title="TRAINING SET IMAGE FILE (train-images-idx3-ubyte):"></a>TRAINING SET IMAGE FILE (train-images-idx3-ubyte):</h3><pre class="language-none"><code class="language-none">&gt;[offset] [type]     [value]     [description]&#96;&gt;&#96;0000   32 bit integer 0x00000803(2051) magic number&#96;&gt;&#96;0004   32 bit integer 60000      number of images&#96;&gt;&#96;0008   32 bit integer 28        number of rows&#96;&gt;&#96;0012   32 bit integer 28        number of columns&#96;&gt;&#96;0016   unsigned byte  ??        pixel&#96;&gt;&#96;0017   unsigned byte  ??        pixel&#96;&gt;&#96;........&#96;&gt;&#96;xxxx   unsigned byte  ??        pixel</code></pre></blockquote><h3 id="Use-Pickle-to-load-dataset"><a href="#Use-Pickle-to-load-dataset" class="headerlink" title="Use Pickle to load dataset"></a>Use <strong>Pickle</strong> to load dataset</h3><h2 id="Define-network"><a href="#Define-network" class="headerlink" title="Define network"></a>Define network</h2><h3 id="Init-function"><a href="#Init-function" class="headerlink" title="Init function"></a>Init function</h3><p><strong><code>np.random.randn(d0, d1, ...)</code> 生成随机初值</strong></p><p>生成均值为 0，标准差为 1 的正态分布</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token operator">>></span><span class="token operator">></span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token number">0.5868205495246934</span><span class="token operator">>></span><span class="token operator">></span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token number">1.0092581568418932</span><span class="token operator">>></span><span class="token operator">></span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token number">0.056108818546049016</span><span class="token operator">>></span><span class="token operator">></span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.19376191</span><span class="token punctuation">,</span>  <span class="token number">0.28157974</span><span class="token punctuation">,</span>  <span class="token number">0.77366971</span><span class="token punctuation">,</span>  <span class="token number">0.06783304</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.8541143</span> <span class="token punctuation">,</span>  <span class="token number">0.75650134</span><span class="token punctuation">,</span>  <span class="token number">0.59333362</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.65325825</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p><strong><code>zip(a, b)</code> 生成矩阵大小元组</strong></p><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span> b <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span> <span class="token builtin">zip</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token operator">&lt;</span><span class="token builtin">zip</span> <span class="token builtin">object</span> at <span class="token number">0x7f48aac07140</span><span class="token operator">></span><span class="token operator">>></span><span class="token operator">></span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">for</span> x<span class="token punctuation">,</span> y <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>     <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token number">1</span> <span class="token number">5</span><span class="token number">2</span> <span class="token number">6</span><span class="token number">3</span> <span class="token number">7</span><span class="token number">4</span> <span class="token number">8</span><span class="token comment"># If their size does not match ?</span><span class="token operator">>></span><span class="token operator">></span> a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span> b <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span> <span class="token builtin">zip</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span><span class="token operator">&lt;</span><span class="token builtin">zip</span> <span class="token builtin">object</span> at <span class="token number">0x7f48a93bfb00</span><span class="token operator">></span><span class="token operator">>></span><span class="token operator">></span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span></code></pre><p><strong>Network 类的构造</strong></p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">class</span> <span class="token class-name">Network</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sizes<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>num_layers <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>sizes<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>sizes <span class="token operator">=</span> sizes        self<span class="token punctuation">.</span>biases <span class="token operator">=</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>y<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">for</span> y <span class="token keyword">in</span> sizes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>weights <span class="token operator">=</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>y<span class="token punctuation">,</span> x<span class="token punctuation">)</span>                        <span class="token keyword">for</span> x<span class="token punctuation">,</span> y <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>sizes<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sizes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span></code></pre><p>对构造函数进行测试…</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># Test</span>network <span class="token operator">=</span> Network<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>network<span class="token punctuation">.</span>biases<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>network<span class="token punctuation">.</span>weights<span class="token punctuation">)</span></code></pre><pre class="language-none"><code class="language-none">[array([[-0.69137659],       [ 0.38159818],       [ 1.99258114],       [-0.21144943]]), array([[1.71569818]])]# Biases: [&lt;Array 4x1&gt;, &lt;Array 1x1&gt;][array([[-1.23719403,  1.53304051,  1.33083345,  0.57793759,  1.64201695,        -1.09019958, -1.72304327, -1.37257551],       [ 1.55140653, -0.66066249,  0.24355521, -0.06090572,  0.7280547 ,            -0.09691786, -1.06502271, -0.39097928],       [-0.37874956,  0.71162084,  0.23456423, -0.34639554, -1.13344851,             0.89943267,  0.44945541, -0.59622082],       [ 1.90930694,  0.4098545 ,  1.7352552 ,  1.04517198, -0.2241232 ,             0.14171245,  0.07186716,  2.50271226]]), array([[ 0.58568595, -1.9973499 ,  0.36131549, -0.07790474]])]# Weights: [&lt;Array 4x8&gt;, &lt;Array 1x4&gt;]</code></pre><p>为什么要这么构造？我们回忆…</p><ul><li>Our notations are: $a^{(2)} = \sigma(W^{(2)}a^{(1)}+b^{(2)})$</li><li>Namely $\begin {bmatrix} a_1^{(2)}\\ a_2^{(2)}\\ \vdots\\ a_k^{(2)}\\ \end {bmatrix}= \sigma( \begin {bmatrix} w^{(2)}_{1,1} &amp; w^{(2)}_{1,2} &amp; \cdots &amp; w^{(2)}_{1,n} \\ w^{(2)}_{2,1} &amp; w^{(2)}_{2,2} &amp; \cdots &amp; w^{(2)}_{2,n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ w^{(2)}_{k,1} &amp; w^{(2)}_{k,2} &amp; \cdots &amp; w^{(2)}_{k,n}  \end {bmatrix}\begin {bmatrix} a_1^{(1)}\\ a_2^{(1)}\\ \vdots\\ a_n^{(1)}\\ \end {bmatrix} + \begin {bmatrix} b_1^{(2)}\\ b_2^{(2)}\\ \vdots\\ b_k^{(2)}\\ \end {bmatrix})$</li><li>Suppose Layer $m-1$​ has $n$​​ neurons and layer $m$ has $k$ neurons.<ul><li>Layer $m$ has $k$​ biases because each neuron in layer $m$ has its own bias.</li><li>Each neuron in layer $m$​ has $n$ related weights in layer $m-1$​, namely neuron $i$ in layer $m$ has its weights $w_{i, 1}, w_{i, 2}, \cdots, w_{1, n}$</li></ul></li></ul><pre class="mermaid">graph LRsubgraph "Layer m-1"A0("1")A1("2")A2("3")A3("4")endsubgraph "Layer m"B2("m(i-1)")B0("m(i)")B1("m(i+1)")endA0 --> B0A1 --> B0A2 --> B0A3 --> B0</pre><p>也就是说，layer $m-1$ 与 layer $m$​​ 之间的权矩阵大小应该是 <code>len(m) * len(m-1)</code>, namely 构造函数中的 <code>y * x</code>.</p><h3 id="Sigmoid-function-and-its-deriative"><a href="#Sigmoid-function-and-its-deriative" class="headerlink" title="Sigmoid function and its deriative"></a>Sigmoid function and its deriative</h3><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> <span class="token number">1.0</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">sigmoid_deriative</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> sigmoid<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>sigmoid<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><script type="math/tex; mode=display">\begin{align*}\sigma'(x) & =  (\frac 1 {1+e^{-x}})' \\& = \frac {e^{-x}} {({1+e^{-x}})^2} \\& = (\frac 1 {1+e^{-x}})*(\frac {e^{-x}} {1+e^{-x}}) \\ & = (\frac 1 {1+e^{-x}})*(1 -\frac {1} {1+e^{-x}}) \\& = \sigma(x)*(1-\sigma(x))\end{align*}</script><h3 id="Stochastic-gradient-descent"><a href="#Stochastic-gradient-descent" class="headerlink" title="Stochastic gradient descent"></a>Stochastic gradient descent</h3><h4 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h4><p>我们回忆，在理论学习中，我们学到的 SGD 内容。</p><p><strong>Batch_size? Epoch? Iteration?</strong></p><p>比如你有1000个数据，这个数据集可能太大了，全部跑一次再调参很慢，于是可以分成100个为一个数据集，这样有10份。<strong>batch_size=100</strong></p><p>这100个数据组成的数据集叫 <strong>batch</strong>，每跑完一个 <strong>batch</strong> 都要更新参数，这个过程叫一个<strong>iteration</strong>.</p><p><strong>epoch</strong> 指的就是跑完这 10 个 <strong>batch</strong> （ 10 个 <strong>iteration</strong> ）的这个过程.</p><h4 id="Implemention"><a href="#Implemention" class="headerlink" title="Implemention"></a>Implemention</h4><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># Stochastic gradient descent</span><span class="token keyword">def</span> <span class="token function">SGD</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> training_data<span class="token punctuation">,</span> epochs<span class="token punctuation">,</span> mini_batch_size<span class="token punctuation">,</span> eta<span class="token punctuation">,</span> test_data<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> test_data<span class="token punctuation">:</span>        n_test <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>test_data<span class="token punctuation">)</span>    n <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>training_data<span class="token punctuation">)</span>    <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>        random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>training_data<span class="token punctuation">)</span>        <span class="token comment"># Divide training data into lists</span>        mini_batches <span class="token operator">=</span> <span class="token punctuation">[</span>training_data<span class="token punctuation">[</span>mini_batch_size<span class="token operator">*</span><span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">:</span> mini_batch_size<span class="token operator">*</span><span class="token punctuation">(</span>            k<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token keyword">for</span> k <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>n<span class="token operator">+</span>mini_batch_size<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">//</span>mini_batch_size<span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> mini_batch <span class="token keyword">in</span> mini_batches<span class="token punctuation">:</span>        self<span class="token punctuation">.</span>update_mini_batch<span class="token punctuation">(</span>mini_batch<span class="token punctuation">,</span> eta<span class="token punctuation">)</span>    <span class="token keyword">if</span> test_data<span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>            <span class="token string-interpolation"><span class="token string">f'''Epoch </span><span class="token interpolation"><span class="token punctuation">&#123;</span>j<span class="token punctuation">&#125;</span></span><span class="token string"> finished, with an accuracy of </span><span class="token interpolation"><span class="token punctuation">&#123;</span>self<span class="token punctuation">.</span>evaluate<span class="token punctuation">(</span>test_data<span class="token punctuation">)</span><span class="token punctuation">&#125;</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">&#123;</span>n_test<span class="token punctuation">&#125;</span></span><span class="token string"> (</span><span class="token interpolation"><span class="token punctuation">&#123;</span>self<span class="token punctuation">.</span>evaluate<span class="token punctuation">(</span>test_data<span class="token punctuation">)</span><span class="token operator">/</span>n_test<span class="token punctuation">&#125;</span></span><span class="token string">)'''</span></span><span class="token punctuation">)</span>    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Epoch </span><span class="token interpolation"><span class="token punctuation">&#123;</span>j<span class="token punctuation">&#125;</span></span><span class="token string"> finished."</span></span><span class="token punctuation">)</span></code></pre><h4 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h4><ul><li>training_data: list of tuples, where each tuple is (DATA, LABEL)</li><li>epochs: determines the training data would be used how many times</li><li>mini_batch_size: determines the size of every small batch</li><li>eta: learning rate, in $ v \rightarrow (v’=v-\eta\nabla C)$</li><li>test_data: for verifying accuracy after each epoch</li></ul><h4 id="Update-mini-batches"><a href="#Update-mini-batches" class="headerlink" title="Update mini-batches"></a>Update mini-batches</h4><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">update_mini_batch</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> mini_batch<span class="token punctuation">,</span> eta<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># Init nabla matrices</span>    nabla_b <span class="token operator">=</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>b<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token keyword">for</span> b <span class="token keyword">in</span> self<span class="token punctuation">.</span>biases<span class="token punctuation">]</span>    nabla_w <span class="token operator">=</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>w<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token keyword">for</span> w <span class="token keyword">in</span> self<span class="token punctuation">.</span>weights<span class="token punctuation">]</span>    <span class="token comment"># Accumulate to calc nablas</span>    <span class="token keyword">for</span> x<span class="token punctuation">,</span> y <span class="token keyword">in</span> mini_batch<span class="token punctuation">:</span>        delta_nabla_b<span class="token punctuation">,</span> delta_nabla_w <span class="token operator">=</span> self<span class="token punctuation">.</span>backprop<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>        nabla_b <span class="token operator">=</span> <span class="token punctuation">[</span>nb<span class="token operator">+</span>dnb <span class="token keyword">for</span> nb<span class="token punctuation">,</span> dnb <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>nabla_b<span class="token punctuation">,</span> delta_nabla_b<span class="token punctuation">)</span><span class="token punctuation">]</span>        nabla_w <span class="token operator">=</span> <span class="token punctuation">[</span>nw<span class="token operator">+</span>dnw <span class="token keyword">for</span> nw<span class="token punctuation">,</span> dnw <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>nabla_w<span class="token punctuation">,</span> delta_nabla_w<span class="token punctuation">)</span><span class="token punctuation">]</span>        <span class="token comment"># Update weights and biases</span>    self<span class="token punctuation">.</span>biases <span class="token operator">=</span> <span class="token punctuation">[</span>b<span class="token operator">-</span>nb<span class="token operator">/</span><span class="token builtin">len</span><span class="token punctuation">(</span>mini_batch<span class="token punctuation">)</span><span class="token operator">*</span>eta <span class="token keyword">for</span> b<span class="token punctuation">,</span>                   nb <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>biases<span class="token punctuation">,</span> nabla_b<span class="token punctuation">)</span><span class="token punctuation">]</span>    self<span class="token punctuation">.</span>weights <span class="token operator">=</span> <span class="token punctuation">[</span>w<span class="token operator">-</span>nw<span class="token operator">/</span><span class="token builtin">len</span><span class="token punctuation">(</span>mini_batch<span class="token punctuation">)</span><span class="token operator">*</span>eta <span class="token keyword">for</span> w<span class="token punctuation">,</span>                    nw <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>weights<span class="token punctuation">,</span> nabla_w<span class="token punctuation">)</span><span class="token punctuation">]</span></code></pre><h3 id="Back-propagation"><a href="#Back-propagation" class="headerlink" title="Back propagation"></a>Back propagation</h3><pre class="mermaid">graph LRsubgraph "Layer m-1"A0("1")A1("2")A2("3")A3("4")endsubgraph "Layer m"B2("output(i-1)")B0("output(i)")B1("output(i+1)")endsubgraph "Anticipated<br/>Label"C2("y(i-1)")C0("y(i)")C1("y(i+1)")endA0 --> B0A1 --> B0A2 --> B0A3 --> B0B0 --> C0B1 --> C1B2 --> C2</pre><p>考虑计算过程…</p><pre class="mermaid">graph RLCX("$C$")aE("$a^L$")zE("$z^L$")wE("$w^L$")aEL("$a^{L-1}$")bE("$b^L$")zEL("$z^{L-1}$")wEL("$w^{L-1}$")aELL("$a^{L-2}$")bEL("$b^{L-1}$")aE -->|"$(y_x-a^L_x)^2$"| CXzE -->|"$\sigma$"| aEwE --> zEaEL --> zEbE --> zEzEL -->|"$\sigma$"| aELwEL --> zELaELL --> zELbEL --> zELdot("$\cdots$") --> aELL</pre><ul><li>Then the core equations of back propagation…<ul><li>$\frac {\partial C} {\partial z_i^L} = \frac {\partial C} {\partial a_i^L}\frac {\partial a_i^L} {\partial z_i^L} = (a_i^L-y)\sigma’(z_i^L)$​​ (Initialize)​</li><li>According to $z^{(L)}_i = \sum_jw^{(L)}_{i, j}a_j^{(L-1)}+ b_i^{(L)}$​​​​, suppose we have calculated $\frac {\partial C} {\partial z^M_i}$​​​ for all neuron $i$​​​ in layer $M$​​​.<ul><li>How can we get $\frac {\partial C} {\partial w^M_{i, j}}$​​​ for all neuron $i$​​​ in layer $M$​​ and $j$​​ in layer $M-1$​​​​?<ul><li>$\frac {\partial C} {\partial w^M_{i, j}} = \frac {\partial C} {\partial z^M_i} \frac {\partial z^M_i}{\partial w^M_{i, j}} = \frac {\partial C} {\partial z^M_i} a_j^{M-1}$​​</li></ul></li><li>How can we get $\frac {\partial C} {\partial b^M_i} $​​ for all neuron $i$​​ in layer $M$​?<ul><li>$\frac {\partial C} {\partial b^M_{i}} = \frac {\partial C} {\partial z^M_i} \frac {\partial z^M_i}{\partial b^M_{i}} = \frac {\partial C} {\partial z^M_i}$​​​</li></ul></li><li>How can we get $\frac {\partial C} {\partial a^{M-1}_j} $ and $\frac {\partial C} {\partial z^{M-1}_j} $ for all neuron $j$ in layer $M-1$?<ul><li>$\frac {\partial C} {\partial a^{M-1}_j}  = \sum_i \frac {\partial C}{\partial z^M_i} \frac {\partial z^M_i} {a_j^{M-1}} = \sum_i \frac {\partial C}{\partial z^M_i} w_{i,j}^M$</li><li>Then $\frac {\partial C} {\partial z^{M-1}_j} = \frac {\partial C} {\partial a^{M-1}_j} \sigma’(z^{M-1}_j)$</li></ul></li></ul></li><li>Then by recursion, we could calculate all the partial derivatives of weights and biases.</li></ul></li></ul><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">backprop</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">'''Return (nabla_b, nabla_w) representing the    gradient for the cost function C.    Each of the return value are layer-by-layer lists of numpy arrays.'''</span>        nabla_b <span class="token operator">=</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>b<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token keyword">for</span> b <span class="token keyword">in</span> self<span class="token punctuation">.</span>biases<span class="token punctuation">]</span>    nabla_w <span class="token operator">=</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>w<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token keyword">for</span> w <span class="token keyword">in</span> self<span class="token punctuation">.</span>weights<span class="token punctuation">]</span>        <span class="token comment"># Feed forward</span>    activation <span class="token operator">=</span> x    activations <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">]</span>    zs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token comment"># Store all Z vectors. z = w*a + b</span>        <span class="token keyword">for</span> b<span class="token punctuation">,</span> w <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>biases<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weights<span class="token punctuation">)</span><span class="token punctuation">:</span>        z <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>w<span class="token punctuation">,</span> activation<span class="token punctuation">)</span> <span class="token operator">+</span> b        zs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>z<span class="token punctuation">)</span>        activation <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>z<span class="token punctuation">)</span>        activations<span class="token punctuation">.</span>append<span class="token punctuation">(</span>activation<span class="token punctuation">)</span>        <span class="token triple-quoted-string string">'''After feeding forward in a M-layer network,     Zs have M-1 elements while Activations have M elements'''</span>    <span class="token comment"># Preparations for recursion</span>    delta <span class="token operator">=</span> <span class="token punctuation">(</span>activations<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> y<span class="token punctuation">)</span> <span class="token operator">*</span> sigmoid_deriative<span class="token punctuation">(</span>z<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    nabla_b<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> delta    nabla_w<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>delta<span class="token punctuation">,</span> activations<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>        delta <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>self<span class="token punctuation">.</span>weights<span class="token punctuation">[</span><span class="token operator">-</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> delta<span class="token punctuation">)</span> <span class="token operator">*</span> sigmoid_deriative<span class="token punctuation">(</span>zs<span class="token punctuation">[</span><span class="token operator">-</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>        nabla_b<span class="token punctuation">[</span><span class="token operator">-</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> delta        nabla_w<span class="token punctuation">[</span><span class="token operator">-</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>delta<span class="token punctuation">,</span> activations<span class="token punctuation">[</span><span class="token operator">-</span>i<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">(</span>nabla_b<span class="token punctuation">,</span> nabla_w<span class="token punctuation">)</span></code></pre><h3 id="Evaluate"><a href="#Evaluate" class="headerlink" title="Evaluate"></a>Evaluate</h3><p><strong>Use <code>np.argmax()</code> to find the predicted label</strong></p><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> <span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token operator">>></span><span class="token operator">></span> a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token number">7</span></code></pre><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> test_data<span class="token punctuation">,</span> see<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># for x, y in test_data:</span>    <span class="token comment">#     print(self.feedforward(x))</span>    test_results <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>self<span class="token punctuation">.</span>feedforward<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token keyword">for</span> x<span class="token punctuation">,</span> y <span class="token keyword">in</span> test_data<span class="token punctuation">]</span>    <span class="token keyword">return</span> <span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>x<span class="token operator">==</span>y<span class="token punctuation">)</span> <span class="token keyword">for</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token keyword">in</span> test_results<span class="token punctuation">)</span></code></pre><h2 id="Test-our-network"><a href="#Test-our-network" class="headerlink" title="Test our network"></a>Test our network</h2><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><p><a href="http://neuralnetworksanddeeplearning.com/chap1.html#implementing_our_network_to_classify_digits">http://neuralnetworksanddeeplearning.com/chap1.html#implementing_our_network_to_classify_digits</a></p></li><li><p><a href="https://www.zhihu.com/question/43673341/answer/341556216">https://www.zhihu.com/question/43673341/answer/341556216</a></p></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;在学习了相关的&lt;a href=&quot;https://cc7w.cf/but-what-is-a-neural-network/&quot;&gt;理论基础&lt;/a&gt;之后，我们自然是要投入到实践之中，完成这项机器学习领域的 Hello World 任务.&lt;/p&gt;
&lt;p&gt;本篇文章会用来记录 c7w 操作的全过程，以及他试图使用 python3 来拟合一篇 python2 教程的痛苦.&lt;/p&gt;</summary>
    
    
    
    <category term="技术" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="技术/机器学习实现" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/%E6%8A%80%E6%9C%AF-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0/"/>
    
    
    <category term="机器学习" scheme="https://www.c7w.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Note for &quot;But what is a neural network?&quot;</title>
    <link href="https://www.c7w.tech/but-what-is-a-neural-network/"/>
    <id>https://www.c7w.tech/but-what-is-a-neural-network/</id>
    <published>2021-09-03T06:37:19.000Z</published>
    <updated>2022-01-10T11:34:51.207Z</updated>
    
    <content type="html"><![CDATA[<p>Notes taken from watching <strong>3Blue1Brown series: Nerual networks</strong>.</p><ul><li>But what is a nerual network?</li><li>Gradient Descent</li><li>Back Propagation</li></ul><a id="more"></a><h2 id="Chapter-1-Overview"><a href="#Chapter-1-Overview" class="headerlink" title="Chapter 1: Overview"></a>Chapter 1: Overview</h2><p>Consider <strong>Nerual network</strong>:</p><ul><li>What are the neurons?<ul><li>Functions, which take several numbers as input and give a number as output</li></ul></li><li>How are they linked together? </li></ul><p><strong>Layers</strong> (Each layer made up of neurons):</p><ul><li>The activations of one layer determines the activations of the next layer.</li></ul><p>What those <strong>middle layers</strong> might be doing?</p><ul><li>It may be holding subcomponents of the image.</li><li>Input layer -&gt; Edge layer -&gt; Pattern layer -&gt; Output Digit Layer (?)</li></ul><p><strong>Edge Detection Example</strong></p><ul><li>Assign a weight to <strong>each connection</strong> between the neuron and the neurons from the former layer</li></ul><p><img src="https://i.loli.net/2021/09/03/2OhjAcN1p9JesRX.png" alt="image-20210903150326487"></p><ul><li>Let activations from the last layer be $a_1, a_2, a_3, …a_n$​ and the weight numbers be $w_1, w_2, w_3, …w_n$​​.</li><li>Let $w_1a_1 + w_2a_2 + … + w_na_n$ represent the neuron activation? No! We have to make the range of activation between [0, 1], but the result comes along with any possible real number.</li></ul><p><img src="https://i.loli.net/2021/09/03/pQn9PXsdu3lZNO1.png" alt="image-20210903150746983"></p><ul><li>We could use the <strong>sigmoid function</strong>, or the <strong>logistic curve</strong> to solve this. $\sigma(x) = \frac 1 {1+e^{-x}}$​</li></ul><p><img src="https://i.loli.net/2021/09/03/aZoQXd3Ec2T1GFL.png" alt="image-20210903151057105"></p><ul><li>So can we let the activation of the neuron be $\sigma(w_1a_1 + w_2a_2 + … + w_na_n)$​, which is basically a measure of how <strong>positively</strong> the relevant weighted sum is?</li><li>Well, maybe we need some <strong>bias</strong>, say, only activate when $w_1a_1 + w_2a_2 + … + w_na_n &gt; 10$?</li><li>So finally we get, the activation of the neuron, which is $\sigma(w_1a_1 + w_2a_2 + … + w_na_n + bias)$, which is -10 in this case.​​</li></ul><p><strong>Counting weights and biases</strong></p><ul><li>All described above is just above <strong>one specific neuron</strong>, and in fact, in a middle layer, we have several neurons!</li><li>Take the video example, just a two-middle-layered network have more than 13k parameters to tweak!</li></ul><p><img src="https://i.loli.net/2021/09/03/OLumsy3nIMQtekc.png" alt="image-20210903151909640"></p><ul><li>So when we talk about learning, it is about <strong>finding the right weights and biases</strong> to make the network behave in the right way.</li></ul><p><strong>Notations and linear algebra</strong></p><ul><li><p>Let activations from one layer be a column vector: $\begin {bmatrix} a_1^{(0)}\\ a_2^{(0)}\\ \vdots\\ a_n^{(0)}\\ \end {bmatrix}$​​​​​​</p></li><li><p>Let weights of connection between two adjacent layers be a matrix: $\begin {bmatrix} w_{1,1} &amp; w_{1,2} &amp; \cdots &amp; w_{1,n} \\ w_{2,1} &amp; w_{2,2} &amp; \cdots &amp; w_{2,n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ w_{k,1} &amp; w_{k,2} &amp; \cdots &amp; w_{k,n}  \end {bmatrix}$​​</p><ul><li>Row $i$ of the matrix represents the connection weight between neuron $i$ with the neurons from the last layer.</li></ul></li></ul><ul><li><p>Let the biases be in a column vector: $\begin {bmatrix} b_1\\ b_2\\ \vdots\\ b_n\\ \end {bmatrix}$​​​​</p></li><li><p>And let $\sigma(\begin {bmatrix} x\\ y\\ \vdots\\ z\\ \end {bmatrix}) \ := \ \begin {bmatrix} \sigma(x)\\ \sigma(y)\\ \vdots\\ \sigma(z)\\ \end {bmatrix}$</p></li><li><p>So we get our notation now: $a^{(2)} = \sigma(W^{(2)}a^{(1)}+b^{(2)})$​​​</p><ul><li>$a^{(i)}$: the activations of the $i$​-th layer</li><li>$W^{(i)}$: connection weight matrix between layer $i$ and $i-1$</li><li>$b^{(i)}$: biases of neurons in the layer $i$</li></ul></li></ul><h2 id="Chapter-2-Gradient-descent"><a href="#Chapter-2-Gradient-descent" class="headerlink" title="Chapter 2: Gradient descent"></a>Chapter 2: Gradient descent</h2><p><strong>Using training data</strong></p><ul><li>We can divide our data with labels into two groups, the training group and the testing group.</li><li>Firstly, we can use the data in the training group to train our network.</li><li>Then we could use the test group to check its accuracy.</li></ul><p><strong>Cost Function</strong></p><ul><li>Review: Nerual network function<ul><li>Input: 784 numbers (pixels)</li><li>Output: 10 numbers</li><li>Parameters: 13k weights or biases</li></ul></li><li>But the cost function might be like…<ul><li>Input: 13k weights or biases</li><li>Output: 1 single number (namely the cost)</li><li>Parameters: Different set of training examples</li></ul></li><li>Notation<ul><li>$C(w_1, w_2, \cdots, w_{13002}) := \frac 1 {2n} \sum_x|| y(x)-a ||^2$​</li><li>Now we can just try to solve this problem: how to find the minimum of $C$ and the corresponding set of $w$​!</li></ul></li><li>It is hard to solve this minimum problem using mathematic methods when the amount of parameters is high, but we can…<ul><li>Start at an old input</li><li>Figure out which direction you should step to make the cost lower</li><li>And that direction is: $-\nabla C$​</li></ul></li><li>So we could just choose $\Delta v =-\eta\nabla C$​.<ul><li>In which $\eta$​ is a small, positive parameter (known as <em>learning rate</em>) </li><li>Then we can make $ v \rightarrow (v’=v-\eta\nabla C)$​ in every iteration</li></ul></li></ul><p><strong>Anaylsing the network</strong></p><ul><li>Does the network’s middle layer really doing what was imagined? Namely, edges, patterns, etc.?<ul><li>Not at all!</li></ul></li></ul><p><img src="https://i.loli.net/2021/09/03/BrJF1SleKfVTqOD.png" alt="image-20210903164518327"></p><ul><li>The magnitude of each element in the gradient is indicating how sensitive the cost function is to each weight or bias.</li></ul><h2 id="Chapter-3-Back-Propagation"><a href="#Chapter-3-Back-Propagation" class="headerlink" title="Chapter 3: Back Propagation"></a>Chapter 3: Back Propagation</h2><p><strong>What is back propagation?</strong></p><ul><li>It is an algorithm for computing the gradients of the cost function.</li></ul><p><strong>Stochastic gradient descent</strong></p><ul><li>It takes the computer rather long time to add up the influence of every single training example.</li><li>So we can randomly shuffle our training data, then divide it into mini-batches.</li><li>Then we can compute a step according to the mini-batch.</li></ul><p><strong>Chain rule</strong></p><p><img src="https://i.loli.net/2021/09/03/rpMKl7V1faHI4S5.png" alt="image-20210903215157598"></p><pre class="mermaid">graph LRsubgraph "Layer m-1"A0("1")A1("2")A2("3")A3("4")endsubgraph "Layer m"B2("output(i-1)")B0("output(i)")B1("output(i+1)")endsubgraph "Anticipated label"C2("y(i-1)")C0("y(i)")C1("y(i+1)")endA0 --> B0A1 --> B0A2 --> B0A3 --> B0B0 --> C0B1 --> C1B2 --> C2</pre><ul><li>We remember $\begin {bmatrix} a_1^{(2)}\\ a_2^{(2)}\\ \vdots\\ a_k^{(2)}\\ \end {bmatrix}= \sigma (\begin {bmatrix} w^{(2)}_{1,1} &amp; w^{(2)}_{1,2} &amp; \cdots &amp; w^{(2)}_{1,n} \\ w^{(2)}_{2,1} &amp; w^{(2)}_{2,2} &amp; \cdots &amp; w^{(2)}_{2,n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ w^{(2)}_{k,1} &amp; w^{(2)}_{k,2} &amp; \cdots &amp; w^{(2)}_{k,n}  \end {bmatrix}\begin {bmatrix} a_1^{(1)}\\ a_2^{(1)}\\ \vdots\\ a_n^{(1)}\\ \end {bmatrix} + \begin {bmatrix} b_1^{(2)}\\ b_2^{(2)}\\ \vdots\\ b_k^{(2)}\\ \end {bmatrix})$<ul><li>Namely $a^{(L)}_i  = \sigma( \sum_jw^{(L)}_{i, j}a_j^{(L-1)}+ b^{(L)})$​​​​​.​</li><li>We denote this by $z^l = w^la^{l-1}+b^l$​ and $a^l = \sigma(z^l)$​</li></ul></li><li>And we have $C(\vec W, \vec b) = \frac 1 {2n} \sum_x|| y(x)-a ||^2$<ul><li>We assume that $C = \frac 1 n \sum_x C_x$​, in which $C_x = \frac 1 2 ||y-a^{L}||^2$​​​</li></ul></li></ul><pre class="mermaid">graph RLCX("$C$")aE("$a^L$")zE("$z^L$")wE("$w^L$")aEL("$a^{L-1}$")bE("$b^L$")zEL("$z^{L-1}$")wEL("$w^{L-1}$")aELL("$a^{L-2}$")bEL("$b^{L-1}$")aE -->|"$(y_x-a^L_x)^2$"| CXzE -->|"$\sigma$"| aEwE --> zEaEL --> zEbE --> zEzEL -->|"$\sigma$"| aELwEL --> zELaELL --> zELbEL --> zELdot("$\cdots$") --> aELL</pre><ul><li>Then the core equations of back propagation…<ul><li>$\frac {\partial C} {\partial z_i^L} = \frac {\partial C} {\partial a_i^L}\frac {\partial a_i^L} {\partial z_i^L} = (a_i^L-y_i)\sigma’(z_i^L)$​​​​ (Initialize)​<ul><li>That is $\delta := \frac {\partial C} {\partial z^L} = (a^L-y) \odot \sigma’(z^L)$​​</li></ul></li><li>According to $z^{(L)}_i = \sum_jw^{(L)}_{i, j}a_j^{(L-1)}+ b_i^{(L)}$​​​​, suppose we have calculated $\frac {\partial C} {\partial z^M_i}$​​​ for all neuron $i$​​​ in layer $M$​​​.<ul><li>How can we get $\frac {\partial C} {\partial w^M_{i, j}}$​​​ for all neuron $i$​​​ in layer $M$​​ and $j$​​ in layer $M-1$​​​​?<ul><li>$\frac {\partial C} {\partial w^M_{i, j}} = \frac {\partial C} {\partial z^M_i} \frac {\partial z^M_i}{\partial w^M_{i, j}} = \frac {\partial C} {\partial z^M_i} a_j^{M-1}$​<ul><li>$\frac {\partial C} {\partial w^M} = \frac {\partial C} {\partial z^L} (a^{M-1})^T$</li></ul></li></ul></li><li>How can we get $\frac {\partial C} {\partial b^M_i} $​​ for all neuron $i$​​ in layer $M$​?<ul><li>$\frac {\partial C} {\partial b^M_{i}} = \frac {\partial C} {\partial z^M_i} \frac {\partial z^M_i}{\partial b^M_{i}} = \frac {\partial C} {\partial z^M_i}$​<ul><li>$\frac {\partial C} {\partial b^M} = \frac {\partial C} {\partial z^M}$</li></ul></li></ul></li><li>How can we get $\frac {\partial C} {\partial a^{M-1}_j} $ and $\frac {\partial C} {\partial z^{M-1}_j} $ for all neuron $j$ in layer $M-1$?<ul><li>$\frac {\partial C} {\partial a^{M-1}_j}  = \sum_i \frac {\partial C}{\partial z^M_i} \frac {\partial z^M_i} {a_j^{M-1}} = \sum_i \frac {\partial C}{\partial z^M_i} w_{i,j}^M$​<ul><li>$\frac {\partial C} {\partial a^{M-1}} = (w^M)^T \frac {\partial C} {\partial z^M}$​</li></ul></li><li>Then $\frac {\partial C} {\partial z^{M-1}_j} = \frac {\partial C} {\partial a^{M-1}_j} \sigma’(z^{M-1}_j)$​<ul><li>$\frac {\partial C} {\partial z^{M-1}} = (w^M)^T \frac {\partial C} {\partial z^M}\odot\sigma’(z^{M-1})$</li></ul></li></ul></li></ul></li><li>Then by recursion, we could calculate all the partial derivatives of weights and biases.</li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;Notes taken from watching &lt;strong&gt;3Blue1Brown series: Nerual networks&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;But what is a nerual network?&lt;/li&gt;
&lt;li&gt;Gradient Descent&lt;/li&gt;
&lt;li&gt;Back Propagation&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="理论" scheme="https://www.c7w.tech/categories/%E7%90%86%E8%AE%BA/"/>
    
    <category term="理论/机器学习" scheme="https://www.c7w.tech/categories/%E7%90%86%E8%AE%BA/%E7%90%86%E8%AE%BA-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://www.c7w.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Python Numpy + Matplotlib 再探</title>
    <link href="https://www.c7w.tech/python-numpy-matplotlib/"/>
    <id>https://www.c7w.tech/python-numpy-matplotlib/</id>
    <published>2021-08-30T06:22:44.000Z</published>
    <updated>2022-01-10T11:30:03.318Z</updated>
    
    <content type="html"><![CDATA[<p>为什么是再探？因为之前咕了不知道多少次了，看了忘忘了看.</p><a id="more"></a><h2 id="Numpy"><a href="#Numpy" class="headerlink" title="Numpy"></a>Numpy</h2><h3 id="ndarray"><a href="#ndarray" class="headerlink" title="ndarray"></a>ndarray</h3><ul><li>NumPy’s array class is called <code>ndarray</code>. It is also known by the alias <code>array</code>. </li></ul><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> npli <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>li<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>li<span class="token punctuation">.</span>ndim<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>li<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>li<span class="token punctuation">.</span>size<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>li<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><pre class="language-none"><code class="language-none">[[ 0  1  2  3  4] [ 5  6  7  8  9] [10 11 12 13 14] [15 16 17 18 19]]2int6420&lt;class &#39;numpy.ndarray&#39;&gt;</code></pre><ul><li><code>arange(lower_bound, upper_bound, step)</code>, <code>zeros</code>, <code>ones</code>, <code>linspace(lo,hi,count)</code></li></ul><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> math<span class="token keyword">import</span> numpy <span class="token keyword">as</span> npx <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> math<span class="token punctuation">.</span>pi<span class="token operator">*</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>y <span class="token operator">=</span> np<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span></code></pre><pre class="language-none"><code class="language-none">[0.         0.6981317  1.3962634  2.0943951  2.7925268  3.4906585 4.1887902  4.88692191 5.58505361 6.28318531] [ 0.00000000e+00  6.42787610e-01  9.84807753e-01  8.66025404e-01        3.42020143e-01 -3.42020143e-01 -8.66025404e-01 -9.84807753e-01 -6.42787610e-01 -2.44929360e-16]</code></pre><ul><li><code>reshape()</code></li></ul><h3 id="Basic-operations"><a href="#Basic-operations" class="headerlink" title="Basic operations"></a>Basic operations</h3><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> npa <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">,</span> <span class="token number">40</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">]</span><span class="token punctuation">)</span>b <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token operator">+</span>b<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token operator">/</span>b<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a <span class="token operator">&lt;</span> <span class="token number">30</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token operator">*</span>b<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span>c <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>d <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>c@d<span class="token punctuation">)</span></code></pre><pre class="language-none"><code class="language-none">[10 20 30 40 50][1 2 3 4 5][11 22 33 44 55][10. 10. 10. 10. 10.][ 1  4  9 16 25][ True  True False False False][ 10  40  90 160 250]550[[19 22] [43 50]]</code></pre><ul><li><code>exp()</code> …</li></ul><h3 id="Indexing-and-Slicing"><a href="#Indexing-and-Slicing" class="headerlink" title="Indexing and Slicing"></a>Indexing and Slicing</h3><ul><li><strong>One-dimensional</strong> arrays can be indexed, sliced and iterated over, much like <a href="https://docs.python.org/tutorial/introduction.html#lists">lists</a> and other Python sequences.</li><li><strong>Multidimensional</strong> arrays can have one index per axis. These indices are given in a tuple separated by commas.</li><li>The <strong>dots</strong> (<code>...</code>) represent as many colons as needed to produce a complete indexing tuple. For example, if <code>x</code> is an array with 5 axes, then<ul><li><code>x[1, 2, ...]</code> is equivalent to <code>x[1, 2, :, :, :]</code>,</li><li><code>x[..., 3]</code> to <code>x[:, :, :, :, 3]</code> and</li><li><code>x[4, ..., 5, :]</code> to <code>x[4, :, :, 5, :]</code>.</li></ul></li></ul><h3 id="Iterating"><a href="#Iterating" class="headerlink" title="Iterating"></a>Iterating</h3><ul><li><strong>Iterating</strong> over multidimensional arrays is done with respect to the first axis.</li><li>But you can use <code>.flat</code> attribute which could serve as an iterator to iterate over all the elements over the array.</li></ul><h2 id="Matplotlib"><a href="#Matplotlib" class="headerlink" title="Matplotlib"></a>Matplotlib</h2><h3 id="Goals"><a href="#Goals" class="headerlink" title="Goals"></a>Goals</h3><ul><li><p>画不同图像</p><ul><li>散点图、折线图</li><li>饼图 </li><li>柱状图 </li></ul></li><li><p>更改表格的 style</p></li><li>子图</li></ul><h3 id="图像的绘制"><a href="#图像的绘制" class="headerlink" title="图像的绘制"></a>图像的绘制</h3><h4 id="散点图与折线图"><a href="#散点图与折线图" class="headerlink" title="散点图与折线图"></a>散点图与折线图</h4><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> mathx <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">*</span>math<span class="token punctuation">.</span>pi<span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>y <span class="token operator">=</span> np<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>x<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>savefig<span class="token punctuation">(</span><span class="token string">"1.png"</span><span class="token punctuation">)</span></code></pre><p><img src="https://i.loli.net/2021/08/30/T8Pj3ICZ64o1NQx.png" alt="1"></p><p>And when we changed 5 points to 500…</p><p><img src="https://i.loli.net/2021/08/30/FJ5qG7nApHutmET.png" alt="1"></p><h5 id="Markers-and-line-styles"><a href="#Markers-and-line-styles" class="headerlink" title="Markers and line styles"></a>Markers and line styles</h5><p><strong>Markers</strong></p><div class="table-container"><table><thead><tr><th>character</th><th>description</th></tr></thead><tbody><tr><td><code>&#39;.&#39;</code></td><td>point marker</td></tr><tr><td><code>&#39;,&#39;</code></td><td>pixel marker</td></tr><tr><td><code>&#39;o&#39;</code></td><td>circle marker</td></tr><tr><td><code>&#39;v&#39;</code></td><td>triangle_down marker</td></tr><tr><td><code>&#39;^&#39;</code></td><td>triangle_up marker</td></tr><tr><td><code>&#39;&lt;&#39;</code></td><td>triangle_left marker</td></tr><tr><td><code>&#39;&gt;&#39;</code></td><td>triangle_right marker</td></tr><tr><td><code>&#39;1&#39;</code></td><td>tri_down marker</td></tr><tr><td><code>&#39;2&#39;</code></td><td>tri_up marker</td></tr><tr><td><code>&#39;3&#39;</code></td><td>tri_left marker</td></tr><tr><td><code>&#39;4&#39;</code></td><td>tri_right marker</td></tr><tr><td><code>&#39;8&#39;</code></td><td>octagon marker</td></tr><tr><td><code>&#39;s&#39;</code></td><td>square marker</td></tr><tr><td><code>&#39;p&#39;</code></td><td>pentagon marker</td></tr><tr><td><code>&#39;P&#39;</code></td><td>plus (filled) marker</td></tr><tr><td><code>&#39;*&#39;</code></td><td>star marker</td></tr><tr><td><code>&#39;h&#39;</code></td><td>hexagon1 marker</td></tr><tr><td><code>&#39;H&#39;</code></td><td>hexagon2 marker</td></tr><tr><td><code>&#39;+&#39;</code></td><td>plus marker</td></tr><tr><td><code>&#39;x&#39;</code></td><td>x marker</td></tr><tr><td><code>&#39;X&#39;</code></td><td>x (filled) marker</td></tr><tr><td><code>&#39;D&#39;</code></td><td>diamond marker</td></tr><tr><td><code>&#39;d&#39;</code></td><td>thin_diamond marker</td></tr><tr><td><img src="https://i.loli.net/2021/09/03/fa3MXH4lvBU6jq7.png" alt="image-20210903222651862"></td><td>vline marker</td></tr><tr><td><code>&#39;_&#39;</code></td><td>hline marker</td></tr></tbody></table></div><ul><li>Example</li></ul><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> mathx <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">*</span>math<span class="token punctuation">.</span>pi<span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>y <span class="token operator">=</span> np<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>x<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> <span class="token string">'s'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>savefig<span class="token punctuation">(</span><span class="token string">"1.png"</span><span class="token punctuation">)</span></code></pre><p><img src="https://i.loli.net/2021/08/30/hkl36mb9unPYUtE.png" alt="1"></p><p><strong>Line styles</strong></p><div class="table-container"><table><thead><tr><th>character</th><th>description</th></tr></thead><tbody><tr><td><code>&#39;-&#39;</code></td><td>solid line style</td></tr><tr><td><code>&#39;--&#39;</code></td><td>dashed line style</td></tr><tr><td><code>&#39;-.&#39;</code></td><td>dash-dot line style</td></tr><tr><td><code>&#39;:&#39;</code></td><td>dotted line style</td></tr></tbody></table></div><ul><li>Example</li></ul><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> mathx <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token operator">*</span>math<span class="token punctuation">.</span>pi<span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span>y <span class="token operator">=</span> np<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>x<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> <span class="token string">'s-'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>savefig<span class="token punctuation">(</span><span class="token string">"1.png"</span><span class="token punctuation">)</span></code></pre><p><img src="https://i.loli.net/2021/08/30/mI6pTX2EBwclnOr.png" alt="1"></p><h4 id="饼图"><a href="#饼图" class="headerlink" title="饼图"></a>饼图</h4><p><code>pyplot.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False, labeldistance=1.1, startangle=0, radius=1, counterclock=True, wedgeprops=None, textprops=None, center=0, 0, frame=False, rotatelabels=False, *, normalize=None, data=None)[source]</code></p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> mathx <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"CA"</span><span class="token punctuation">,</span> <span class="token string">"LA"</span><span class="token punctuation">,</span> <span class="token string">"FOP"</span><span class="token punctuation">,</span> <span class="token string">"DM"</span><span class="token punctuation">]</span>y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">90</span><span class="token punctuation">,</span> <span class="token number">90</span><span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">]</span>plt<span class="token punctuation">.</span>pie<span class="token punctuation">(</span>y<span class="token punctuation">,</span> labels<span class="token operator">=</span>x<span class="token punctuation">)</span>plt<span class="token punctuation">.</span>savefig<span class="token punctuation">(</span><span class="token string">"1.png"</span><span class="token punctuation">)</span></code></pre><p><img src="https://i.loli.net/2021/08/30/Io1HG9FxpVXTyli.png" alt="1"></p><p><strong>参数的使用</strong></p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> mathx <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"CA"</span><span class="token punctuation">,</span> <span class="token string">"LA"</span><span class="token punctuation">,</span> <span class="token string">"FOP"</span><span class="token punctuation">,</span> <span class="token string">"DM"</span><span class="token punctuation">]</span>y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">80</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">]</span>plt<span class="token punctuation">.</span>pie<span class="token punctuation">(</span>y<span class="token punctuation">,</span> labels<span class="token operator">=</span>x<span class="token punctuation">,</span> explode<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> autopct<span class="token operator">=</span><span class="token string">'%.2f%%'</span><span class="token punctuation">)</span><span class="token comment"># %d%% 整数百分比，%0.1f%% 一位小数百分比， %0.2f%% 两位小数百分比</span>plt<span class="token punctuation">.</span>savefig<span class="token punctuation">(</span><span class="token string">"1.png"</span><span class="token punctuation">)</span></code></pre><p><img src="https://i.loli.net/2021/08/30/Cl6AKWtjYJQ2soH.png" alt="1"></p><h4 id="柱形图"><a href="#柱形图" class="headerlink" title="柱形图"></a>柱形图</h4><p><code>matplotlib.pyplot.bar(x, height, width=0.8, bottom=None, *, align=&#39;center&#39;, data=None, **kwargs)</code></p><h3 id="Prettify"><a href="#Prettify" class="headerlink" title="Prettify"></a>Prettify</h3><p><strong>设置标题</strong></p><p><code>plt.title(title)</code></p><p><strong>设置图例</strong></p><p><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.legend.html#matplotlib.pyplot.legend">https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.legend.html#matplotlib.pyplot.legend</a></p><h3 id="Subplots"><a href="#Subplots" class="headerlink" title="Subplots"></a>Subplots</h3><p><code>plt.subplot(nrows, ncols, index)</code></p><p><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot">https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot</a></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://numpy.org/doc/stable/user/quickstart.html">https://numpy.org/doc/stable/user/quickstart.html</a></li><li><a href="https://www.runoob.com/numpy/numpy-ndarray-object.html">https://www.runoob.com/numpy/numpy-ndarray-object.html</a></li><li><a href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html">https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html</a></li><li><a href="https://www.runoob.com/matplotlib/matplotlib-pie.html">https://www.runoob.com/matplotlib/matplotlib-pie.html</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;为什么是再探？因为之前咕了不知道多少次了，看了忘忘了看.&lt;/p&gt;</summary>
    
    
    
    <category term="技术" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="技术/Python应用" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/%E6%8A%80%E6%9C%AF-Python%E5%BA%94%E7%94%A8/"/>
    
    
    <category term="数据分析" scheme="https://www.c7w.tech/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>Nana Chat —— QQ Bot 模块</title>
    <link href="https://www.c7w.tech/NanaChat/"/>
    <id>https://www.c7w.tech/NanaChat/</id>
    <published>2021-08-25T15:33:14.000Z</published>
    <updated>2022-01-10T11:32:05.697Z</updated>
    
    <content type="html"><![CDATA[<p>服务器基本的配置已经完成，我们可以使用她来做有趣的事了。</p><p>接下来和 Nana 的日常，主要是从“实现项目”这个角度来展开。</p><p>所以，我们这次还是祭出我们的传统艺能，QQ Bot.</p><p>不过这次要在项目管理方面做好一点，不能再是 if-else 的大杂烩了.</p><h1 id="Current-Status-咕了"><a href="#Current-Status-咕了" class="headerlink" title="Current Status: 咕了"></a>Current Status: 咕了</h1><a id="more"></a><h2 id="Dependencies-前期调研"><a href="#Dependencies-前期调研" class="headerlink" title="Dependencies 前期调研"></a>Dependencies 前期调研</h2><h3 id="mirai-amp-mirai-api-http"><a href="#mirai-amp-mirai-api-http" class="headerlink" title="mirai &amp; mirai-api-http"></a>mirai &amp; mirai-api-http</h3><p>Mirai 是一个在全平台下运行，提供 QQ Android 和 TIM PC 协议支持的高效率机器人框架.</p><p><a href="https://github.com/project-mirai/mirai-api-http">https://github.com/project-mirai/mirai-api-http</a></p><h4 id="下载与设置守护进程"><a href="#下载与设置守护进程" class="headerlink" title="下载与设置守护进程"></a>下载与设置守护进程</h4><ul><li>安装 Java 运行时 <code>sudo apt install openjdk-11-jre</code></li><li>下载 MCL，安装 mirai-api-http</li><li>登录 Bot（使用 QQ 浏览器验证？使用 QQ 打开链接即可）<ul><li>如果有需要的话可以安装验证码库</li></ul></li><li><p>设置为自动登录</p></li><li><p>配置 supervisor</p></li></ul><pre class="language-conf" data-language="conf"><code class="language-conf">[program:mcl]command&#x3D;&#x2F;home&#x2F;ftpuser&#x2F;5050-mcl&#x2F;mcl-1.2.2&#x2F;mclautostart&#x3D;trueautorestart&#x3D;trueuser&#x3D;root</code></pre><h3 id="Brainstorming-项目构思"><a href="#Brainstorming-项目构思" class="headerlink" title="Brainstorming 项目构思"></a>Brainstorming 项目构思</h3><p>我们可以考虑采用 websocket 这种 adapter，可以同时处理主动发信和消息上报事件.</p><pre class="mermaid">graph TB    subgraph "NanaChat (Daemon)"        nCore["NanaChat-Core (Port 5051)"]        nPlugin[NanaChat-Plugins]    end    subgraph "Mirai-api-http (Daemon)"        ws["WebSocket Server (Port 5050)"]    end    wa-->|"收信 [1]"|nCore    nCore -->|"主动发信"| ws    ws -->|"消息上报"| nCore    nCore --> nPlugin    nPlugin --> nCore</pre><pre class="mermaid">graph TD;ncore[NanaChat-Core]s[Scheduler<br/>定时事件]d[Dispatcher<br/>消息发送处理]l[Listener<br/>创建 Socket 连接<br/>监听事件]pm[PluginManager<br/>管理插件<br/>协助注册监听器与调度器]ncore --> lncore --> sncore --> dncore --> pm</pre><h2 id="Docs"><a href="#Docs" class="headerlink" title="Docs"></a>Docs</h2><h3 id="Nana-Core"><a href="#Nana-Core" class="headerlink" title="Nana-Core"></a>Nana-Core</h3><h3 id="Nana-Plugins"><a href="#Nana-Plugins" class="headerlink" title="Nana-Plugins"></a>Nana-Plugins</h3><p>注册 Nana-Core::Listener…</p><p>使用 Nana-Core::Dispatcher…</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;服务器基本的配置已经完成，我们可以使用她来做有趣的事了。&lt;/p&gt;
&lt;p&gt;接下来和 Nana 的日常，主要是从“实现项目”这个角度来展开。&lt;/p&gt;
&lt;p&gt;所以，我们这次还是祭出我们的传统艺能，QQ Bot.&lt;/p&gt;
&lt;p&gt;不过这次要在项目管理方面做好一点，不能再是 if-else 的大杂烩了.&lt;/p&gt;
&lt;h1 id=&quot;Current-Status-咕了&quot;&gt;&lt;a href=&quot;#Current-Status-咕了&quot; class=&quot;headerlink&quot; title=&quot;Current Status: 咕了&quot;&gt;&lt;/a&gt;Current Status: 咕了&lt;/h1&gt;</summary>
    
    
    
    <category term="技术" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="技术/综合" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/%E6%8A%80%E6%9C%AF-%E7%BB%BC%E5%90%88/"/>
    
    
    <category term="QQ Bot" scheme="https://www.c7w.tech/tags/QQ-Bot/"/>
    
  </entry>
  
  <entry>
    <title>与 Nana 的日常 —— 从零开始的 Linux 调教指北(2)</title>
    <link href="https://www.c7w.tech/dear-memory-with-nana-2/"/>
    <id>https://www.c7w.tech/dear-memory-with-nana-2/</id>
    <published>2021-08-25T13:10:28.000Z</published>
    <updated>2022-01-10T11:28:23.657Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/08/23/78DGrYSMnVgKkwE.jpg" alt="QQ图片20210823091857"></p><p>(Nana 印象图 初版)</p><p>速览本文：</p><ul><li>Docker 的安装</li><li>数据库的安装<ul><li>MariaDB</li><li>phpMyAdmin</li></ul></li><li>nodejs 与 npm 的安装</li></ul><a id="more"></a><h2 id="Docker-的介绍与安装"><a href="#Docker-的介绍与安装" class="headerlink" title="Docker 的介绍与安装"></a>Docker 的介绍与安装</h2><blockquote><p>What is <strong>Docker</strong>?</p><p>Docker takes away repetitive, mundane configuration tasks and is used throughout the development lifecycle for fast, easy and portable application development - desktop and cloud.</p><p>简而言之，就是打包好的应用，可以创建Docker容器来托管一系列应用.</p></blockquote><p><strong>安装</strong>：</p><p>(Option 1) 使用官方安装脚本自动安装即可</p><p><code>curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun</code></p><p>(Option 2) 也可使用国内 daocloud 一键安装：</p><p><code>curl -sSL https://get.daocloud.io/docker | sh</code></p><blockquote><p>但是在这里 c7w ran into problems.</p><p>详情可以看这里. 针对 armhf ubuntu 20.04: <a href="https://github.com/docker/for-linux/issues/1035">https://github.com/docker/for-linux/issues/1035</a></p><p>如果一键安装成功，请略过此处.</p><p><s><strong>想不到吧 爷去装arm64了 不伺候您armhf了</strong> <strong>只不过是从头再来</strong> <strong>停更一天</strong></s></p><p>大家就当无事发生过，这里 Nana 已经变成 arm64 了.</p></blockquote><p>(Option 3) 也可以使用 Tsinghua 源.</p><p>可以参考 <a href="https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/">https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/</a> 手动安装.</p><h2 id="数据库安装与配置"><a href="#数据库安装与配置" class="headerlink" title="数据库安装与配置"></a>数据库安装与配置</h2><h3 id="MariaDB"><a href="#MariaDB" class="headerlink" title="MariaDB"></a>MariaDB</h3><blockquote><p>什么是 <strong>MariaDB</strong>?</p><p>MariaDB Server is one of the most popular database servers in the world. It’s made by the original developers of MySQL and guaranteed to stay open source. </p><p>MariaDB数据库管理系统是MySQL的一个分支，主要由开源社区在维护，采用GPL授权许可。 开发这个分支的原因之一是：甲骨文公司收购了MySQL后，有将MySQL闭源的潜在风险，因此社区采用分支的方式来避开这个风险。 MariaDB的目的是完全兼容MySQL，包括API和命令行，使之能轻松成为MySQL的代替品。</p><p>Reference:</p><ul><li><a href="https://cloud.tencent.com/developer/article/1635038">https://cloud.tencent.com/developer/article/1635038</a></li><li><a href="https://hub.docker.com/_/mariadb">https://hub.docker.com/_/mariadb</a></li></ul></blockquote><p><strong>安装</strong></p><p><code>sudo apt install mariadb-server</code></p><p><strong>查看字符集</strong></p><ul><li><code>sudo mysql</code> 进入控制台</li></ul><pre class="language-none"><code class="language-none"> ubuntu@ubuntu &gt; &#x2F;etc&#x2F;mysql &gt; sudo mysqlWelcome to the MariaDB monitor.  Commands end with ; or \g.Your MariaDB connection id is 52Server version: 10.3.31-MariaDB-0ubuntu0.20.04.1 Ubuntu 20.04Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.Type &#39;help;&#39; or &#39;\h&#39; for help. Type &#39;\c&#39; to clear the current input statement.MariaDB [(none)]&gt; </code></pre><ul><li><code>show variables like &quot;%character%&quot;;show variables like &quot;%collation%&quot;;</code> 查看当前字符集</li></ul><pre class="language-none"><code class="language-none">+--------------------------+----------------------------+| Variable_name            | Value                      |+--------------------------+----------------------------+| character_set_client     | utf8mb4                    || character_set_connection | utf8mb4                    || character_set_database   | utf8mb4                    || character_set_filesystem | binary                     || character_set_results    | utf8mb4                    || character_set_server     | utf8mb4                    || character_set_system     | utf8                       || character_sets_dir       | &#x2F;usr&#x2F;share&#x2F;mysql&#x2F;charsets&#x2F; |+--------------------------+----------------------------+8 rows in set (0.004 sec)+----------------------+--------------------+| Variable_name        | Value              |+----------------------+--------------------+| collation_connection | utf8mb4_general_ci || collation_database   | utf8mb4_general_ci || collation_server     | utf8mb4_general_ci |+----------------------+--------------------+3 rows in set (0.003 sec)</code></pre><p>这里已经是 <code>utf8mb4</code> 了，无须再更多配置.</p><p><strong>安全性配置</strong></p><p><code>sudo mysql_secure_installation</code> 进行安全性配置.</p><pre class="language-none"><code class="language-none">NOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MariaDB      SERVERS IN PRODUCTION USE!  PLEASE READ EACH STEP CAREFULLY!In order to log into MariaDB to secure it, we&#39;ll need the currentpassword for the root user.  If you&#39;ve just installed MariaDB, andyou haven&#39;t set the root password yet, the password will be blank,so you should just press enter here.Enter current password for root (enter for none):OK, successfully used password, moving on...Setting the root password ensures that nobody can log into the MariaDBroot user without the proper authorisation.Set root password? [Y&#x2F;n] YNew password:Re-enter new password:Password updated successfully!Reloading privilege tables.. ... Success!By default, a MariaDB installation has an anonymous user, allowing anyoneto log into MariaDB without having to have a user account created forthem.  This is intended only for testing, and to make the installationgo a bit smoother.  You should remove them before moving into aproduction environment.Remove anonymous users? [Y&#x2F;n] Y ... Success!Normally, root should only be allowed to connect from &#39;localhost&#39;.  Thisensures that someone cannot guess at the root password from the network.Disallow root login remotely? [Y&#x2F;n] Y ... Success!By default, MariaDB comes with a database named &#39;test&#39; that anyone canaccess.  This is also intended only for testing, and should be removedbefore moving into a production environment.Remove test database and access to it? [Y&#x2F;n] Y - Dropping test database... ... Success! - Removing privileges on test database... ... Success!Reloading the privilege tables will ensure that all changes made so farwill take effect immediately.Reload privilege tables now? [Y&#x2F;n] Y ... Success!Cleaning up...All done!  If you&#39;ve completed all of the above steps, your MariaDBinstallation should now be secure.Thanks for using MariaDB!</code></pre><p><strong>新建管理员用户</strong></p><p>先 <code>sudo mysql</code> 进入控制台.</p><ul><li><code>CREATE USER &#39;&lt;USERNAME&gt;&#39;@&#39;%&#39; IDENTIFIED BY &#39;&lt;YOUR PASSWORD&gt;&#39;;</code> 创建用户</li><li><code>GRANT ALL PRIVILEGES ON *.* TO &#39;&lt;USERNAME&gt;&#39;@&#39;%&#39; WITH GRANT OPTION;</code> 给予管理员权限</li></ul><h3 id="phpMyAdmin"><a href="#phpMyAdmin" class="headerlink" title="phpMyAdmin"></a>phpMyAdmin</h3><p><strong>安装</strong></p><p><code>sudo apt install phpmyadmin</code></p><p>在询问是否需要帮助创建数据库时，选择取消.</p><p>在询问选择哪个软件作为管理 phpmyadmin server 的工具时，选择 apache2.</p><p><strong>更改配置</strong></p><ul><li>把 phpmyadmin 提供的默认的 apache2 配置文件移动到 apache2 目录下:</li></ul><p><code>sudo mv /etc/phpmyadmin/apache.conf /etc/apache2/conf-enabled</code></p><ul><li>然后更改 apache2 监听的端口：</li></ul><p><code>sudo vi /etc/apache2/ports.conf</code></p><p>把 80 端口改为没有被占用的端口.</p><ul><li>重启 apache2 服务：<code>sudo systemctl apache2 restart</code></li><li>尝试访问 <a href="http://IP_ADDRESS:PORT/phpmyadmin，然后使用上述新建的管理员账户密码完成登录">http://IP_ADDRESS:PORT/phpmyadmin，然后使用上述新建的管理员账户密码完成登录</a>.</li></ul><p><img src="https://i.loli.net/2021/08/25/NsUTCOdzWXnYmSu.png" alt="image-20210825202754563"></p><h2 id="Node-的安装"><a href="#Node-的安装" class="headerlink" title="Node 的安装"></a>Node 的安装</h2><p>我们直接使用已完成编译的包.</p><ul><li><p>先下载安装文件 <code>wget https://nodejs.org/dist/v14.17.5/node-v14.17.5-linux-arm64.tar.xz</code></p></li><li><p>解压 <code>tar xf node-v14.17.5-linux-arm64.tar.xz</code></p></li><li>进入目录 <code>cd node-v14.17.5-linux-arm64/</code></li><li>执行 node 查看版本 <code>./bin/node -v</code></li></ul><blockquote><p>这里 c7w 强迫症，还去搜索了这个文件应该放哪里比较好.</p><pre><code>有时候需要配置ubuntu安装的软件，一般安装软件都是使用apt-get install。那么安装完后，软件的安装目录在哪里呢，可执行文件又放在哪里呢。A、下载的软件的存放位置：/var/cache/apt/archivesB、安装后软件的默认位置：/usr/shareC、可执行文件位置：/usr/binD、配置文件位置：/etcE、lib文件位置：/usr/lib</code></pre><p>Reference:</p><ul><li><a href="https://blog.csdn.net/u013276277/article/details/81033129">https://blog.csdn.net/u013276277/article/details/81033129</a></li></ul></blockquote><ul><li>移动文件： <code>sudo mv node-v14.17.5-linux-arm64 /usr/share</code></li><li>建立软链接</li></ul><pre class="language-none"><code class="language-none">ubuntu@ubuntu  &#x2F;usr&#x2F;bin  sudo ln -s &#x2F;usr&#x2F;share&#x2F;node-v14.17.5-linux-arm64&#x2F;bin&#x2F;npm &#x2F;usr&#x2F;local&#x2F;bin ubuntu@ubuntu  &#x2F;usr&#x2F;bin  sudo ln -s &#x2F;usr&#x2F;share&#x2F;node-v14.17.5-linux-arm64&#x2F;bin&#x2F;node &#x2F;usr&#x2F;local&#x2F;bin ubuntu@ubuntu  &#x2F;usr&#x2F;bin  nodeWelcome to Node.js v14.17.5.Type &quot;.help&quot; for more information.&gt;</code></pre>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/08/23/78DGrYSMnVgKkwE.jpg&quot; alt=&quot;QQ图片20210823091857&quot;&gt;&lt;/p&gt;
&lt;p&gt;(Nana 印象图 初版)&lt;/p&gt;
&lt;p&gt;速览本文：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Docker 的安装&lt;/li&gt;
&lt;li&gt;数据库的安装&lt;ul&gt;
&lt;li&gt;MariaDB&lt;/li&gt;
&lt;li&gt;phpMyAdmin&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;nodejs 与 npm 的安装&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="技术" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="技术/Linux" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/%E6%8A%80%E6%9C%AF-Linux/"/>
    
    
    <category term="Linux" scheme="https://www.c7w.tech/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>与 Nana 的日常 —— 从零开始的 Linux 调教指北(1)</title>
    <link href="https://www.c7w.tech/dear-memory-with-nana-1/"/>
    <id>https://www.c7w.tech/dear-memory-with-nana-1/</id>
    <published>2021-08-23T12:55:49.000Z</published>
    <updated>2022-01-10T11:32:24.894Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/08/23/78DGrYSMnVgKkwE.jpg" alt="QQ图片20210823091857"></p><p>(Nana 印象图 初版)</p><p>速览本文：</p><ul><li>FTP 配置</li><li>网络配置<ul><li>Cloudflared 内网穿透</li><li>nginx 反向代理</li><li>gunicorn [Python WSGI HTTP Server for UNIX]</li><li>Supervisor 设置守护进程</li></ul></li></ul><a id="more"></a><h2 id="FTP-配置"><a href="#FTP-配置" class="headerlink" title="FTP 配置"></a>FTP 配置</h2><blockquote><p>前置知识：【FTP 文件传输】</p></blockquote><p><strong>安装 vsftpd</strong></p><blockquote><p>什么是 vsftpd ?</p><p><strong>vsftpd</strong> is a GPL licensed <strong>FTP server</strong> for UNIX systems, including Linux.</p></blockquote><p><code>sudo apt install vsftpd</code></p><p><code>sudo systemctl start vsftpd</code> // 开启进程<br><code>sudo systemctl enable vsftpd</code> // 开机启动</p><p><strong>配置 FTP 用户</strong></p><ul><li>创建用户目录：<code>sudo mkdir /home/ftpuser</code></li><li>创建用户并关联用户文件夹，这样使用 ftpuser 用户登陆的时候，就可以直接登陆到 /home/ftpuser 下：<code>sudo useradd -d /home/ftpuser -s /bin/bash ftpuser</code></li><li>更改用户目录权限：<code>sudo chown ftpuser:ftpuser /home/ftpuser</code></li><li>重设 ftpuser 的密码：<code>sudo passwd ftpuser</code> </li></ul><p><strong>配置 FTP 设置</strong></p><p><code>sudo vi /etc/vsftpd.conf</code></p><pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># Uncomment this to enable any form of FTP write command.</span><span class="token comment">#write_enable=YES</span></code></pre><p>把这行注释解除.</p><p><code>vim /etc/pam.d/vsftpd</code></p><p>注释掉</p><p><code>#auth   required pam_shells.so</code></p><p><strong>重启 FTP 服务</strong></p><p><code>sudo service vsftpd restart</code></p><p><strong>测试连接</strong></p><p><img src="https://i.loli.net/2021/08/23/OafiV2PeX9NYRrh.png" alt="image-20210823211829678"></p><p><img src="https://i.loli.net/2021/08/23/L8xdsFEH2UYXDnV.png" alt="image-20210823212448472"></p><p>Reference:</p><ul><li><a href="https://www.huaweicloud.com/articles/4fc98eb98f27a99aa1b06b0d71dab20a.html">https://www.huaweicloud.com/articles/4fc98eb98f27a99aa1b06b0d71dab20a.html</a></li><li><a href="https://devanswers.co/vsftpd-550-permission-denied-ubuntu/">https://devanswers.co/vsftpd-550-permission-denied-ubuntu/</a></li></ul><h2 id="网络配置"><a href="#网络配置" class="headerlink" title="网络配置"></a>网络配置</h2><h3 id="Cloudflared-内网穿透"><a href="#Cloudflared-内网穿透" class="headerlink" title="Cloudflared 内网穿透"></a>Cloudflared 内网穿透</h3><blockquote><p><strong>为什么要做内网穿透？</strong></p><p>在没有做内网穿透之前，我们的 Nana 只能被 Peking-Visitor（也就是路由器管理的网络）和 Zerotier 创建的虚拟网络中被访问到.</p><p>而内网穿透就是为了让我们的 Nana 提供的服务能够被 public Internet 访问到.</p><p><strong>什么是 Cloudflare? </strong></p><p>[这里认为读者已经对<strong>域名</strong>,<strong>域名解析</strong>有了基本的理解]</p><p>Cloudflare.com 是一家域名解析商. 可以为域名提供解析服务.</p><p>例如：</p><p><img src="https://i.loli.net/2021/08/23/bYPs3hkwjvnxc8E.png" alt="image-20210823204436613"></p><p>[请自行探索 DNS 记录：什么是 A 记录？什么是 CNAME 记录？]</p><p><strong>什么是 Cloudflared?</strong></p><p>Cloudflare 这家公司十分大方，为用户们提供了隧道服务。</p><p>Cloudflare Tunnel requires the installation of a lightweight server-side <strong>daemon</strong>, <strong>cloudflared</strong>, to connect your infrastructure to Cloudflare. </p><p><strong>cloudflared</strong> is an open source project maintained by Cloudflare.</p><p><em>daemon</em>: 守护进程，需要常驻后台的进程</p><p><em>cloudflared</em>: 我们的主人公，需要安装配置到 Nana 上.</p></blockquote><p><strong>软件下载</strong></p><ul><li><a href="https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/install-and-setup/installation">https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/install-and-setup/installation</a></li></ul><p>在这里下载对应的安装文件，然后安装.</p><p>我们直接下载二进制的文件 <code>cloudflared-linux-arm</code> 后上传到服务器.</p><p><code>sudo cp ./cloudflared-linux-arm /usr/local/bin/cloudflared</code> </p><p>// 添加到 Path</p><p><code>sudo chmod +x /usr/local/bin/cloudflared</code> // 切换执行权限</p><p><code>cloudflared -v</code> // 验证是否安装成功</p><p><strong>配置穿透</strong></p><blockquote><p>Before you start:</p><p>​    向 Cloudflare 添加一个域名并确认其已被接管</p></blockquote><ul><li>验证身份：<code>cloudflared tunnel login</code>，复制链接到浏览器，登录后选择域名，完成认证.</li><li>创建隧道：<code>cloudflared tunnel create &lt;NAME&gt;</code>，随机生成隧道的 UUID. </li><li>编辑配置文件 <code>sudo vi ~/.cloudflared/config.yml</code></li></ul><pre class="language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">tunnel</span><span class="token punctuation">:</span> 6ff42ae2<span class="token punctuation">-</span>765d<span class="token punctuation">-</span>4adf<span class="token punctuation">-</span>8112<span class="token punctuation">-</span>31c55c1551ef<span class="token key atrule">credentials-file</span><span class="token punctuation">:</span> /root/.cloudflared/6ff42ae2<span class="token punctuation">-</span>765d<span class="token punctuation">-</span>4adf<span class="token punctuation">-</span>8112<span class="token punctuation">-</span>31c55c1551ef.json<span class="token key atrule">ingress</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">hostname</span><span class="token punctuation">:</span> gitlab.widgetcorp.tech    <span class="token key atrule">service</span><span class="token punctuation">:</span> http<span class="token punctuation">:</span>//localhost<span class="token punctuation">:</span><span class="token number">80</span>  <span class="token punctuation">-</span> <span class="token key atrule">hostname</span><span class="token punctuation">:</span> gitlab<span class="token punctuation">-</span>ssh.widgetcorp.tech    <span class="token key atrule">service</span><span class="token punctuation">:</span> ssh<span class="token punctuation">:</span>//localhost<span class="token punctuation">:</span><span class="token number">22</span>  <span class="token punctuation">-</span> <span class="token key atrule">service</span><span class="token punctuation">:</span> http_status<span class="token punctuation">:</span><span class="token number">404</span></code></pre><ul><li>绑定隧道到域名 <code>cloudflared tunnel route dns &lt;UUID or NAME&gt; &lt;DOMAIN&gt;</code></li></ul><p><strong>设置自动启动</strong></p><p><code>sudo cloudflared service install</code></p><pre class="language-bash" data-language="bash"><code class="language-bash"> ✘ ubuntu@ubuntu  /etc/cloudflared  <span class="token function">sudo</span> <span class="token function">mv</span> ~/.cloudflared/config.yml /etc/cloudflared ubuntu@ubuntu  /etc/cloudflared  <span class="token function">sudo</span> cloudflared <span class="token function">service</span> <span class="token function">install</span><span class="token number">2021</span>-08-23T14:16:19Z INF Using Systemd<span class="token number">2021</span>-08-23T14:16:21Z INF systemctl daemon-reload</code></pre><p>设置完成.</p><p>Reference:</p><ul><li><a href="https://www.jianshu.com/p/8fdd0e3b7339">https://www.jianshu.com/p/8fdd0e3b7339</a></li><li><a href="https://blog.zapic.moe/Archives/Tutorial-176.html">https://blog.zapic.moe/Archives/Tutorial-176.html</a></li><li><a href="https://docs.pi-hole.net/guides/dns/cloudflared/#armhf-architecture-32-bit-raspberry-pi">https://docs.pi-hole.net/guides/dns/cloudflared/#armhf-architecture-32-bit-raspberry-pi</a></li></ul><h3 id="nginx-反向代理"><a href="#nginx-反向代理" class="headerlink" title="nginx 反向代理"></a>nginx 反向代理</h3><blockquote><p><strong>nginx</strong> [engine x] is an <strong>HTTP and reverse proxy server</strong>, a mail proxy server, and a generic TCP/UDP proxy server, originally written by Igor Sysoev.</p><p><strong>什么是正向代理？什么是反向代理？</strong></p><p><strong>正向代理（forward proxy）</strong>：是一个位于客户端和目标服务器之间的服务器(代理服务器)，为了从目标服务器取得内容，客户端向代理服务器发送一个请求并指定目标，然后代理服务器向目标服务器转交请求并将获得的内容返回给客户端。（例如 酸酸乳，幻影飞梭，蓝色宝灯 etc.）</p><p><strong>反向代理（reverse proxy）</strong>：是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。（例如我们本节的主角 nginx）</p><p>二者的功能不同，适用的场景也不同。</p></blockquote><p><strong>安装 Nginx</strong></p><p><code>sudo apt install nginx</code></p><p><strong>命令</strong></p><ul><li><code>nginx -s stop</code> 关闭</li><li><code>nginx -s reload</code> 重载配置</li></ul><p><strong>常用功能介绍</strong></p><ul><li>提供静态文件（根据访问路径路由）</li></ul><p>我们可以先去看一下默认配置，<code>sudo vi /etc/nginx/nginx.conf</code></p><p>先把 <code>user</code> 改成 <code>ftpuser</code>，给予文件读写权限.</p><p>可以看到，在 HTTP 节 的 <code>Virtual Host Configs</code> 中 include 了 sites-enabled 的所有文件. 也就是说，我们可以通过在这个目录中新建文件的方式，来进行站点的添加.</p><p>在文件的最后，我们可以看到一份默认站点的样子.</p><pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># Virtual Host configuration for example.com                                              </span><span class="token comment">#                                                                                         </span><span class="token comment"># You can move that to a different file under sites-available/ and symlink that           </span><span class="token comment"># to sites-enabled/ to enable it.                                                         </span><span class="token comment">#                                                                                         </span><span class="token comment">#server &#123;                                                                                 </span><span class="token comment">#       listen 80;                                                                        </span><span class="token comment">#       listen [::]:80;                                                                   </span><span class="token comment">#                                                                                         </span><span class="token comment">#       server_name example.com;                                                          </span><span class="token comment">#                                                                                         </span><span class="token comment">#       root /var/www/example.com;                                                        </span><span class="token comment">#       index index.html;</span><span class="token comment">#</span><span class="token comment">#       location / &#123;</span><span class="token comment">#               try_files $uri $uri/ =404;</span><span class="token comment">#       &#125;</span><span class="token comment">#&#125;</span></code></pre><p>改一改，写一份自己的网站配置，然后丢到 sites-enabled 里面：</p><pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># 10001-TestPage.conf</span>server <span class="token punctuation">&#123;</span>                                                                                       listen <span class="token number">10001</span><span class="token punctuation">;</span>                                                                              listen <span class="token punctuation">[</span>::<span class="token punctuation">]</span>:10001<span class="token punctuation">;</span>                                                                                                                                                                 server_name server.cc7w.cf<span class="token punctuation">;</span>                                                                                                                                                        root /home/ftpuser/10001-TestPage<span class="token punctuation">;</span>                                                      index index.html<span class="token punctuation">;</span>      location / <span class="token punctuation">&#123;</span>              try_files <span class="token variable">$uri</span> <span class="token variable">$uri</span>/ <span class="token operator">=</span><span class="token number">404</span><span class="token punctuation">;</span>      <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span></code></pre><p>Reference:</p><ul><li><a href="https://cloud.tencent.com/developer/article/1418457">https://cloud.tencent.com/developer/article/1418457</a></li><li><a href="https://nginx.org/en/docs/beginners_guide.html">https://nginx.org/en/docs/beginners_guide.html</a></li><li><a href="https://tengine.taobao.org/nginx_docs/cn/docs/http/request_processing.html">https://tengine.taobao.org/nginx_docs/cn/docs/http/request_processing.html</a></li></ul><h3 id="Gunicorn-绿色独角兽"><a href="#Gunicorn-绿色独角兽" class="headerlink" title="Gunicorn 绿色独角兽"></a>Gunicorn 绿色独角兽</h3><blockquote><p><strong>什么是 gunicorn?</strong></p><p><strong>Gunicorn (Green Unicorn)</strong> 是 Python Web 服务器网关接口HTTP服务器。</p><p>对于 Python App 接口，我们显然是不可能像上面的静态文件一样，直接提供给用户，因此便需要 Gunicorn 进行转接.</p><p><strong>我还没有安装 pip ?</strong></p><p><code>sudo apt install python3-pip</code></p><p><strong>我的 pip 还没有连接 Tsinghua 源？</strong></p><p><code>sudo pip3 config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</code></p></blockquote><p><strong>安装</strong></p><p><code>sudo pip3 install gunicorn</code></p><p><em>在之前进行 pip 包管理时，不同用户安装的包会存储在不同的地方，有些需要修改 Path 的包也没能获得足够的权限，懒得改配置文件的 c7w 决定所有的包安装干脆全用 sudo 完事.</em></p><p><strong>以命令方式运行 Gunicorn 的参数</strong></p><ul><li><p><code>-c CONFIG_PATH</code> 以配置文件运行</p></li><li><p><code>-b BIND</code> Support <code>(HOST)</code>, <code>(HOST):(PORT)</code>, or <code>unix:(PATH)</code>(Use socket)</p></li><li><code>-w WORKERS</code> 线程数</li></ul><p><strong>通过配置文件写入</strong></p><p>这里我们选择通过配置文件运行 Gunicorn.</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> multiprocessing<span class="token keyword">import</span> uvicorndebug <span class="token operator">=</span> <span class="token boolean">False</span><span class="token builtin">reload</span> <span class="token operator">=</span> <span class="token boolean">True</span>loglevel <span class="token operator">=</span> <span class="token string">'debug'</span>bind <span class="token operator">=</span> <span class="token string">'unix:/home/ftpuser/10002-TikTokTeenBackend/gunicorn/gunicorn.sock'</span>pidfile <span class="token operator">=</span> <span class="token string">'/home/ftpuser/10002-TikTokTeenBackend/gunicorn/gunicorn.pid'</span>logfile <span class="token operator">=</span> <span class="token string">'/home/ftpuser/10002-TikTokTeenBackend/gunicorn/debug.log'</span>workers <span class="token operator">=</span> multiprocessing<span class="token punctuation">.</span>cpu_count<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">2</span> <span class="token operator">+</span> <span class="token number">1</span>worker_class <span class="token operator">=</span> <span class="token string">'uvicorn.workers.UvicornWorker'</span> <span class="token comment"># 这里一般用 sync</span><span class="token comment"># 但因为我要部署 FastAPI 项目，所以就用了 uvicorn</span></code></pre><p>注意到，我们的 bind 填写了本地 socket，这样可以让 nginx 与 gunicorn 通过本地 socket 通信.</p><p>然后在 nginx 里新建一个站点.</p><pre class="language-none"><code class="language-none">server &#123;    listen 10002;    root &#x2F;home&#x2F;ftpuser&#x2F;10002-TikTokTeenBackend&#x2F;Backend;    server_name server.cc7w.cf;    location &#x2F; &#123;        proxy_set_header x-Real-IP $remote_addr;                                                          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;        proxy_set_header Host $http_host;                                                                 proxy_pass unix:&#x2F;home&#x2F;ftpuser&#x2F;10002-TiktokTeenBackend&#x2F;gunicorn&#x2F;gunicorn.sock    &#125;                                                                                 &#125;</code></pre><h3 id="Supervisor-设置守护进程"><a href="#Supervisor-设置守护进程" class="headerlink" title="Supervisor 设置守护进程"></a>Supervisor 设置守护进程</h3><blockquote><p><strong>什么是 Supervisor?</strong></p><p>Supervisor是用Python开发的一套通用的<strong>进程管理程序</strong>，能将一个普通的命令行进程变为后台daemon（守护进程，一直在运行的进程），并监控进程状态，异常退出时能自动重启。</p></blockquote><p><strong>安装</strong></p><p><code>sudo apt install supervisor</code></p><blockquote><p>这里真的很坑，c7w 查资料发现是 Python 开发的后，没动脑子直接 <code>pip3 install supervisor</code>，发现配置无效后<code>sudo pip3 install supervisor</code>，发现还是没有默认配置后去 stackoverflow 了一波甚至补全了配置文件，但是还是没有成功运行. 仔细阅读安装方式后才发现应该用 apt 装.</p></blockquote><p><strong>配置</strong></p><pre class="language-conf" data-language="conf"><code class="language-conf">[program:10002-TikTokTeenBackend]command&#x3D;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;gunicorn -c gunicorn_config.py main:appdirectory&#x3D;&#x2F;home&#x2F;ftpuser&#x2F;10002-TikTokTeenBackend&#x2F;backenduser&#x3D;rootautorestart&#x3D;truestartretires&#x3D;3</code></pre><p>这样我们的 App 应该至少是能够跑起来了.</p><p><img src="https://i.loli.net/2021/08/24/gpVzlniWst5jYmJ.png" alt="image-20210824114820839"></p><p>Reference:</p><ul><li><p><a href="https://docs.gunicorn.org/en/stable/deploy.html">https://docs.gunicorn.org/en/stable/deploy.html</a></p></li><li><p><a href="https://mirrors.tuna.tsinghua.edu.cn/help/pypi/">https://mirrors.tuna.tsinghua.edu.cn/help/pypi/</a></p></li><li><a href="https://www.jianshu.com/p/be2b587a900e">https://www.jianshu.com/p/be2b587a900e</a></li></ul><p><em>TO BE CONTINUED</em></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/08/23/78DGrYSMnVgKkwE.jpg&quot; alt=&quot;QQ图片20210823091857&quot;&gt;&lt;/p&gt;
&lt;p&gt;(Nana 印象图 初版)&lt;/p&gt;
&lt;p&gt;速览本文：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;FTP 配置&lt;/li&gt;
&lt;li&gt;网络配置&lt;ul&gt;
&lt;li&gt;Cloudflared 内网穿透&lt;/li&gt;
&lt;li&gt;nginx 反向代理&lt;/li&gt;
&lt;li&gt;gunicorn [Python WSGI HTTP Server for UNIX]&lt;/li&gt;
&lt;li&gt;Supervisor 设置守护进程&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="技术" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="技术/Linux" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/%E6%8A%80%E6%9C%AF-Linux/"/>
    
    
    <category term="Linux" scheme="https://www.c7w.tech/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>React Redux 学习手记</title>
    <link href="https://www.c7w.tech/react-redux/"/>
    <id>https://www.c7w.tech/react-redux/</id>
    <published>2021-08-23T07:57:22.000Z</published>
    <updated>2022-01-10T11:27:41.712Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>React Redux</strong> is the official React UI bindings layer for Redux. It lets your React components read data from a Redux store, and dispatch actions to the store to update state.</p><p>(Excerpted from react-redux.js.org)</p><p>Redux 是 JavaScript 应用的<strong>状态容器</strong>，提供可预测的状态管理。</p></blockquote><p><s>想做全局变量管理，于是来学</s></p><a id="more"></a><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p><img src="http://cn.redux.js.org/assets/images/ReduxDataFlowDiagram-49fa8c3968371d9ef6f2a1486bd40a26.gif" alt="数据流更新动画"></p><h3 id="单向数据流"><a href="#单向数据流" class="headerlink" title="单向数据流"></a>单向数据流</h3><p>有以下基本流程：</p><ul><li>用 state 来描述应用程序在特定时间点的状况</li><li>基于 state 来渲染出 view</li><li>当发生某些事情时（例如用户单击按钮），state 会根据发生的事情进行更新，生成新的 state</li><li>基于新的 state 重新渲染 view</li></ul><h3 id="不可变性"><a href="#不可变性" class="headerlink" title="不可变性"></a>不可变性</h3><p>原来的对象或数组中的内容<strong>不改变</strong>，通过复制的方式先获取一份 copy，然后更新 copy 中的内容。</p><pre class="language-javascript" data-language="javascript"><code class="language-javascript"><span class="token keyword">const</span> obj <span class="token operator">=</span> <span class="token punctuation">&#123;</span>  a<span class="token operator">:</span> <span class="token punctuation">&#123;</span>    <span class="token comment">// 为了安全的更新 obj.a.c，需要先复制一份</span>    c<span class="token operator">:</span> <span class="token number">3</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>  b<span class="token operator">:</span> <span class="token number">2</span><span class="token punctuation">&#125;</span><span class="token keyword">const</span> obj2 <span class="token operator">=</span> <span class="token punctuation">&#123;</span>  <span class="token comment">// obj 的备份</span>  <span class="token operator">...</span>obj<span class="token punctuation">,</span>  <span class="token comment">// 覆盖 a</span>  a<span class="token operator">:</span> <span class="token punctuation">&#123;</span>    <span class="token comment">// obj.a 的备份</span>    <span class="token operator">...</span>obj<span class="token punctuation">.</span>a<span class="token punctuation">,</span>    <span class="token comment">// 覆盖 c</span>    c<span class="token operator">:</span> <span class="token number">42</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span class="token keyword">const</span> arr <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'a'</span><span class="token punctuation">,</span> <span class="token string">'b'</span><span class="token punctuation">]</span><span class="token comment">// 创建 arr 的备份，并把 c 拼接到最后。</span><span class="token keyword">const</span> arr2 <span class="token operator">=</span> arr<span class="token punctuation">.</span><span class="token function">concat</span><span class="token punctuation">(</span><span class="token string">'c'</span><span class="token punctuation">)</span><span class="token comment">// 或者，可以对原来的数组创建复制体</span><span class="token keyword">const</span> arr3 <span class="token operator">=</span> arr<span class="token punctuation">.</span><span class="token function">slice</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">// 修改复制体</span>arr3<span class="token punctuation">.</span><span class="token function">push</span><span class="token punctuation">(</span><span class="token string">'c'</span><span class="token punctuation">)</span></code></pre><p><em>Redux 期望所有状态更新都是使用不可变的方式</em>。</p><h3 id="state-view-action"><a href="#state-view-action" class="headerlink" title="state, view, action"></a>state, view, action</h3><ul><li>state: 储存数据的一个个“状态”</li><li>view: 当前绘制出的 UI</li><li>action: 由用户交互而触发的<strong>事件</strong>，可以引起 state 的更新，进而重新渲染 view.<ul><li><strong>action</strong> 是一个具有 <code>type</code> 字段的普通 JavaScript 对象。</li><li><code>type</code> 字段是一个字符串，给这个 action 一个描述性的名字，比如<code>&quot;todos/todoAdded&quot;</code>。我们通常把那个类型的字符串写成“域/事件名称”，其中第一部分是这个 action 所属的特征或类别，第二部分是发生的具体事情。</li><li>action 对象可以有其他字段，其中包含有关发生的事情的附加信息。按照惯例，我们将该信息放在名为 <code>payload</code> 的字段中。</li></ul></li></ul><pre class="language-javascript" data-language="javascript"><code class="language-javascript"><span class="token comment">// Example for action</span><span class="token keyword">const</span> addTodoAction <span class="token operator">=</span> <span class="token punctuation">&#123;</span>  type<span class="token operator">:</span> <span class="token string">'todos/todoAdded'</span><span class="token punctuation">,</span>  payload<span class="token operator">:</span> <span class="token string">'Buy milk'</span><span class="token punctuation">&#125;</span></code></pre><h3 id="reducer"><a href="#reducer" class="headerlink" title="reducer"></a>reducer</h3><p><strong>reducer</strong> 是一个函数，接收当前的 <code>state</code> 和一个 <code>action</code> 对象，必要时决定如何更新状态，并返回新状态。函数签名是：<code>(state, action) =&gt; newState</code>。 </p><p><strong>你可以将 reducer 视为一个事件监听器，它根据接收到的 action（事件）类型处理事件。</strong></p><p>Reducer 必需符合以下规则：</p><ul><li>仅使用 <code>state</code> 和 <code>action</code> 参数计算新的状态值</li><li>禁止直接修改 <code>state</code>。必须通过复制现有的 <code>state</code> 并对复制的值进行更改的方式来做 <em>不可变更新（immutable updates）</em>。</li><li>禁止任何异步逻辑、依赖随机值或导致其他“副作用”的代码</li></ul><pre class="language-javascript" data-language="javascript"><code class="language-javascript"><span class="token keyword">const</span> initialState <span class="token operator">=</span> <span class="token punctuation">&#123;</span> value<span class="token operator">:</span> <span class="token number">0</span> <span class="token punctuation">&#125;</span><span class="token keyword">function</span> <span class="token function">counterReducer</span><span class="token punctuation">(</span><span class="token parameter">state <span class="token operator">=</span> initialState<span class="token punctuation">,</span> action</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>  <span class="token comment">// 检查 reducer 是否关心这个 action</span>  <span class="token keyword">if</span> <span class="token punctuation">(</span>action<span class="token punctuation">.</span>type <span class="token operator">===</span> <span class="token string">'counter/increment'</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    <span class="token comment">// 如果是，复制 `state`</span>    <span class="token keyword">return</span> <span class="token punctuation">&#123;</span>      <span class="token operator">...</span>state<span class="token punctuation">,</span>      <span class="token comment">// 使用新值更新 state 副本</span>      value<span class="token operator">:</span> state<span class="token punctuation">.</span>value <span class="token operator">+</span> <span class="token number">1</span>    <span class="token punctuation">&#125;</span>  <span class="token punctuation">&#125;</span>  <span class="token comment">// 返回原来的 state 不变</span>  <span class="token keyword">return</span> state<span class="token punctuation">&#125;</span></code></pre><h3 id="store"><a href="#store" class="headerlink" title="store"></a>store</h3><p>当前 Redux 应用的状态存在于一个名为 <strong>store</strong> 的对象中。</p><p>store 是通过传入一个 reducer 来创建的，并且有一个名为 <code>getState</code> 的方法，它返回当前状态值.</p><pre class="language-javascript" data-language="javascript"><code class="language-javascript"><span class="token keyword">import</span> <span class="token punctuation">&#123;</span> configureStore <span class="token punctuation">&#125;</span> <span class="token keyword">from</span> <span class="token string">'@reduxjs/toolkit'</span><span class="token keyword">const</span> store <span class="token operator">=</span> <span class="token function">configureStore</span><span class="token punctuation">(</span><span class="token punctuation">&#123;</span> reducer<span class="token operator">:</span> counterReducer <span class="token punctuation">&#125;</span><span class="token punctuation">)</span>console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span>store<span class="token punctuation">.</span><span class="token function">getState</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">// &#123;value: 0&#125;</span></code></pre><h3 id="dispatch"><a href="#dispatch" class="headerlink" title="dispatch"></a>dispatch</h3><p><strong>dispatch 一个 action 可以形象的理解为 “触发一个事件”</strong>。</p><p>Reducer 就像事件监听器一样，当它们收到关注的 action 后，它就会更新 state 作为响应。</p><pre class="language-javascript" data-language="javascript"><code class="language-javascript">store<span class="token punctuation">.</span><span class="token function">dispatch</span><span class="token punctuation">(</span><span class="token punctuation">&#123;</span> type<span class="token operator">:</span> <span class="token string">'counter/increment'</span> <span class="token punctuation">&#125;</span><span class="token punctuation">)</span>console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span>store<span class="token punctuation">.</span><span class="token function">getState</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">// &#123;value: 1&#125;</span><span class="token keyword">const</span> <span class="token function-variable function">increment</span> <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">=></span> <span class="token punctuation">&#123;</span>  <span class="token keyword">return</span> <span class="token punctuation">&#123;</span>    type<span class="token operator">:</span> <span class="token string">'counter/increment'</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span>store<span class="token punctuation">.</span><span class="token function">dispatch</span><span class="token punctuation">(</span><span class="token function">increment</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span>store<span class="token punctuation">.</span><span class="token function">getState</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">// &#123;value: 2&#125;</span></code></pre><h3 id="辅助函数类型"><a href="#辅助函数类型" class="headerlink" title="辅助函数类型"></a>辅助函数类型</h3><h4 id="action-creator"><a href="#action-creator" class="headerlink" title="action creator"></a>action creator</h4><p><strong>action creator</strong> (<strong>[text =&gt; action]</strong>)是一个创建并返回一个 action 对象的函数。它的作用是让你不必每次都手动编写 action 对象.</p><pre class="language-javascript" data-language="javascript"><code class="language-javascript"><span class="token keyword">const</span> <span class="token function-variable function">addTodo</span> <span class="token operator">=</span> <span class="token parameter">text</span> <span class="token operator">=></span> <span class="token punctuation">&#123;</span>  <span class="token keyword">return</span> <span class="token punctuation">&#123;</span>    type<span class="token operator">:</span> <span class="token string">'todos/todoAdded'</span><span class="token punctuation">,</span>    payload<span class="token operator">:</span> text  <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span></code></pre><h4 id="selector"><a href="#selector" class="headerlink" title="selector"></a>selector</h4><p><strong>Selector</strong> 函数可以从 store 状态树中提取指定的片段.</p><pre class="language-javascript" data-language="javascript"><code class="language-javascript"><span class="token keyword">const</span> <span class="token function-variable function">selectCounterValue</span> <span class="token operator">=</span> <span class="token parameter">state</span> <span class="token operator">=></span> state<span class="token punctuation">.</span>value<span class="token keyword">const</span> currentValue <span class="token operator">=</span> <span class="token function">selectCounterValue</span><span class="token punctuation">(</span>store<span class="token punctuation">.</span><span class="token function">getState</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span>currentValue<span class="token punctuation">)</span><span class="token comment">// 2</span></code></pre><h3 id="functions-provided-by-toolkit"><a href="#functions-provided-by-toolkit" class="headerlink" title="functions provided by toolkit"></a>functions provided by toolkit</h3><h4 id="createSlice"><a href="#createSlice" class="headerlink" title="createSlice"></a>createSlice</h4><p>可以定义 初始状态, reducer 函数, slice name, 然后自动生成相应的action creator 和 action type.</p><p>内部重写了实现逻辑，可以使用<strong>可变</strong>的方式来进行状态修改。</p><pre class="language-typescript" data-language="typescript"><code class="language-typescript"><span class="token keyword">import</span> <span class="token punctuation">&#123;</span> createSlice<span class="token punctuation">,</span> PayloadAction <span class="token punctuation">&#125;</span> <span class="token keyword">from</span> <span class="token string">'@reduxjs/toolkit'</span><span class="token keyword">interface</span> <span class="token class-name">CounterState</span> <span class="token punctuation">&#123;</span>  value<span class="token operator">:</span> <span class="token builtin">number</span><span class="token punctuation">&#125;</span><span class="token keyword">const</span> initialState <span class="token operator">=</span> <span class="token punctuation">&#123;</span> value<span class="token operator">:</span> <span class="token number">0</span> <span class="token punctuation">&#125;</span> <span class="token keyword">as</span> CounterState<span class="token keyword">const</span> counterSlice <span class="token operator">=</span> <span class="token function">createSlice</span><span class="token punctuation">(</span><span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">'counter'</span><span class="token punctuation">,</span>  initialState<span class="token punctuation">,</span>  reducers<span class="token operator">:</span> <span class="token punctuation">&#123;</span>    <span class="token function">increment</span><span class="token punctuation">(</span>state<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>      state<span class="token punctuation">.</span>value<span class="token operator">++</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>    <span class="token function">decrement</span><span class="token punctuation">(</span>state<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>      state<span class="token punctuation">.</span>value<span class="token operator">--</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>    <span class="token function">incrementByAmount</span><span class="token punctuation">(</span>state<span class="token punctuation">,</span> action<span class="token operator">:</span> PayloadAction<span class="token operator">&lt;</span><span class="token builtin">number</span><span class="token operator">></span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>      state<span class="token punctuation">.</span>value <span class="token operator">+=</span> action<span class="token punctuation">.</span>payload    <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">,</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token keyword">export</span> <span class="token keyword">const</span> <span class="token punctuation">&#123;</span> increment<span class="token punctuation">,</span> decrement<span class="token punctuation">,</span> incrementByAmount <span class="token punctuation">&#125;</span> <span class="token operator">=</span> counterSlice<span class="token punctuation">.</span>actions<span class="token keyword">export</span> <span class="token keyword">default</span> counterSlice<span class="token punctuation">.</span>reducer</code></pre><p>Return values:</p><pre class="language-typescript" data-language="typescript"><code class="language-typescript"><span class="token punctuation">&#123;</span>    name <span class="token operator">:</span> <span class="token builtin">string</span><span class="token punctuation">,</span>    reducer <span class="token operator">:</span> ReducerFunction<span class="token punctuation">,</span>    actions <span class="token operator">:</span> Record<span class="token operator">&lt;</span><span class="token builtin">string</span><span class="token punctuation">,</span> ActionCreator<span class="token operator">></span><span class="token punctuation">,</span>    caseReducers<span class="token operator">:</span> Record<span class="token operator">&lt;</span><span class="token builtin">string</span><span class="token punctuation">,</span> CaseReducer<span class="token operator">></span><span class="token punctuation">&#125;</span></code></pre><h3 id="Redux-数据流"><a href="#Redux-数据流" class="headerlink" title="Redux 数据流"></a>Redux 数据流</h3><p>具体来说，对于 Redux，我们可以将这些步骤分解为更详细的内容：</p><ul><li>初始启动：<ul><li>使用最顶层的 root reducer 函数创建 Redux store</li><li>store 调用一次 root reducer，并将返回值保存为它的初始 <code>state</code></li><li>当 UI 首次渲染时，UI 组件访问 Redux store 的当前 state，并使用该数据来决定要呈现的内容。同时监听 store 的更新，以便他们可以知道 state 是否已更改。</li></ul></li><li>更新环节：<ul><li>应用程序中发生了某些事情，例如用户单击按钮</li><li>dispatch 一个 action 到 Redux store，例如 <code>dispatch(&#123;type: &#39;counter/increment&#39;&#125;)</code></li><li>store 用之前的 <code>state</code> 和当前的 <code>action</code> 再次运行 reducer 函数，并将返回值保存为新的 <code>state</code></li><li>store 通知所有订阅过的 UI，通知它们 store 发生更新</li><li>每个订阅过 store 数据的 UI 组件都会检查它们需要的 state 部分是否被更新。</li><li>发现数据被更新的每个组件都强制使用新数据重新渲染，紧接着更新网页</li></ul></li></ul><p><img src="http://cn.redux.js.org/assets/images/ReduxDataFlowDiagram-49fa8c3968371d9ef6f2a1486bd40a26.gif" alt="数据流更新动画"></p><h2 id="Practice"><a href="#Practice" class="headerlink" title="Practice"></a>Practice</h2><p><s>好，现在我已经掌握了 React Redux 的基本用法了</s></p><p><img src="https://i.loli.net/2021/08/23/EFG2eolQ7dzgkm3.png" alt="img"></p><p><strong>Store.tsx</strong></p><pre class="language-typescript" data-language="typescript"><code class="language-typescript"><span class="token keyword">import</span> <span class="token punctuation">&#123;</span>configureStore<span class="token punctuation">,</span> createSlice<span class="token punctuation">&#125;</span> <span class="token keyword">from</span> <span class="token string">'@reduxjs/toolkit'</span><span class="token punctuation">;</span><span class="token keyword">const</span> DataSlice <span class="token operator">=</span> <span class="token function">createSlice</span><span class="token punctuation">(</span><span class="token punctuation">&#123;</span>  name<span class="token operator">:</span> <span class="token string">'data'</span><span class="token punctuation">,</span>  initialState<span class="token operator">:</span> <span class="token punctuation">&#123;</span>loggedIn<span class="token operator">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span> role<span class="token operator">:</span> <span class="token string">''</span><span class="token punctuation">,</span> childProps<span class="token operator">:</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span> parentProps<span class="token operator">:</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span>  reducers<span class="token operator">:</span> <span class="token punctuation">&#123;</span>    <span class="token function-variable function">setLoggedIn</span><span class="token operator">:</span> <span class="token punctuation">(</span>state<span class="token punctuation">,</span> action<span class="token punctuation">)</span> <span class="token operator">=></span> <span class="token punctuation">&#123;</span>      state<span class="token punctuation">.</span>loggedIn <span class="token operator">=</span> action<span class="token punctuation">.</span>payload<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>    <span class="token function-variable function">setRole</span><span class="token operator">:</span> <span class="token punctuation">(</span>state<span class="token punctuation">,</span> action<span class="token punctuation">)</span> <span class="token operator">=></span> <span class="token punctuation">&#123;</span>      state<span class="token punctuation">.</span>role <span class="token operator">=</span> action<span class="token punctuation">.</span>payload<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>    <span class="token function-variable function">setChildProps</span><span class="token operator">:</span> <span class="token punctuation">(</span>state<span class="token punctuation">,</span> action<span class="token punctuation">)</span> <span class="token operator">=></span> <span class="token punctuation">&#123;</span>      state<span class="token punctuation">.</span>childProps <span class="token operator">=</span> action<span class="token punctuation">.</span>payload<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>    <span class="token function-variable function">setParentProps</span><span class="token operator">:</span> <span class="token punctuation">(</span>state<span class="token punctuation">,</span> action<span class="token punctuation">)</span> <span class="token operator">=></span> <span class="token punctuation">&#123;</span>      state<span class="token punctuation">.</span>parentProps <span class="token operator">=</span> action<span class="token punctuation">.</span>payload<span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>  <span class="token punctuation">&#125;</span><span class="token punctuation">,</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">export</span> <span class="token keyword">const</span> <span class="token punctuation">&#123;</span>setLoggedIn<span class="token punctuation">,</span> setRole<span class="token punctuation">,</span> setChildProps<span class="token punctuation">,</span> setParentProps<span class="token punctuation">&#125;</span> <span class="token operator">=</span>  DataSlice<span class="token punctuation">.</span>actions<span class="token punctuation">;</span><span class="token keyword">export</span> <span class="token keyword">default</span> <span class="token function">configureStore</span><span class="token punctuation">(</span><span class="token punctuation">&#123;</span>  reducer<span class="token operator">:</span> <span class="token punctuation">&#123;</span>data<span class="token operator">:</span> DataSlice<span class="token punctuation">.</span>reducer<span class="token punctuation">&#125;</span><span class="token punctuation">,</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p><strong>App.tsx</strong></p><pre class="language-typescript" data-language="typescript"><code class="language-typescript"><span class="token keyword">import</span> <span class="token punctuation">&#123;</span>Provider<span class="token punctuation">&#125;</span> <span class="token keyword">from</span> <span class="token string">'react-redux'</span><span class="token punctuation">;</span><span class="token keyword">import</span> Child <span class="token keyword">from</span> <span class="token string">'./UI/Child'</span><span class="token punctuation">;</span><span class="token keyword">const</span> <span class="token function-variable function">App</span> <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">=></span> <span class="token punctuation">&#123;</span>  <span class="token keyword">return</span> <span class="token punctuation">(</span>    <span class="token operator">&lt;</span>Provider store<span class="token operator">=</span><span class="token punctuation">&#123;</span>Store<span class="token punctuation">&#125;</span><span class="token operator">></span>      <span class="token operator">&lt;</span>View <span class="token operator">/</span><span class="token operator">></span>    <span class="token operator">&lt;</span><span class="token operator">/</span>Provider<span class="token operator">></span>  <span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token punctuation">;</span></code></pre><p>然后就可以在代码中调用 Store 提供的 setLoggedIn, setRole, setChildProps, setParentProps 这些 action creators 和 dispatch 方法了.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://react-redux.js.org/tutorials/quick-start">https://react-redux.js.org/tutorials/quick-start</a></li><li><a href="http://cn.redux.js.org/tutorials/essentials/part-1-overview-concepts">http://cn.redux.js.org/tutorials/essentials/part-1-overview-concepts</a></li><li><a href="https://redux-toolkit.js.org/api/createslice">https://redux-toolkit.js.org/api/createslice</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;React Redux&lt;/strong&gt; is the official React UI bindings layer for Redux. It lets your React components read data from a Redux store, and dispatch actions to the store to update state.&lt;/p&gt;
&lt;p&gt;(Excerpted from react-redux.js.org)&lt;/p&gt;
&lt;p&gt;Redux 是 JavaScript 应用的&lt;strong&gt;状态容器&lt;/strong&gt;，提供可预测的状态管理。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;s&gt;想做全局变量管理，于是来学&lt;/s&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="技术" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="技术/前端" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/%E6%8A%80%E6%9C%AF-%E5%89%8D%E7%AB%AF/"/>
    
    
    <category term="React" scheme="https://www.c7w.tech/tags/React/"/>
    
    <category term="全局数据流" scheme="https://www.c7w.tech/tags/%E5%85%A8%E5%B1%80%E6%95%B0%E6%8D%AE%E6%B5%81/"/>
    
  </entry>
  
  <entry>
    <title>与 Nana 的日常 —— 从零开始的 Linux 调教指北(0)</title>
    <link href="https://www.c7w.tech/dear-memory-with-nana-0/"/>
    <id>https://www.c7w.tech/dear-memory-with-nana-0/</id>
    <published>2021-08-23T01:15:04.000Z</published>
    <updated>2022-01-10T11:28:11.929Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2021/08/23/78DGrYSMnVgKkwE.jpg" alt="QQ图片20210823091857"></p><p>(Nana 印象图 初版)</p><p>速览本文：</p><ul><li>Nana 是什么</li><li>系统安装</li><li>Wi-Fi 配置</li><li>初次连接与工作环境配置</li><li>硬盘挂载</li><li>Bash 界面美化</li></ul><h2 id="Nana-是什么？"><a href="#Nana-是什么？" class="headerlink" title="Nana 是什么？"></a>Nana 是什么？</h2><p><s>Nana 是 c7w 幻想中的 npy</s> Nana 是目前正在宿舍中运行的一台树莓派 4B 的代号，其上装载了 Ubuntu 20.04 LTS.</p><p>本系列旨在<s>记录c7w和ta的每一天</s>将 Linux 踩坑指南和一些常用软件的配置过程记录下来，同时也方便学习与交流。</p><a id="more"></a><h2 id="系统安装"><a href="#系统安装" class="headerlink" title="系统安装"></a>系统安装</h2><p>我们采用了最为简单的方式，在官网上下载了 <a href="https://www.raspberrypi.org/software/">Raspberry Pi Imager</a> .</p><p>比起 balenaEtcher 好处是我懒得去 Tsinghua Tuna 上面下载镜像了.</p><p><img src="https://i.loli.net/2021/08/23/EOedo5ZnClFKT4J.png" alt="image-20210823093223369"></p><p>系统选择，用的比较顺手的 Ubuntu，选了 armhf 的 32bit（内存一共才2G</p><p>然后把数据写进去，就完成了系统安装操作.</p><h2 id="网络配置"><a href="#网络配置" class="headerlink" title="网络配置"></a>网络配置</h2><h3 id="Wi-Fi-Connection"><a href="#Wi-Fi-Connection" class="headerlink" title="Wi-Fi Connection"></a>Wi-Fi Connection</h3><p>传统艺能，在烧录好的 <code>system-boot</code> 盘的根目录的 network-config 配置文件内添加配置信息就好.</p><pre class="language-none"><code class="language-none"># This file contains a netplan-compatible configuration which cloud-init# will apply on first-boot. Please refer to the cloud-init documentation and# the netplan reference for full details:## https:&#x2F;&#x2F;cloudinit.readthedocs.io&#x2F;# https:&#x2F;&#x2F;netplan.io&#x2F;reference## Some additional examples are commented out belowversion: 2ethernets:  eth0:    dhcp4: true    optional: truewifis:  wlan0:    dhcp4: true    optional: true    access-points:      &quot;iLovePKU&quot;:        password: &quot;1145141919810&quot;      &quot;Peking-Visitor&quot;:        password: &quot;c7w,yyds&quot;#      myworkwifi:#        password: &quot;correct battery horse staple&quot;#      workssid:#        auth:#          key-management: eap#          method: peap#          identity: &quot;me@example.com&quot;#          password: &quot;passw0rd&quot;#          ca-certificate: &#x2F;etc&#x2F;my_ca.pem# :)</code></pre><h2 id="初次连接及配置"><a href="#初次连接及配置" class="headerlink" title="初次连接及配置"></a>初次连接及配置</h2><p>初次连接, ssh 到对应 ip 就好，用户名和密码都是 ubuntu.</p><p>然后会提示修改密码，完成操作即可.</p><p><img src="https://i.loli.net/2021/08/23/RZir5LadNWCUmcK.png" alt="image-20210823095803256"></p><h3 id="切换软件安装源"><a href="#切换软件安装源" class="headerlink" title="切换软件安装源"></a>切换软件安装源</h3><p>我们当然推出 Tsinghua Tuna.</p><p><a href="https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/">https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/</a></p><p><img src="https://i.loli.net/2021/08/23/CGsyKxtMdqZDWHT.png" alt="image-20210823095949515"></p><p>按照提示把文件内容替换即可.（这里我们不对 Linux 指令做过多介绍，默认读者已经对 Linux 系统的命令有了基本的了解与认知）</p><p>注意到我们的树莓派是 arm64 架构，所以要将链接末尾的 <em>ubuntu</em> 替换为 <strong>ubuntu-ports</strong>.</p><p>然后我们就可以 <strong>sudo apt update</strong> 获取软件包链接缓存了.(然后是愉快的系统升级，不过 c7w 输入 <code>sudo apt upgrade &amp;</code> 之后就去睡觉了)</p><h3 id="公钥私钥配置"><a href="#公钥私钥配置" class="headerlink" title="公钥私钥配置"></a>公钥私钥配置</h3><p><s>由于这台机器的所有权与使用者都仅有 c7w 一人，所以 c7w 可以放心的将自己一直在用的私钥上传.</s></p><p>想了想还是重新配一个新的吧，万一真丢了就不好办了x</p><pre class="language-none"><code class="language-none">ubuntu@ubuntu:~$ ssh-keygen -oGenerating public&#x2F;private rsa key pair.Enter file in which to save the key (&#x2F;home&#x2F;ubuntu&#x2F;.ssh&#x2F;id_rsa):Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in &#x2F;home&#x2F;ubuntu&#x2F;.ssh&#x2F;id_rsaYour public key has been saved in &#x2F;home&#x2F;ubuntu&#x2F;.ssh&#x2F;id_rsa.pub</code></pre><p>敲回车的功夫就搞定了.</p><p>然后首先让我们把公钥取回本地. <s>vim 打开复制就好</s></p><p>然后我们把本地的公钥配置到远端.</p><p>Reference: <a href="https://blog.csdn.net/alifrank/article/details/48241699">https://blog.csdn.net/alifrank/article/details/48241699</a></p><p><img src="https://i.loli.net/2021/08/23/YgqyQ1M2fBARVFo.png" alt="image-20210823100955900"></p><p><img src="https://i.loli.net/2021/08/23/HAujmDEfBeLdNtl.png" alt="image-20210823101140515"></p><p>免密登录完成.</p><h3 id="Zerotier"><a href="#Zerotier" class="headerlink" title="Zerotier"></a>Zerotier</h3><p><strong>什么是 Zerotier ?</strong></p><blockquote><h4 id="Connect-team-members-from-anywhere-in-the-world-on-any-device-ZeroTier-creates-secure-networks-between-on-premise-cloud-desktop-and-mobile-devices"><a href="#Connect-team-members-from-anywhere-in-the-world-on-any-device-ZeroTier-creates-secure-networks-between-on-premise-cloud-desktop-and-mobile-devices" class="headerlink" title="Connect team members from anywhere in the world on any device. ZeroTier creates secure networks between on-premise, cloud, desktop, and mobile devices."></a>Connect team members from anywhere in the world on any device. ZeroTier creates secure networks between on-premise, cloud, desktop, and mobile devices.</h4><p>(Excerpted from zerotier.com)</p></blockquote><p>简而言之，跟过去的 Hamachi 这种东西差不多（应该现在的人没多少人知道这是什么东西了吧），加入到同一个虚拟网络中的设备可以在<strong>虚拟局域网</strong>中互相通信，对于 c7w 这种喜欢在外面(必须在机房)进行摸大鱼<s>和 Nana 贴贴</s>的人来说基本是必需品.</p><p><strong>如何安装？</strong></p><p>不用动脑子，反正一条命令的事.</p><p><a href="https://www.zerotier.com/download/">https://www.zerotier.com/download/</a></p><p><code>curl -s https://install.zerotier.com | sudo bash</code></p><p><img src="https://i.loli.net/2021/08/23/JzxMELeOf49Hb83.png" alt="image-20210823095301599"></p><p><strong>如何配置？</strong></p><p><code>sudo zerotier-cli join &lt;Network ID&gt;</code></p><p>其中 网络ID 自行注册账号后创建.</p><p>join 后记得到控制台点击验证通过.</p><p><img src="https://i.loli.net/2021/08/23/Us7j5tvZnRHEKia.png" alt="image-20210823095614878"></p><p>这样就配置好了.</p><h2 id="硬盘挂载"><a href="#硬盘挂载" class="headerlink" title="硬盘挂载"></a>硬盘挂载</h2><p><s>为了显示我对 Nana 的诚意，</s>特别地配置了一块1T硬盘.</p><p>我们 <code>sudo fdisk -l</code> 来查看当前的所有硬盘.</p><p><img src="https://i.loli.net/2021/08/23/YAXjEk1mT43fsqp.png" alt="image-20210823102142524"></p><p>由于这块硬盘之前我在 Windows 上格式化过，是可以正确显示类型的.</p><p>然后我们创建好想要挂载的目录，比如 <code>mkdir /mnt/data</code></p><p><img src="https://i.loli.net/2021/08/23/lCO4azUJPHy8VAx.png" alt="image-20210823102540274"></p><p>这样就挂载好了，甚至还能看到我一个月前写的 this.txt</p><h3 id="开机自动挂载"><a href="#开机自动挂载" class="headerlink" title="开机自动挂载"></a>开机自动挂载</h3><p>我们没法保证服务器运行过程中不关机，<s>事实上紫荆宿舍的断电也没让我失望过</s>.</p><p>于是我们需要添加开机自动挂载功能.</p><p><img src="https://i.loli.net/2021/08/23/MbiUeAxYuaqf1mC.png" alt="image-20210823103213405"></p><p>这里由于 c7w 的硬盘是 exfat 格式，于是将 ext3 写为 exfat.</p><p>Reference:</p><ul><li><a href="https://cloud.tencent.com/developer/article/1746763">https://cloud.tencent.com/developer/article/1746763</a></li><li><a href="https://blog.51cto.com/zkxfoo/1758529">https://blog.51cto.com/zkxfoo/1758529</a></li></ul><h2 id="美化-Bash"><a href="#美化-Bash" class="headerlink" title="美化 Bash"></a>美化 Bash</h2><p>先呈现最终效果.</p><p><img src="https://i.loli.net/2021/08/23/yHqfpJ2rkX4CQmI.png" alt="image-20210823112959044"></p><p>这里我们使用了 zsh + oh my zsh 进行配置.</p><h3 id="安装与配置流程"><a href="#安装与配置流程" class="headerlink" title="安装与配置流程"></a>安装与配置流程</h3><p><strong>zsh</strong></p><blockquote><p><strong>什么是 shell ？</strong></p><p>In computing, a shell is a computer program which exposes an operating system’s services to a human user or other program. </p><p>In general, operating system shells use either a command-line interface (<strong>CLI</strong>) or graphical user interface (<strong>GUI</strong>), depending on a computer’s role and particular operation. </p><p>It is named a shell because it is the <strong>outermost layer </strong>around the operating system.</p><p>(Excerpeted from Wikipedia)</p><p>也就是说，shell 是给人用的，用来和 操作系统 交互的工具.</p><p>我们 Windows 采用的默认的 shell 便是我们的这个图形界面.</p><p><img src="https://i.loli.net/2021/08/23/Llsk9A7brdg1826.png" alt="image-20210823104413290"></p><p>我们应该能够理解，shell 是一种<strong>软件</strong>，是可以被更改配置，甚至整个换掉的<strong>软件</strong>。</p><p>比如，我们也可以用 Windows 带有的 cmd 这个 shell.</p><p><img src="https://i.loli.net/2021/08/23/sNWHr3qAKQ25uZF.png" alt="image-20210823104538789"></p><p> 只是看起来好丑（x 而且操作也不方便.</p><hr><p>如何查看自己 Linux <strong>目前</strong>用的是什么 shell ？如何查看自己都有哪些<strong>shell</strong>？</p><p><code>cat /etc/shells</code></p><p><img src="https://i.loli.net/2021/08/23/qExZXcGlYPrDgiQ.png" alt="image-20210823105242514"></p><p><code>echo $SHELL</code></p><p><img src="https://i.loli.net/2021/08/23/5gjSYGxfWO9IluC.png" alt="image-20210823105305619"></p><p>可以看到，我们现在正在使用 <code>/bin/bash</code> 这个软件作为 shell.</p><p><s>没错就是这个丑不拉几的界面</s></p><p>说了这么多，我们接下来就要换 shell，然后换界面主题了.</p></blockquote><p><strong>什么是 zsh ？</strong></p><p><a href="https://www.zsh.org/">https://www.zsh.org/</a></p><blockquote><p>Zsh is a shell designed for interactive use, although it is also a powerful scripting language.</p><p>按照介绍，是 shell 的一种.</p></blockquote><p>为什么我们必须安装 zsh ？<s>废话，因为我想装的主题只支持 zsh</s></p><p>自动补全 好用！（x</p><p><strong>如何安装zsh？</strong></p><p><img src="https://i.loli.net/2021/08/23/mlnRi8VbaQoFvMT.png" alt="image-20210823111207852"></p><ol><li>传统艺能 <code>sudo apt install zsh</code></li><li>运行 <code>zsh --version</code> 检查版本</li><li>设为默认 shell. <code>chsh -s $(which zsh)</code></li><li><p>重新登录.</p></li><li><p>提示配置 zsh 时我们暂且先 (q) quit.</p></li></ol><p>基本差不多了，我们接下来安装 <code>oh my zsh</code>.</p><p><strong>什么是 oh my zsh?</strong></p><blockquote><p>shell的类型有很多种，linux下默认的是bash，虽然bash的功能已经很强大，但对于以懒惰为美德的程序员来说，bash的提示功能不够强大，界面也不够炫，并非理想工具。</p><p>而zsh的功能极其强大，只是<strong>配置过于复杂</strong>，起初只有极客才在用。后来，有个穷极无聊的程序员可能是实在看不下去广大猿友一直只能使用单调的bash, 于是他创建了一个名为<code>oh-my-zsh</code>的开源项目…</p><p>(Excerpted from <a href="https://www.jianshu.com/p/d194d29e488c">https://www.jianshu.com/p/d194d29e488c</a>)</p></blockquote><p><img src="https://i.loli.net/2021/08/23/VuAr3vmbST8tIxX.png" alt="image-20210823111944157"></p><p>然后是安装，复制粘贴命令就好.</p><p><code>sh -c &quot;$(wget -O- https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot;</code></p><p><img src="https://i.loli.net/2021/08/23/QZVqsB5tCgdSi3c.png" alt="image-20210823112219705"></p><p>会自动帮我们写入刚刚 quit 掉的配置文件.</p><p>然后我们重新登入，oh my zsh 就已经配置好了.</p><p><img src="https://i.loli.net/2021/08/23/6OD7IHGPbTLWzqZ.png" alt="image-20210823112321731"></p><p><strong>切换主题</strong></p><p>目前这个样子吧，还是好丑.</p><p>默认的配置文件位于 <code>~/.zshrc</code> 这里. </p><p>其中 ~ 代表用户的家目录，如<code>/home/ubuntu</code></p><p><img src="https://i.loli.net/2021/08/23/xMSeVDYoQ76hNZs.png" alt="image-20210823112824697"></p><p><s>好，改他。</s>个人比较习惯使用 <code>agnoster</code>，也就是最初展示的那个效果。</p><p><img src="https://i.loli.net/2021/08/23/yHqfpJ2rkX4CQmI.png" alt="image-20210823112959044"></p><p>Reference:</p><ul><li><a href="https://github.com/ohmyzsh/ohmyzsh/wiki/Settings">https://github.com/ohmyzsh/ohmyzsh/wiki/Settings</a></li></ul><p><strong>本地适配</strong></p><p>如果读到这里也做到这里，Windows 选手可能还有一个问题。</p><p>那就是这么好看的箭头符号展示出来是乱码。</p><p>问题不大，我们还有最后一步，安装字体文件。</p><p>可以参考如下链接。</p><p>Reference:</p><ul><li><p><a href="https://blog.csdn.net/qiphon3650/article/details/109165495">https://blog.csdn.net/qiphon3650/article/details/109165495</a></p></li><li><p><a href="https://github.com/powerline/fonts">https://github.com/powerline/fonts</a></p></li></ul><p>First edited by c7w, 2021-8-23 11:33:41.</p><p>咕咕咕 qwq</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/08/23/78DGrYSMnVgKkwE.jpg&quot; alt=&quot;QQ图片20210823091857&quot;&gt;&lt;/p&gt;
&lt;p&gt;(Nana 印象图 初版)&lt;/p&gt;
&lt;p&gt;速览本文：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nana 是什么&lt;/li&gt;
&lt;li&gt;系统安装&lt;/li&gt;
&lt;li&gt;Wi-Fi 配置&lt;/li&gt;
&lt;li&gt;初次连接与工作环境配置&lt;/li&gt;
&lt;li&gt;硬盘挂载&lt;/li&gt;
&lt;li&gt;Bash 界面美化&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Nana-是什么？&quot;&gt;&lt;a href=&quot;#Nana-是什么？&quot; class=&quot;headerlink&quot; title=&quot;Nana 是什么？&quot;&gt;&lt;/a&gt;Nana 是什么？&lt;/h2&gt;&lt;p&gt;&lt;s&gt;Nana 是 c7w 幻想中的 npy&lt;/s&gt; Nana 是目前正在宿舍中运行的一台树莓派 4B 的代号，其上装载了 Ubuntu 20.04 LTS.&lt;/p&gt;
&lt;p&gt;本系列旨在&lt;s&gt;记录c7w和ta的每一天&lt;/s&gt;将 Linux 踩坑指南和一些常用软件的配置过程记录下来，同时也方便学习与交流。&lt;/p&gt;</summary>
    
    
    
    <category term="技术" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="技术/Linux" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/%E6%8A%80%E6%9C%AF-Linux/"/>
    
    
    <category term="Linux" scheme="https://www.c7w.tech/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>WSL 2 的升级与 USB 调试的配置</title>
    <link href="https://www.c7w.tech/wsl-update-config/"/>
    <id>https://www.c7w.tech/wsl-update-config/</id>
    <published>2021-08-14T12:07:30.000Z</published>
    <updated>2022-01-10T11:27:20.209Z</updated>
    
    <content type="html"><![CDATA[<p><strong>什么是 WSL?</strong></p><p>即 <strong>Windows Subsystem for Linux</strong> 的缩写.</p><blockquote><p><strong>Windows Subsystem for Linux (WSL)</strong> is a compatibility layer for running Linux binary executables natively on Windows 10… (Excerpted from Wikipedia)</p></blockquote><p>也就是说，WSL 是安装在 Windows 系统上的 Linux 虚拟系统。</p><p>在 OOP 课程中我们将会初步接触到该系统的安装与使用。</p><p>本篇主要收集了 WSL 升级的方法和对于 Android 开发调试中的 USB 调试的实现过程。</p><p>此外，一旦完成了 WSL 的升级，我们还可以进行 Docker 部署等等进阶操作…</p><a id="more"></a><h2 id="WSL-2-的安装"><a href="#WSL-2-的安装" class="headerlink" title="WSL 2 的安装"></a>WSL 2 的安装</h2><p>WSL 的安装过程我们这里略去.</p><p>(1) 开启支持 WSL 2 的可选组件</p><pre class="language-bash" data-language="bash"><code class="language-bash">dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestartdism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart</code></pre><p>(2) 安装 Kernel update package</p><p>Download at: <a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10#step-4---download-the-linux-kernel-update-package">https://docs.microsoft.com/en-us/windows/wsl/install-win10#step-4---download-the-linux-kernel-update-package</a></p><p>(3) 输入命令 进行转换</p><pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># List all the distributions</span>wsl --list<span class="token comment"># Set version to WSL 2</span>wsl --set-version <span class="token operator">&lt;</span>Distro<span class="token operator">></span> <span class="token number">2</span><span class="token comment"># Set default version to 2</span>wsl --set-default-version <span class="token number">2</span><span class="token comment"># List all the distributions with verbose information, namely their versions</span>wsl --list -v</code></pre><p>等了好久，大概两关方舟吧，就转换好了.</p><blockquote><p>[Interlude] 【插曲】</p><p>第一次运行时中间有个报错：<em>WSL2 请启用虚拟机平台 Windows 功能并确保在 BIOS 中启用虚拟化</em>.</p><p>进行了一次面向CSDN的问题解决，发现问题在于原来安装安卓虚拟机（<s>方舟挂机专用引擎</s>）的时候对某项设置进行了调整.</p><p><a href="https://blog.csdn.net/weixin_43271225/article/details/115698940">https://blog.csdn.net/weixin_43271225/article/details/115698940</a></p><p>对应的命令执行过之后，重新 <code>set-version</code> 就可以了.</p><p>但是，但是，这样做的问题在于，方舟挂机引擎打不开了.</p><p>市面上大部分安卓模拟器的通用版本和 Hyper V 还是不兼容的.</p><p>于是又进行了一次面向 Google 的问题解决，最终选用 Bluestacks 国际版提供的 HyperV 版本.</p><p><s>开发个 APP 不能老婆不养了啊对不对</s></p><p><s>昨天修了一个晚上 老婆材料挂机引擎还是没修好 先继续做App开发吧 大不了做完关掉HyperV（划掉</s></p><p>进度：最后改用了 Mumu 黄.</p></blockquote><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://dowww.spencerwoo.com/1-preparations/1-0-intro.html">https://dowww.spencerwoo.com/1-preparations/1-0-intro.html</a></li><li><a href="https://dowww.spencerwoo.com/1-preparations/1-1-installation.html#wsl-2-%E7%9A%84%E5%AE%89%E8%A3%85">https://dowww.spencerwoo.com/1-preparations/1-1-installation.html#wsl-2-%E7%9A%84%E5%AE%89%E8%A3%85</a></li><li><a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10#step-2---check-requirements-for-running-wsl-2">https://docs.microsoft.com/en-us/windows/wsl/install-win10#step-2---check-requirements-for-running-wsl-2</a></li><li><a href="https://developer.android.com/studio/command-line/adb">https://developer.android.com/studio/command-line/adb</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;什么是 WSL?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;即 &lt;strong&gt;Windows Subsystem for Linux&lt;/strong&gt; 的缩写.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Windows Subsystem for Linux (WSL)&lt;/strong&gt; is a compatibility layer for running Linux binary executables natively on Windows 10… (Excerpted from Wikipedia)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;也就是说，WSL 是安装在 Windows 系统上的 Linux 虚拟系统。&lt;/p&gt;
&lt;p&gt;在 OOP 课程中我们将会初步接触到该系统的安装与使用。&lt;/p&gt;
&lt;p&gt;本篇主要收集了 WSL 升级的方法和对于 Android 开发调试中的 USB 调试的实现过程。&lt;/p&gt;
&lt;p&gt;此外，一旦完成了 WSL 的升级，我们还可以进行 Docker 部署等等进阶操作…&lt;/p&gt;</summary>
    
    
    
    <category term="技术" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="技术/Linux" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/%E6%8A%80%E6%9C%AF-Linux/"/>
    
    
    <category term="WSL" scheme="https://www.c7w.tech/tags/WSL/"/>
    
  </entry>
  
  <entry>
    <title>React-Native Android App 开发手记</title>
    <link href="https://www.c7w.tech/react-native-record/"/>
    <id>https://www.c7w.tech/react-native-record/</id>
    <published>2021-08-14T03:32:48.000Z</published>
    <updated>2022-01-10T11:27:59.014Z</updated>
    
    <content type="html"><![CDATA[<p>是这样的，简而言之，有口大锅，让我背着。</p><p><s>在小学期两周速成 Android App 开发什么的，这合理嘛.</s></p><p>这恒河里.</p><p>于是这篇文章就完全是记录性质的，可能最后会根据 咕的情况 &amp;&amp; 是否便于整理成学习笔记 这两个因素决定要不要再水一篇指北。</p><p>咕咕咕。</p><a id="more"></a><h1 id="8-13"><a href="#8-13" class="headerlink" title="8.13"></a>8.13</h1><ul><li>下午 23：00</li></ul><p>开会，定App方案。第二天七夕节，大晚上还定方案。离谱（</p><p>实践视频改稿，等风扇转到晚上 03:00，终于能入睡，咕咕咕，啥都没干</p><h1 id="8-14"><a href="#8-14" class="headerlink" title="8.14"></a>8.14</h1><ul><li>凌晨 10:00</li></ul><p>装模作样地打开了<a href="https://reactnative.dev/">https://reactnative.dev/</a>.</p><p><s>明明后端更好写吧 为什么要在前端耗着</s></p><p>看了配置说明 感觉好麻烦x</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><strong>什么是 React Native？</strong></p><p><code>React Native</code>是使用 React 来构建 Android 和 iOS 应用的开源社区框架。也就是说，我们使用 Javascript 来实现应用的主要界面和功能。</p><p><strong>Views</strong>: Just like ReactElement，或是 Web 开发中的一个<code>&lt;div&gt;</code>块，有层级结构</p><p><strong>核心组件</strong>(Core Components)与<strong>本地组件</strong>(Native Components)</p><p><img src="https://i.loli.net/2021/08/14/RSQOvdVLxl7fpt9.png" alt="image-20210814120050939"></p><p>如何理解？核心组件在两个平台上都能正常显示，本地组件只能显示在特定平台上。</p><ul><li>中午 16:40</li></ul><p>不行 不能咕咕咕了 干活干活</p><p>今天要不把开发环境给配置好 c7w 就是爬爬怪</p><h2 id="Setting-up-the-development-environment-开发环境配置"><a href="#Setting-up-the-development-environment-开发环境配置" class="headerlink" title="Setting up the development environment 开发环境配置"></a>Setting up the development environment 开发环境配置</h2><p>这里我们介绍 <code>Development OS=Linux headless(WSL), TargetOS=Android</code> 的环境配置方法。</p><h3 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h3><ul><li>Node (12+)</li><li>JDK (8+)</li></ul><p>首先可以使用 <code>java -version</code> 查看是否已成功安装 java.</p><p>查看结果后，如果没有成功安装，我们可以运行 <code>apt install</code></p><pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt</span> <span class="token function">install</span> openjdk-11-jre-headless</code></pre><p>然后我们再运行查看版本:</p><pre class="language-bash" data-language="bash"><code class="language-bash">c7w@cc7w  /mnt/e/Project  java -versionopenjdk version <span class="token string">"11.0.11"</span> <span class="token number">2021</span>-04-20OpenJDK Runtime Environment <span class="token punctuation">(</span>build <span class="token number">11.0</span>.11+9-Ubuntu-0ubuntu2.20.04<span class="token punctuation">)</span>OpenJDK <span class="token number">64</span>-Bit Server VM <span class="token punctuation">(</span>build <span class="token number">11.0</span>.11+9-Ubuntu-0ubuntu2.20.04, mixed mode, sharing<span class="token punctuation">)</span></code></pre><ul><li>Android SDK</li></ul><blockquote><p>什么是 SDK？</p><p>A software development kit (SDK) is a collection of software development tools in one installable package. They facilitate the creation of applications by having a compiler, debugger and perhaps a software framework. They are normally specific to a hardware platform and operating system combination. (Excerpted from Wikipedia)</p></blockquote><p>由于我们要开发 Android App，所以显然我们是需要 Android SDK的.</p><p>SDK的安装参考了这里.</p><p><a href="https://gist.github.com/steveclarke/d988d89e8cdf51a8a5766d69ecb07e7b">https://gist.github.com/steveclarke/d988d89e8cdf51a8a5766d69ecb07e7b</a></p><h2 id="创建新应用"><a href="#创建新应用" class="headerlink" title="创建新应用"></a>创建新应用</h2><p>新建文件夹，然后运行<code>npx react-native init AwesomeProject</code>. 简单易用.</p><p>要想使用 Typescript，我们可以运行<code>npx react-native init AwesomeTSProject --template react-native-template-typescript</code>.</p><p>然后我们就能打开喜闻乐见的编辑器.</p><p><img src="https://i.loli.net/2021/08/14/SbQTErNlz3qky5G.png" alt="image-20210814170452919"></p><p>然后什么都不管先运行了一波 <code>yarn android</code>.</p><p>然后项目会帮我们安装 gradle.</p><p>然后我们需要设置 在<code>local.properties</code> 中 <code>sdk.dir=/home/c7w/Android</code></p><p>然后这时再 <code>yarn android</code> 便可以成功. 但由于没有emulation env 我们仍没有完成调试的配置，<s>所以我们直接快进到打包发布</s></p><h2 id="打包发布"><a href="#打包发布" class="headerlink" title="打包发布"></a>打包发布</h2><p><a href="https://reactnative.cn/docs/signed-apk-androidhttps://reactnative.cn/docs/signed-apk-android">https://reactnative.cn/docs/signed-apk-androidhttps://reactnative.cn/docs/signed-apk-android</a></p><p><s>饿饿 饭饭</s></p><ul><li>下午 19:00</li></ul><h3 id="初次配置：设置签名"><a href="#初次配置：设置签名" class="headerlink" title="初次配置：设置签名"></a>初次配置：设置签名</h3><pre class="language-bash" data-language="bash"><code class="language-bash">keytool -genkeypair -v -storetype PKCS12 -keystore my-release-key.keystore -alias my-key-alias -keyalg RSA -keysize <span class="token number">2048</span> -validity <span class="token number">1000</span></code></pre><p>这条命令会要求你输入密钥库（keystore）和对应密钥的密码，然后设置一些发行相关的信息。最后它会生成一个叫做<code>my-release-key.keystore</code>的密钥库文件。</p><p>在运行上面这条语句之后，密钥库里应该已经生成了一个单独的密钥，有效期为 1000 天。—alias 参数后面的别名是你将来为应用签名时所需要用到的，所以记得记录这个别名。</p><p>然后，我们需要配置 gradle 变量.</p><ol><li>把<code>my-release-key.keystore</code>文件放到工程中的<code>android/app</code>文件夹下。</li><li>编辑<code>项目目录/android/gradle.properties</code>。如果没有<code>gradle.properties</code>文件你就自己创建一个，添加如下的代码（注意把其中的<code>****</code>替换为相应密码）</li></ol><pre class="language-properties" data-language="properties"><code class="language-properties"><span class="token attr-name">MYAPP_RELEASE_STORE_FILE</span><span class="token punctuation">=</span><span class="token attr-value">my-release-key.keystore</span><span class="token attr-name">MYAPP_RELEASE_KEY_ALIAS</span><span class="token punctuation">=</span><span class="token attr-value">my-key-alias</span><span class="token attr-name">MYAPP_RELEASE_STORE_PASSWORD</span><span class="token punctuation">=</span><span class="token attr-value">*****</span><span class="token attr-name">MYAPP_RELEASE_KEY_PASSWORD</span><span class="token punctuation">=</span><span class="token attr-value">*****</span></code></pre><h3 id="生成-APK"><a href="#生成-APK" class="headerlink" title="生成 APK"></a>生成 APK</h3><blockquote><p>What is <strong>APK</strong>?</p><p>Android Package (APK) is the Android <strong>application package</strong> file format used by the Android operating system, and a number of other Android-based operating systems for distribution and installation of mobile apps, mobile games and middleware.</p><p>(Excerpted from Wikipedia)</p></blockquote><p>只需在终端中运行以下命令：</p><pre class="language-sh" data-language="sh"><code class="language-sh">cd android.&#x2F;gradlew assembleRelease</code></pre><p>生成的 APK 文件位于<code>android/app/build/outputs/apk/release/app-release.apk</code>，它已经可以用来发布了。</p><p><strong>启用 Proguard 来减少 apk 的大小</strong></p><p>Proguard 是一个 Java 字节码混淆压缩工具，它可以移除掉 React Native Java（和它的依赖库中）中没有被使用到的部分，最终有效的减少 APK 的大小。</p><p>要启用 Proguard，修改<code>android/app/build.gradle</code>文件：</p><pre class="language-gradle" data-language="gradle"><code class="language-gradle">&#x2F;** * Run Proguard to shrink the Java bytecode in release builds. *&#x2F;def enableProguardInReleaseBuilds &#x3D; true</code></pre><h2 id="碎碎念"><a href="#碎碎念" class="headerlink" title="碎碎念"></a>碎碎念</h2><blockquote><p>Dev on Windows with WSL</p><p>在 Windows 上用 WSL 优雅开发</p><p><a href="https://dowww.spencerwoo.com/">https://dowww.spencerwoo.com/</a> 看了这个Doc网站</p><p>感觉对USB调试有效.</p><p>是时候该升级成wsl2了.</p><p><a href="https://xwsoul.com/posts/199">https://xwsoul.com/posts/199</a></p><p>感觉似乎可行.</p><p>总比每次先build然后传到手机上调试好吧.</p><p>这就干.</p><p>准备水另一篇文章.</p></blockquote><h1 id="8-15"><a href="#8-15" class="headerlink" title="8.15"></a>8.15</h1><h2 id="Android-Debugging"><a href="#Android-Debugging" class="headerlink" title="Android Debugging"></a>Android Debugging</h2><h3 id="ADB-的使用"><a href="#ADB-的使用" class="headerlink" title="ADB 的使用"></a>ADB 的使用</h3><blockquote><p>What is <strong>ADB</strong>?</p><p>Android Debug Bridge (adb) is a versatile command-line tool that lets you communicate with a device. (Excerpted from developer.android.com)</p></blockquote><p>在 Windows 上下载：<a href="https://developer.android.com/studio/releases/platform-tools">https://developer.android.com/studio/releases/platform-tools</a></p><p>下载好直接就能用.</p><p>使用方法：</p><ul><li>在 Windows 上先 <code>adb devices</code> 查看可用设备列表，确保配置好 USB 调试.</li><li>在 Windows 上使用 <code>adb tcpip &lt;port&gt;</code> 为设备开启 tcpip 端口.</li><li>在 WSL 上使用 <code>abd connect &lt;Device IP&gt;:&lt;port&gt;</code> 连接该端口.</li></ul><pre class="language-bash" data-language="bash"><code class="language-bash">c7w@cc7w  ~/Android/platform-tools  ./adb devicesList of devices attached<span class="token number">183.172</span>.***.***:5556    device</code></pre><p>配置完成.</p><p>但是死活连接不上去，目测是辣鸡 Tsinghua-Secure 会吃掉连接.</p><p>从中厅把路由器拿来装好. Peking-Secure 也不行.</p><p>在 Windows 和 WSL 之间的通信总是会出问题.</p><p><s>需要把 WSL 的 IP 也纳入到 Peking-Secure 下才行，但是解决不了.</s></p><p>大恼，决定把整个开发环境从 WSL 里面搬出来，换成 Windows.</p><ul><li>下午 20:00</li></ul><p>整个 Windows 的环境配置完成了. 也成功在 Debug 专用板砖上输出了 Hello React Native.</p><ul><li>下午 23:00</li></ul><p>画了一个简单的登录界面. 睡大觉.</p><h1 id="8-16"><a href="#8-16" class="headerlink" title="8.16"></a>8.16</h1><ul><li>半夜 8:00</li></ul><p>舍友早九 不得已被迫起床，开始干活.</p><p>先从方舟挂机开始（大嘘</p><p><img src="https://i.loli.net/2021/08/16/xUu1hqgyZ5PrDjN.png" alt="image-20210816093146543"></p><p>安装 React-Native-Navigator 之后没有<code>yarn android</code> ，狂按R按了半小时，还搁那搜索为什么报错.</p><p>安装依赖后一定要重新 Build 一遍发到手机上再开 Debugger.</p><p><a href="https://reactnavigation.org/docs/navigating">https://reactnavigation.org/docs/navigating</a></p><h1 id="8-21-amp-8-22"><a href="#8-21-amp-8-22" class="headerlink" title="8.21 &amp; 8.22"></a>8.21 &amp; 8.22</h1><p>画前端。装了什么 icons navigator 之类的包。还在研究怎么用。</p><h1 id="8-23"><a href="#8-23" class="headerlink" title="8.23"></a>8.23</h1><p>做跳转。打算先从 redux 的教程写起。</p><h1 id="8-24"><a href="#8-24" class="headerlink" title="8.24"></a>8.24</h1><p>后端 deploy 好了。这下大鱼摸不成了。</p><p>写前端。把请求加进去.</p><p>被队友痛击.</p><p>前端 狗都不写.</p><p>逻辑十分不清晰. 感觉需要重构.</p><p>但是就这么一个破 App 似乎没有重构的必要.</p><p><em>TO BE CONTINUED</em></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;是这样的，简而言之，有口大锅，让我背着。&lt;/p&gt;
&lt;p&gt;&lt;s&gt;在小学期两周速成 Android App 开发什么的，这合理嘛.&lt;/s&gt;&lt;/p&gt;
&lt;p&gt;这恒河里.&lt;/p&gt;
&lt;p&gt;于是这篇文章就完全是记录性质的，可能最后会根据 咕的情况 &amp;amp;&amp;amp; 是否便于整理成学习笔记 这两个因素决定要不要再水一篇指北。&lt;/p&gt;
&lt;p&gt;咕咕咕。&lt;/p&gt;</summary>
    
    
    
    <category term="技术" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="技术/安卓开发" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/%E6%8A%80%E6%9C%AF-%E5%AE%89%E5%8D%93%E5%BC%80%E5%8F%91/"/>
    
    
    <category term="React" scheme="https://www.c7w.tech/tags/React/"/>
    
    <category term="安卓开发" scheme="https://www.c7w.tech/tags/%E5%AE%89%E5%8D%93%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
</feed>
