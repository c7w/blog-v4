<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>c7w 的博客</title>
  
  <subtitle>是一只 c7w 的博客</subtitle>
  <link href="https://www.c7w.tech/atom.xml" rel="self"/>
  
  <link href="https://www.c7w.tech/"/>
  <updated>2022-01-27T08:01:43.651Z</updated>
  <id>https://www.c7w.tech/</id>
  
  <author>
    <name>c7w</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>爬取《雨课堂》慕课字幕</title>
    <link href="https://www.c7w.tech/yuketang-caption-crawler/"/>
    <id>https://www.c7w.tech/yuketang-caption-crawler/</id>
    <published>2022-01-27T07:47:57.000Z</published>
    <updated>2022-01-27T08:01:43.651Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://s2.loli.net/2022/01/27/vkPmGVL9IZoWFiX.png" alt="image-20220127154939393"></p><a id="more"></a><p>// 这篇文章现在还很乱，只是草稿.jpeg</p><p>什么爬不爬的，ta 就在那里。</p><p>目前只是逆向找到了这个字幕存的地址。</p><h2 id="Break-on-change"><a href="#Break-on-change" class="headerlink" title="Break on change"></a>Break on change</h2><p>首先自然是取字幕所在的那个 xt-caption 元素，然后打上 Break on change.</p><blockquote><p>在Javascript调试中，我们经常会使用到断点调试。其实，在DOM结构的调试中，我们也可以使用断点方法，这就是DOM Breakpoint（DOM断点）。</p><p>具体的使用方法：</p><p>在Chrome浏览器中，打开开发者工具，先选中一个页面元素，然后点击鼠标右键，依次点击菜单中的”Break on …”——勾选“Attributes modifications”。刷新页面，当该元素的属性发生变化时，就会暂停脚本的执行，并且定位到改变发生的地方。</p><p>除了可以监视DOM元素本身的属性变化，Chrome 和 Firebug 还可以监视其子元素的变化，以及何时元素被删除。</p></blockquote><h2 id="查看调用栈"><a href="#查看调用栈" class="headerlink" title="查看调用栈"></a>查看调用栈</h2><p>然后是当 Trigger 了字幕更改 Event 之后，逐个检查这里的调用栈</p><p><img src="https://s2.loli.net/2022/01/27/e9qdIOvc2butLP3.png" alt="image-20220127155350265"></p><p>然后在这里看起来像是在可疑的发请求的地方找到了一个地址</p><p><img src="https://s2.loli.net/2022/01/27/NoOutDYEpwHPzy3.png" alt="image-20220127155631381"></p><p>然后在 HTML 里面全文检索竟然找到了一样的地址。Lucky，本来我觉得这必须要爬取HTML后把对应的JS全都爬下来才能找到每个视频对应的字幕地址TaT</p><p><img src="https://s2.loli.net/2022/01/27/HdU58pV4NkJyASC.png" alt="image-20220127155810865"></p><p>批量爬数据的脚本没写，回头用的时候再说吧。</p><p><img src="https://s2.loli.net/2022/01/27/uGwhPFHA176dXKy.png" alt="image-20220127160055541"></p><p>清华的这个 yuketang MOOC 平台和学堂在线那个平台竟然是一样的，只不过后端不一样，这简直就和 net9.org 和 stu.cs.tsinghua.edu.cn 的后台一样，写一份程序自己用一份，对外推一份。</p><p>这篇文章有时间再好好整理吧。想要爬数据的都不用点进来这篇文章，直接看 banner 头图应该就没问题了。当然要是头图看不懂的话这篇文章更是看不懂的。</p><p>以上desu。<s>不对我怎么又在摸鱼</s></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://s2.loli.net/2022/01/27/vkPmGVL9IZoWFiX.png&quot; alt=&quot;image-20220127154939393&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="技术" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/"/>
    
    <category term="技术/Python应用" scheme="https://www.c7w.tech/categories/%E6%8A%80%E6%9C%AF/%E6%8A%80%E6%9C%AF-Python%E5%BA%94%E7%94%A8/"/>
    
    
    <category term="爬虫" scheme="https://www.c7w.tech/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>《动手学深度学习》 Pytorch ver. 阅读摘录 Part C</title>
    <link href="https://www.c7w.tech/dive-into-dl-pytorch-C/"/>
    <id>https://www.c7w.tech/dive-into-dl-pytorch-C/</id>
    <published>2022-01-27T07:00:32.000Z</published>
    <updated>2022-01-27T12:20:39.182Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/cover.png" alt=""></p><ul><li>《动手学深度学习》原书地址：<a href="https://github.com/d2l-ai/d2l-zh">https://github.com/d2l-ai/d2l-zh</a></li><li>《动手学深度学习》(Pytorch ver.)：<a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/">https://tangshusen.me/Dive-into-DL-PyTorch/#/</a></li></ul><p>知识架构：</p><p><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/book-org.svg" alt="封面"></p><p>本文的主要作用是在阅读过程中做一些摘录。对于「机器学习」领域， c7w 虽然曾尝试从各个领域入门，也尝试训过一些模型，但是还是缺少系统性、结构性的学习。希望阅读本书能带来更多的收获吧。</p><p>与前面的一些笔记相比，本文更加侧重于「实践」。也就是说切实地提升自己的代码能力。</p><p>Part C 包含：</p><ul><li>§ 7 优化算法<ul><li>优化与深度学习，优化存在的挑战</li><li>梯度下降（略）</li><li>Momentum, Adagrad</li><li>RMSProp, AdaDelta</li><li>Adam</li></ul></li><li>§ 8 计算性能<ul><li>多 GPU 计算</li><li>多 GPU 计算时模型的保存与加载</li></ul></li><li>§ 9 </li></ul><a id="more"></a><h2 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h2><h3 id="优化与深度学习"><a href="#优化与深度学习" class="headerlink" title="优化与深度学习"></a>优化与深度学习</h3><p>本节将讨论优化与深度学习的关系，以及优化在深度学习中的挑战。</p><p>在一个深度学习问题中，我们通常会预先定义一个损失函数。有了损失函数以后，我们就可以使用优化算法试图将其最小化。在优化中，这样的损失函数通常被称作优化问题的<strong>目标函数</strong>。依据惯例，优化算法通常只考虑最小化目标函数。其实，任何最大化问题都可以很容易地转化为最小化问题，只需令目标函数的相反数为新的目标函数即可。</p><p>优化的挑战：</p><ul><li>局部最小值</li><li>Saddle Point</li></ul><h3 id="Gradient-Descent-与-SGD"><a href="#Gradient-Descent-与-SGD" class="headerlink" title="Gradient Descent 与 SGD"></a>Gradient Descent 与 SGD</h3><p>（之前的笔记中记录已十分详细，此处略去）</p><h3 id="动量法"><a href="#动量法" class="headerlink" title="动量法"></a>动量法</h3><blockquote><p>　指数移动加权平均法，是指<strong>各数值的加权系数随时间呈指数式递减，越靠近当前时刻的数值加权系数就越大</strong>。</p><p>　指数移动加权平均较传统的平均法来说，一是不需要保存过去所有的数值；二是计算量显著减小。</p></blockquote><p>目标函数有关自变量的梯度代表了目标函数在自变量当前位置下降最快的方向。因此，梯度下降也叫作最陡下降。在每次迭代中，梯度下降根据自变量当前位置，沿着当前位置的梯度更新自变量。</p><p>然而，如果自变量的迭代方向仅仅取决于自变量当前位置，这可能会带来一些问题。</p><p>让我们考虑一个输入和输出分别为二维向量 $\boldsymbol{x} = [x_1, x_2]^\top$​ 和标量的目标函数 $f(\boldsymbol{x})=0.1x_1^2+2x_2^2$​。</p><p><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/chapter07/7.4_output1.png" alt="img"></p><p>可以看到，同一位置上，目标函数在竖直方向（$x_2$ 轴方向）比在水平方向（$x_1$ 轴方向）的斜率的绝对值更大。因此，给定学习率，梯度下降迭代自变量时会使自变量在竖直方向比在水平方向移动幅度更大。那么，我们需要一个较小的学习率从而避免自变量在竖直方向上越过目标函数最优解。然而，这会造成自变量在水平方向上朝最优解移动变慢。</p><p>但如果我们试着将学习率调得稍大一点，此时自变量在竖直方向不断越过最优解并逐渐发散。</p><p>动量法的提出是为了解决梯度下降的上述问题。设时间步 $t$​ 的自变量为 $\boldsymbol{x}_t$​，学习率为 $\eta_t$​，对应梯度为 $\boldsymbol  g_t$​。<br>在时间步 $0$​，动量法创建速度变量 $\boldsymbol{v}_0$​，并将其元素初始化成 0。在时间步 $t&gt;0$​，动量法对每次迭代的步骤做如下修改：</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{v}_t &\leftarrow \gamma \boldsymbol{v}_{t-1} + \eta_t \boldsymbol{g}_t, \\\boldsymbol{x}_t &\leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{v}_t,\end{aligned}</script><p>其中，动量超参数$\gamma$满足$0 \leq \gamma &lt; 1$。当$\gamma=0$时，动量法等价于小批量随机梯度下降。</p><p><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/chapter07/7.4_output3.png" alt="img"></p><p>在动量法中，自变量在各个方向上的移动幅度不仅取决当前梯度，还取决于过去的各个梯度在各个方向上是否一致。</p><p>实现的话也是大调库：</p><pre class="language-python" data-language="python"><code class="language-python">d2l<span class="token punctuation">.</span>train_pytorch_ch7<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">,</span> <span class="token punctuation">&#123;</span><span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.004</span><span class="token punctuation">,</span> <span class="token string">'momentum'</span><span class="token punctuation">:</span> <span class="token number">0.9</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span>                    features<span class="token punctuation">,</span> labels<span class="token punctuation">)</span></code></pre><h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>AdaGrad 算法根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题。</p><p>AdaGrad 算法会使用一个小批量随机梯度 $\boldsymbol{g}_t$​​ 按元素平方的累加变量 $\boldsymbol{s}_t$​​。在时间步 0，AdaGrad 将 $\boldsymbol{s}_0$​​ 中每个元素初始化为 0。在时间步 $t$​​，首先将小批量随机梯度 $\boldsymbol{g}_t$​​ 按元素平方后累加到变量 $\boldsymbol{s}_t$​​：</p><script type="math/tex; mode=display">\boldsymbol{s}_t \leftarrow \boldsymbol{s}_{t-1} + \boldsymbol{g}_t \odot \boldsymbol{g}_t</script><p>接着，我们将目标函数自变量中每个元素的学习率通过按元素运算重新调整一下：</p><script type="math/tex; mode=display">\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \frac{\eta}{\sqrt{\boldsymbol{s}_t + \epsilon}} \odot \boldsymbol{g}_t</script><p>其中 $\eta$​ 是学习率，$\epsilon$​ 是为了维持数值稳定性而添加的常数，如 $10^{-6}$​​​。这里开方、除法和乘法的运算都是按元素运算的。这些按元素运算使得目标函数自变量中每个元素都分别拥有自己的学习率。</p><p>由于 $\boldsymbol{s}_t$ 一直在累加按元素平方的梯度，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。所以，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。</p><p>通过名称为 <code>Adagrad</code> 的优化器方法，我们便可使用 PyTorch 提供的 AdaGrad 算法来训练模型。</p><pre class="language-python" data-language="python"><code class="language-python">d2l<span class="token punctuation">.</span>train_pytorch_ch7<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adagrad<span class="token punctuation">,</span> <span class="token punctuation">&#123;</span><span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span> features<span class="token punctuation">,</span> labels<span class="token punctuation">)</span></code></pre><h3 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h3><p>针对最后我们提出的 Adagrad 算法存在的迭代后期 learning_rate 过小问题，RMSProp 算法被提出。</p><p>不同于 AdaGrad 算法里状态变量 $\boldsymbol{s}_t$ 是截至时间步 $t$ 所有小批量随机梯度 $\boldsymbol{g}_t$ 按元素平方和，RMSProp 算法将这些梯度按元素平方做指数加权移动平均。具体来说，给定超参数 $0 \leq \gamma &lt; 1$，RMSProp 算法在时间步 $t&gt;0$ 计算 $\boldsymbol{s}_t \leftarrow \gamma \boldsymbol{s}_{t-1} + (1 - \gamma) \boldsymbol{g}_t \odot \boldsymbol{g}_t$​.</p><p>和 AdaGrad 算法一样，RMSProp 算法将目标函数自变量中每个元素的学习率通过按元素运算重新调整，然后更新自变量 $\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \frac{\eta}{\sqrt{\boldsymbol{s}_t + \epsilon}} \odot \boldsymbol{g}_t $.</p><p>因为 RMSProp 算法的状态变量 $\boldsymbol{s}_t$ 是对平方项 $\boldsymbol{g}_t \odot \boldsymbol{g}_t$ 的指数加权移动平均，所以可以看作是最近 $ \dfrac 1 {1-\gamma}$ 个时间步的小批量随机梯度平方项的加权平均。如此一来，自变量每个元素的学习率在迭代过程中就不再一直降低（或不变）。</p><pre class="language-python" data-language="python"><code class="language-python">d2l<span class="token punctuation">.</span>train_pytorch_ch7<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>RMSprop<span class="token punctuation">,</span> <span class="token punctuation">&#123;</span><span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.01</span><span class="token punctuation">,</span> <span class="token string">'alpha'</span><span class="token punctuation">:</span> <span class="token number">0.9</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span>                    features<span class="token punctuation">,</span> labels<span class="token punctuation">)</span></code></pre><h3 id="AdaDelta"><a href="#AdaDelta" class="headerlink" title="AdaDelta"></a>AdaDelta</h3><p>除了 RMSProp 算法以外，另一个常用优化算法 AdaDelta 算法也针对 AdaGrad 算法在迭代后期可能较难找到有用解的问题做了改进。有意思的是，<strong>AdaDelta 算法没有学习率这一超参数</strong>。</p><p>AdaDelta 算法也像 RMSProp 算法一样，使用了小批量随机梯度 $\boldsymbol{g}_t$ 按元素平方的指数加权移动平均变量 $\boldsymbol{s}_t$。在时间步 0，它的所有元素被初始化为 0。给定超参数 $0 \leq \rho &lt; 1$（对应RMSProp算法中的 $\gamma$），在时间步 $t&gt;0$，同RMSProp算法一样计算 $ \boldsymbol{s}_t \leftarrow \rho \boldsymbol{s}_{t-1} + (1 - \rho) \boldsymbol{g}_t \odot \boldsymbol{g}_t $。​</p><p>与 RMSProp 算法不同的是，AdaDelta 算法还维护一个额外的状态变量 $\Delta\boldsymbol{x}_t$，其元素同样在时间步 0 时被初始化为 0。我们使用 $\Delta\boldsymbol{x}_{t-1}$ 来计算自变量的变化量：$ \boldsymbol{g}_t’ \leftarrow \sqrt{\frac{\Delta\boldsymbol{x}_{t-1} + \epsilon}{\boldsymbol{s}_t + \epsilon}}   \odot \boldsymbol{g}_t $​，其中 $\epsilon$ 是为了维持数值稳定性而添加的常数，如$10^{-5}$。</p><p>接着更新自变量：$ \boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{g}’_t $​。</p><p>最后，我们使用 $\Delta\boldsymbol{x}_t$ 来记录自变量变化量 $\boldsymbol{g}’_t$ 按元素平方的指数加权移动平均：$\Delta\boldsymbol{x}_t \leftarrow \rho \Delta\boldsymbol{x}_{t-1} + (1 - \rho) \boldsymbol{g}’_t \odot \boldsymbol{g}’_t$。</p><p>可以看到，如不考虑 $\epsilon$​ 的影响，<strong>AdaDelta 算法跟 RMSProp 算法的不同之处在于使用 $\sqrt{\Delta\boldsymbol{x}_{t-1}}$​ 来替代学习率 $\eta$​</strong>。</p><pre class="language-python" data-language="python"><code class="language-python">d2l<span class="token punctuation">.</span>train_pytorch_ch7<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adadelta<span class="token punctuation">,</span> <span class="token punctuation">&#123;</span><span class="token string">'rho'</span><span class="token punctuation">:</span> <span class="token number">0.9</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span> features<span class="token punctuation">,</span> labels<span class="token punctuation">)</span></code></pre><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>Adam 算法在 RMSProp 算法基础上对小批量随机梯度也做了指数加权移动平均，所以 Adam 算法可以看做是 RMSProp 算法与动量法的结合。</p><p>Adam 算法使用了动量变量 $\boldsymbol{v}_t$ 和 RMSProp 算法中小批量随机梯度按元素平方的指数加权移动平均变量 $\boldsymbol{s}_t$，并在时间步 0 将它们中每个元素初始化为 0。</p><p>给定超参数 $0 \leq \beta_1 &lt; 1$（算法作者建议设为 0.9），时间步 $t$ 的动量变量 $\boldsymbol{v}_t$ 即小批量随机梯度 $\boldsymbol{g}_t$ 的指数加权移动平均：$\boldsymbol{v}_t \leftarrow \beta_1 \boldsymbol{v}_{t-1} + (1 - \beta_1) \boldsymbol{g}_t $。​</p><p>和 RMSProp 算法中一样，给定超参数 $0 \leq \beta_2 &lt; 1$（算法作者建议设为0.999），<br>将小批量随机梯度按元素平方后的项 $\boldsymbol{g}_t \odot \boldsymbol{g}_t$ 做指数加权移动平均得到 $\boldsymbol{s}_t$：$\boldsymbol{s}_t \leftarrow \beta_2 \boldsymbol{s}_{t-1} + (1 - \beta_2) \boldsymbol{g}_t \odot \boldsymbol{g}_t$</p><ul><li>偏差修正</li></ul><p>由于我们将 $\boldsymbol{v}_0$ 和 $\boldsymbol{s}_0$ 中的元素都初始化为 0，在时间步 $t$ 我们得到 $\boldsymbol{v}_t =  (1-\beta_1) \sum_{i=1}^t \beta_1^{t-i} \boldsymbol{g}_i$。</p><p>将过去各时间步小批量随机梯度的权值相加，得到 $(1-\beta_1) \sum_{i=1}^t \beta_1^{t-i} = 1 - \beta_1^t$。</p><p>需要注意的是，当 $t$ 较小时，过去各时间步小批量随机梯度权值之和会较小。例如，当 $\beta_1 = 0.9$ 时，$\boldsymbol{v}_1 = 0.1\boldsymbol{g}_1$。</p><p>为了消除这样的影响，对于任意时间步 $t$，我们可以将 $\boldsymbol{v}_t$ 再除以 $1 - \beta_1^t$​，从而使过去各时间步小批量随机梯度权值之和为 1。这也叫作偏差修正。</p><p>在 Adam 算法中，我们对变量 $\boldsymbol{v}_t$ 和 $\boldsymbol{s}_t$ 均作偏差修正：</p><script type="math/tex; mode=display">\hat{\boldsymbol{v}}_t \leftarrow \frac{\boldsymbol{v}_t}{1 - \beta_1^t}, \\ \hat{\boldsymbol{s}}_t \leftarrow \frac{\boldsymbol{s}_t}{1 - \beta_2^t}.</script><p>接下来，Adam 算法使用以上偏差修正后的变量 $\hat{\boldsymbol{v}}_t$ 和 $\hat{\boldsymbol{s}}_t$，将模型参数中每个元素的学习率通过按元素运算重新调整：$\boldsymbol{g}_t’ \leftarrow \frac{\eta \hat{\boldsymbol{v}}_t}{\sqrt{\hat{\boldsymbol{s}}_t} + \epsilon}$。​​其中 $\eta$ ​是学习率，$\epsilon$ ​是为了维持数值稳定性而添加的常数，如 $10^{-8}$​。</p><p>和 AdaGrad 算法、RMSProp 算法以及 AdaDelta 算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。最后，使用 $\boldsymbol{g}_t’$ ​迭代自变量：$\boldsymbol{x}_t \leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{g}_t’$。</p><pre class="language-python" data-language="python"><code class="language-python">d2l<span class="token punctuation">.</span>train_pytorch_ch7<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">,</span> <span class="token punctuation">&#123;</span><span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.01</span><span class="token punctuation">&#125;</span><span class="token punctuation">,</span> features<span class="token punctuation">,</span> labels<span class="token punctuation">)</span></code></pre><h2 id="计算性能：多-GPU-计算"><a href="#计算性能：多-GPU-计算" class="headerlink" title="计算性能：多 GPU 计算"></a>计算性能：多 GPU 计算</h2><ul><li>多 GPU 计算</li></ul><p>先定义一个模型：</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torchnet <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span> <span class="token comment"># Linear(in_features=10, out_features=1, bias=True)</span></code></pre><p>要想使用 PyTorch 进行多 GPU 计算，最简单的方法是直接用 <code>torch.nn.DataParallel</code> 将模型wrap一下即可：<br><pre class="language-python" data-language="python"><code class="language-python">net <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>DataParallel<span class="token punctuation">(</span>net<span class="token punctuation">)</span>net</code></pre><br>输出：<br><pre class="language-none"><code class="language-none">DataParallel(  (module): Linear(in_features&#x3D;10, out_features&#x3D;1, bias&#x3D;True))</code></pre><br>这时，默认所有存在的 GPU 都会被使用。</p><p>如果我们机子中有很多 GPU (例如上面显示我们有 4 张显卡，但是只有第 0、3 块还剩下一点点显存)，但我们只想使用 0、3 号显卡，那么我们可以用参数 <code>device_ids</code> 指定即可：<code>torch.nn.DataParallel(net, device_ids=[0, 3])</code>。</p><ul><li>多 GPU 模型的保存与加载</li></ul><p>按之前的方法，被 <code>DataParallel</code> 包围的模型保存时正常，但加载时会出问题。</p><p>正确的方法是保存的时候只保存 <code>net.module</code>:</p><pre class="language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>net<span class="token punctuation">.</span>module<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"./8.4_model.pt"</span><span class="token punctuation">)</span>new_net<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"./8.4_model.pt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 加载成功</span></code></pre><p>或者先将 <code>new_net</code> 用 <code>DataParallel</code> 包括以下再用上面报错的方法进行模型加载:<br><pre class="language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>net<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"./8.4_model.pt"</span><span class="token punctuation">)</span>new_net <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>new_net <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>DataParallel<span class="token punctuation">(</span>new_net<span class="token punctuation">)</span>new_net<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"./8.4_model.pt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 加载成功</span></code></pre></p><h2 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h2><h3 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h3><p>Word2vec 是 Google 于 2013 年推出的开源的获取词向量 word2vec 的工具包。它包括了一组用于 word embedding 的模型，这些模型通常都是用浅层（两层）神经网络训练词向量。</p><p>Word2vec 的模型以大规模语料库作为输入，然后生成一个向量空间（通常为几百维）。词典中的每个词都对应了向量空间中的一个独一的向量，而且<strong>语料库中拥有共同上下文的词映射到向量空间中的距离会更近</strong>。</p><p>本节参考 <a href="https://www.zybuluo.com/Dounm/note/591752，梳理其发展历程及原理。">https://www.zybuluo.com/Dounm/note/591752，梳理其发展历程及原理。</a></p><ul><li><strong>神经概率语言模型</strong></li></ul><p><img src="https://raw.githubusercontent.com/Dounm/TheFarmOfDounm/master/resources/images/word2vec/1.png" alt="1"></p><ul><li>训练样本为 <code>(Context(w), w)</code>，其中 $w \in Corpus$, $Context(w)$ 为其前面的 $n-1$ 个词</li><li>$X_w$ 为直接将 $Context(w)$​ 收尾顺次相接得到的 $(n-1) \cdot m$ 长度的向量，其中 $m$ 为词向量长度</li><li>$Z_w = \tanh (WX_w + p), \, y_w = Uz_w + q$.</li></ul><p>于是在对 $y_w$ Softmax 归一化之后，$y_w$ 在对应维度的分量就是对应词的预测概率。</p><p>这个模型存在的问题：计算量太大。假设 $n \sim  5, m \sim 10^2, |Z_w| \sim 10^2, |y_w| = | \Sigma| \sim 10^5$，那么<strong>隐层和输出层之间的矩阵运算</strong>，以及<strong>输出层的 Softmax 运算</strong>会大大增加模型的计算量。</p><ul><li><strong>Word2vec 对网格结构的优化</strong></li></ul><p><img src="https://raw.githubusercontent.com/Dounm/TheFarmOfDounm/master/resources/images/word2vec/2.png" alt="7"></p><p>网格结构删除了隐藏层，而且 Projection Layer 也由“拼接”变成了“求和”，因此 Projection Layer 的结点数为词向量对应维数；输出层规模仍然是词典中词的个数。</p><p>但是，对于神经网络而言，我们的输入层的参数是中各个词的词向量，那么这个词向量是怎么来的呢？</p><p>事实上，我们在给定 $x$ 计算 $y_i$ 的时候，采用的方法是 $Y = Wx$，即 $y_i = w_i^Tx$，即矩阵 $W$ 第 $i$ 行与投影层做点积。于是这个词向量就可以直接用输出层和映射层之间的参数 $W$ 的第 $i$ 行 $w_i$ 来表示。</p><p>这样一来的话，我们训练神经网络的参数，就相当于训练了每个词的词向量，也就得到了词典中每个词的词向量。</p><ul><li><strong>对 Softmax 归一化的优化</strong></li></ul><p>但是这样我们还是没有绕开 Softmax 对计算量的消耗。</p><script type="math/tex; mode=display">\begin {align}p(y_i | Context(w)) &= \dfrac {e^{y_i}}{\sum_{k=1}^{|C|} e^{y_k} } \\ &=\dfrac {e^{w_i^Tx}} { \sum_{k=1}^{|C|} e^{w_k^T x}}\end {align}</script><p>上述式子的计算瓶颈在于分母。分母需要枚举一遍词典中所有的词，而词典中的词的数目在 $10^5$ 的量级。同时，我们需要对语料库中的每个词进行训练并按照这个公式计算所有 $y_i$ 的归一化概率，而语料库中的词的数量级通常在 million 甚至 billion 量级，这样一来的话，训练复杂度就无法接受。</p><p>因此，Word2vec 提出了两种优化 Softmax 计算过程的方法，同样也对应着 Word2vec 的两种框架，即： Hieraichical Softmax 和 Negative Sampling。</p><ul><li><strong>Hierarchical Softmax</strong></li></ul><p>本框架之所以叫 Hierarchical Softmax，是因为它利用了树实现了分层的 Softmax，即用树形结构替代了输出层的结构。</p><p>这里我们直接以 CBOW（Continous Bag-of-words）模型来说明 Hierarchical Softmax 的作用方法。</p><ul><li><strong>Negative Sampling</strong></li></ul><p>除了使用上述 Hierarchical Softmax 方法之外，也可以使用 Noise Contrastive Estimation 来优化。</p><blockquote><p>NCE posits that a good model should <strong>be able to differentiate data from noise</strong> by means of <strong>logistic regression</strong>.</p></blockquote><p>Word2vec 采用的 Negative Sampling 是 NCE 的一种简化版本，目的是为了提高训练速度以及改善所得词的质量。相比于 Hierarchical Softmax，Negative Sampling 不再采用 Huffman 树，而是采用<strong>随机负采样</strong>。</p><p>考虑：</p><script type="math/tex; mode=display">\begin {align}p(y_i | Context(w)) &= \dfrac {e^{y_i}}{\sum_{k=1}^{|C|} e^{y_k} } \\ &=\dfrac {e^{w_i^Tx}} { \sum_{k=1}^{|C|} e^{w_k^T x}}\end {align}</script><p>我们要让这个值最大化，也就是说要最大化 $w_i$ 和 $x$ 的余弦相似度，最小化非 $w_i$ 与 $x$ 的余弦相似度。</p><p>我们可以将分子的 $(Context(w), w_i)$​​ 看做一个正样本，将分母的 $(Context(w), w_k)$​​ 看做负样本，这里 $k \ne i$。</p><p>问题在于，上面公式将词典里的几乎所有词都看做了负样本，因此计算分母太耗时间。所以，我们使用 Negative Sampling 的思路，每次只从词典里随机选一些词作为当前词 $w$ 的负样本（称为 $NCE(w)$​），而不是以所有的字典里的其他词作为负样本。</p><p>其实在做出随机选取负样本的动作之后，我们就已经抛弃了 Softmax 这个函数所代表的归一化的意思了。也就代表了我们已经不再关注求解<strong>语言模型</strong>的问题，而只关注求解<strong>词向量</strong>的问题。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://tangshusen.me/Dive-into-DL-PyTorch/img/cover.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;《动手学深度学习》原书地址：&lt;a href=&quot;https://github.com/d2l-ai/d2l-zh&quot;&gt;https://github.com/d2l-ai/d2l-zh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;《动手学深度学习》(Pytorch ver.)：&lt;a href=&quot;https://tangshusen.me/Dive-into-DL-PyTorch/#/&quot;&gt;https://tangshusen.me/Dive-into-DL-PyTorch/#/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;知识架构：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tangshusen.me/Dive-into-DL-PyTorch/img/book-org.svg&quot; alt=&quot;封面&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文的主要作用是在阅读过程中做一些摘录。对于「机器学习」领域， c7w 虽然曾尝试从各个领域入门，也尝试训过一些模型，但是还是缺少系统性、结构性的学习。希望阅读本书能带来更多的收获吧。&lt;/p&gt;
&lt;p&gt;与前面的一些笔记相比，本文更加侧重于「实践」。也就是说切实地提升自己的代码能力。&lt;/p&gt;
&lt;p&gt;Part C 包含：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;§ 7 优化算法&lt;ul&gt;
&lt;li&gt;优化与深度学习，优化存在的挑战&lt;/li&gt;
&lt;li&gt;梯度下降（略）&lt;/li&gt;
&lt;li&gt;Momentum, Adagrad&lt;/li&gt;
&lt;li&gt;RMSProp, AdaDelta&lt;/li&gt;
&lt;li&gt;Adam&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;§ 8 计算性能&lt;ul&gt;
&lt;li&gt;多 GPU 计算&lt;/li&gt;
&lt;li&gt;多 GPU 计算时模型的保存与加载&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;§ 9 &lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="理论" scheme="https://www.c7w.tech/categories/%E7%90%86%E8%AE%BA/"/>
    
    <category term="理论/机器学习" scheme="https://www.c7w.tech/categories/%E7%90%86%E8%AE%BA/%E7%90%86%E8%AE%BA-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://www.c7w.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>《动手学深度学习》 Pytorch ver. 阅读后练习</title>
    <link href="https://www.c7w.tech/dive-into-dl-pytorch-practice/"/>
    <id>https://www.c7w.tech/dive-into-dl-pytorch-practice/</id>
    <published>2022-01-26T13:44:56.000Z</published>
    <updated>2022-01-26T13:44:58.837Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/cover.png" alt=""></p><ul><li>《动手学深度学习》原书地址：<a href="https://github.com/d2l-ai/d2l-zh">https://github.com/d2l-ai/d2l-zh</a></li><li>《动手学深度学习》(Pytorch ver.)：<a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/">https://tangshusen.me/Dive-into-DL-PyTorch/#/</a></li></ul><p>知识架构：</p><p><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/book-org.svg" alt="封面"></p><p>本文的主要作用是在阅读过程中做一些摘录。对于「机器学习」领域， c7w 虽然曾尝试从各个领域入门，也尝试训过一些模型，但是还是缺少系统性、结构性的学习。希望阅读本书能带来更多的收获吧。</p><p>与前面的一些笔记相比，本文更加侧重于「实践」。也就是说切实地提升自己的代码能力。</p><p>本部分包含：</p><ol><li>{Unfinished} [5-5] 实现一个可以实现表情识别的 LeNet 并训练，重点在于造出一个机器学习的框架，然后评估其准确率。</li><li>{Finished} [5-11, 5-12] 实现 ResNet 和 DenseNet，注意体会怎样才能使得运算维度匹配。</li></ol><a id="more"></a><h2 id="Resnet-amp-DenseNet-5-11-5-12"><a href="#Resnet-amp-DenseNet-5-11-5-12" class="headerlink" title="Resnet &amp; DenseNet (5-11, 5-12)"></a>Resnet &amp; DenseNet (5-11, 5-12)</h2><ul><li><code>main.py</code></li></ul><p>随机生成数据，模拟 <code>batch_size = 4</code>，<code>input_channels = 3</code>, <code>pic_size = 96x96</code> 的情况，然后将其丢入实现的网络中查看运行结果，没有发生错误则说明维度对应正确。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> ResNet <span class="token keyword">import</span> ResNet<span class="token keyword">from</span> DenseNet <span class="token keyword">import</span> DenseNet<span class="token keyword">from</span> icecream <span class="token keyword">import</span> ic <span class="token keyword">as</span> <span class="token keyword">print</span>data <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">96</span><span class="token punctuation">,</span> <span class="token number">96</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># print(data)</span><span class="token comment"># net = ResNet(3)</span>net <span class="token operator">=</span> DenseNet<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span class="token comment"># print(net(data))</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span></code></pre><ul><li><code>ResNet.py</code></li></ul><p>本文件中实现了 ResNet-18.</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token comment"># Implementation of ResNet-18</span><span class="token keyword">class</span> <span class="token class-name">Residual</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># Stride: to control the height/width of the manipulating data</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>                <span class="token comment"># If in_channels != out_channels</span>        <span class="token comment"># Then use 1x1 conv layer to change channel size</span>        self<span class="token punctuation">.</span>conv3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">)</span> <span class="token keyword">if</span> in_channels <span class="token operator">!=</span> out_channels <span class="token keyword">else</span> <span class="token boolean">None</span>                self<span class="token punctuation">.</span>bn <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bn2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>bn<span class="token punctuation">(</span>y<span class="token punctuation">)</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>y<span class="token punctuation">)</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>y<span class="token punctuation">)</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>bn2<span class="token punctuation">(</span>y<span class="token punctuation">)</span>                <span class="token keyword">if</span> self<span class="token punctuation">.</span>conv3<span class="token punctuation">:</span>            x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>                <span class="token keyword">return</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x <span class="token operator">+</span> y<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">resnet_block</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> num_residuals<span class="token punctuation">,</span> first_block<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    blk <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_residuals<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">and</span> <span class="token keyword">not</span> first_block<span class="token punctuation">:</span>            blk<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Residual<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            blk<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Residual<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>blk<span class="token punctuation">)</span>    <span class="token keyword">class</span> <span class="token class-name">ResNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>start <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>residual <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>            resnet_block<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            resnet_block<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            resnet_block<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            resnet_block<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">]</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>output <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        old_shape <span class="token operator">=</span> x<span class="token punctuation">.</span>shape        x <span class="token operator">=</span> self<span class="token punctuation">.</span>start<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">for</span> rb <span class="token keyword">in</span> self<span class="token punctuation">.</span>residual<span class="token punctuation">:</span>            x <span class="token operator">=</span> rb<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>old_shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>output<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x</code></pre><pre class="language-none"><code class="language-none">ic| net: ResNet(           (start): Sequential(             (0): Conv2d(3, 64, kernel_size&#x3D;(7, 7), stride&#x3D;(2, 2), padding&#x3D;(3, 3))             (1): BatchNorm2d(64, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)             (2): ReLU()             (3): MaxPool2d(kernel_size&#x3D;3, stride&#x3D;2, padding&#x3D;1, dilation&#x3D;1, ceil_mode&#x3D;False)           )           (residual): ModuleList(             (0): Sequential(               (0): Residual(                 (conv1): Conv2d(64, 64, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 (conv2): Conv2d(64, 64, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 (bn): BatchNorm2d(64, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                 (bn2): BatchNorm2d(64, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                 (relu): ReLU()               )               (1): Residual(                 (conv1): Conv2d(64, 64, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 (conv2): Conv2d(64, 64, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 (bn): BatchNorm2d(64, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                 (bn2): BatchNorm2d(64, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                 (relu): ReLU()               )             )             (1): Sequential(               (0): Residual(                 (conv1): Conv2d(64, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(2, 2), padding&#x3D;(1, 1))                 (conv2): Conv2d(128, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 (conv3): Conv2d(64, 128, kernel_size&#x3D;(1, 1), stride&#x3D;(2, 2))                 (bn): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                 (bn2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                 (relu): ReLU()               )               (1): Residual(                 (conv1): Conv2d(128, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 (conv2): Conv2d(128, 128, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 (bn): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                 (bn2): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                 (relu): ReLU()               )             )             (2): Sequential(               (0): Residual(                 (conv1): Conv2d(128, 256, kernel_size&#x3D;(3, 3), stride&#x3D;(2, 2), padding&#x3D;(1, 1))                 (conv2): Conv2d(256, 256, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 (conv3): Conv2d(128, 256, kernel_size&#x3D;(1, 1), stride&#x3D;(2, 2))                 (bn): BatchNorm2d(256, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                 (bn2): BatchNorm2d(256, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                 (relu): ReLU()               )               (1): Residual(                 (conv1): Conv2d(256, 256, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 (conv2): Conv2d(256, 256, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 (bn): BatchNorm2d(256, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                 (bn2): BatchNorm2d(256, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                 (relu): ReLU()               )             )             (3): Sequential(               (0): Residual(                 (conv1): Conv2d(256, 512, kernel_size&#x3D;(3, 3), stride&#x3D;(2, 2), padding&#x3D;(1, 1))                 (conv2): Conv2d(512, 512, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 (conv3): Conv2d(256, 512, kernel_size&#x3D;(1, 1), stride&#x3D;(2, 2))                 (bn): BatchNorm2d(512, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                 (bn2): BatchNorm2d(512, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                 (relu): ReLU()               )               (1): Residual(                 (conv1): Conv2d(512, 512, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 (conv2): Conv2d(512, 512, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 (bn): BatchNorm2d(512, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                 (bn2): BatchNorm2d(512, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                 (relu): ReLU()               )             )           )           (output): Sequential(             (0): Flatten(start_dim&#x3D;1, end_dim&#x3D;-1)             (1): Linear(in_features&#x3D;512, out_features&#x3D;10, bias&#x3D;True)           )         )ic| net(data).shape: torch.Size([4, 10])</code></pre><ul><li><code>DenseNet.py</code></li></ul><p>本文件实现了 DenseNet.</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> enum<span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token comment"># Implementation of DenseNet</span><span class="token keyword">def</span> <span class="token function">conv_block</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>        nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">DenseBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment"># Out_channels number is the increasing rate of channels</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_convs<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        net <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_convs<span class="token punctuation">)</span><span class="token punctuation">:</span>            in_c <span class="token operator">=</span> in_channels <span class="token operator">+</span> i <span class="token operator">*</span> out_channels            net<span class="token punctuation">.</span>append<span class="token punctuation">(</span>conv_block<span class="token punctuation">(</span>in_c<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span>net<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>out_channels <span class="token operator">=</span> in_channels <span class="token operator">+</span> num_convs <span class="token operator">*</span> out_channels        <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> blk <span class="token keyword">in</span> self<span class="token punctuation">.</span>net<span class="token punctuation">:</span>            y <span class="token operator">=</span> blk<span class="token punctuation">(</span>x<span class="token punctuation">)</span>            x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> x<span class="token keyword">def</span> <span class="token function">transition_block</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>        nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>    <span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">DenseNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_channels<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>start <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>                dense_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>                num_channels<span class="token punctuation">,</span> growth_rate <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">32</span>        num_convs_in_dense_blocks <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span>                <span class="token keyword">for</span> i<span class="token punctuation">,</span> num_convs <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>num_convs_in_dense_blocks<span class="token punctuation">)</span><span class="token punctuation">:</span>            DB <span class="token operator">=</span> DenseBlock<span class="token punctuation">(</span>num_convs<span class="token punctuation">,</span> num_channels<span class="token punctuation">,</span> growth_rate<span class="token punctuation">)</span>            num_channels <span class="token operator">=</span> DB<span class="token punctuation">.</span>out_channels                        dense_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>DB<span class="token punctuation">)</span>                        <span class="token keyword">if</span> i <span class="token operator">!=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>num_convs_in_dense_blocks<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">:</span>                dense_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span> transition_block<span class="token punctuation">(</span>num_channels<span class="token punctuation">,</span> num_channels <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                num_channels <span class="token operator">=</span> num_channels <span class="token operator">//</span> <span class="token number">2</span>                self<span class="token punctuation">.</span>dense <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span>dense_list<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>output <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>num_channels<span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_channels<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>start<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>dense<span class="token punctuation">:</span>            x <span class="token operator">=</span> layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>output<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x</code></pre><pre class="language-none"><code class="language-none">ic| net: DenseNet(           (start): Sequential(             (0): Conv2d(3, 64, kernel_size&#x3D;(7, 7), stride&#x3D;(2, 2), padding&#x3D;(3, 3))             (1): BatchNorm2d(64, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)             (2): ReLU()             (3): MaxPool2d(kernel_size&#x3D;3, stride&#x3D;2, padding&#x3D;1, dilation&#x3D;1, ceil_mode&#x3D;False)           )           (dense): ModuleList(             (0): DenseBlock(               (net): ModuleList(                 (0): Sequential(                   (0): BatchNorm2d(64, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                   (1): ReLU()                   (2): Conv2d(64, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 )                 (1): Sequential(                   (0): BatchNorm2d(96, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                   (1): ReLU()                   (2): Conv2d(96, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 )                 (2): Sequential(                   (0): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                   (1): ReLU()                   (2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 )                 (3): Sequential(                   (0): BatchNorm2d(160, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                   (1): ReLU()                   (2): Conv2d(160, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 )               )             )             (1): Sequential(               (0): BatchNorm2d(192, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)               (1): ReLU()               (2): Conv2d(192, 96, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1))               (3): AvgPool2d(kernel_size&#x3D;2, stride&#x3D;2, padding&#x3D;0)             )             (2): DenseBlock(               (net): ModuleList(                 (0): Sequential(                   (0): BatchNorm2d(96, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                   (1): ReLU()                   (2): Conv2d(96, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 )                 (1): Sequential(                   (0): BatchNorm2d(128, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                   (1): ReLU()                   (2): Conv2d(128, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 )                 (2): Sequential(                   (0): BatchNorm2d(160, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                   (1): ReLU()                   (2): Conv2d(160, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 )                 (3): Sequential(                   (0): BatchNorm2d(192, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                   (1): ReLU()                   (2): Conv2d(192, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 )               )             )             (3): Sequential(               (0): BatchNorm2d(224, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)               (1): ReLU()               (2): Conv2d(224, 112, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1))               (3): AvgPool2d(kernel_size&#x3D;2, stride&#x3D;2, padding&#x3D;0)             )             (4): DenseBlock(               (net): ModuleList(                 (0): Sequential(                   (0): BatchNorm2d(112, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                   (1): ReLU()                   (2): Conv2d(112, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 )                 (1): Sequential(                   (0): BatchNorm2d(144, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                   (1): ReLU()                   (2): Conv2d(144, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 )                 (2): Sequential(                   (0): BatchNorm2d(176, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                   (1): ReLU()                   (2): Conv2d(176, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 )                 (3): Sequential(                   (0): BatchNorm2d(208, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                   (1): ReLU()                   (2): Conv2d(208, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 )               )             )             (5): Sequential(               (0): BatchNorm2d(240, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)               (1): ReLU()               (2): Conv2d(240, 120, kernel_size&#x3D;(1, 1), stride&#x3D;(1, 1))               (3): AvgPool2d(kernel_size&#x3D;2, stride&#x3D;2, padding&#x3D;0)             )             (6): DenseBlock(               (net): ModuleList(                 (0): Sequential(                   (0): BatchNorm2d(120, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                   (1): ReLU()                   (2): Conv2d(120, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 )                 (1): Sequential(                   (0): BatchNorm2d(152, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                   (1): ReLU()                   (2): Conv2d(152, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 )                 (2): Sequential(                   (0): BatchNorm2d(184, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                   (1): ReLU()                   (2): Conv2d(184, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 )                 (3): Sequential(                   (0): BatchNorm2d(216, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)                   (1): ReLU()                   (2): Conv2d(216, 32, kernel_size&#x3D;(3, 3), stride&#x3D;(1, 1), padding&#x3D;(1, 1))                 )               )             )           )           (output): Sequential(             (0): BatchNorm2d(248, eps&#x3D;1e-05, momentum&#x3D;0.1, affine&#x3D;True, track_running_stats&#x3D;True)             (1): ReLU()           )           (fc): Sequential(             (0): Flatten(start_dim&#x3D;1, end_dim&#x3D;-1)             (1): Linear(in_features&#x3D;248, out_features&#x3D;10, bias&#x3D;True)           )         )ic| net(data).shape: torch.Size([4, 10])</code></pre>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://tangshusen.me/Dive-into-DL-PyTorch/img/cover.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;《动手学深度学习》原书地址：&lt;a href=&quot;https://github.com/d2l-ai/d2l-zh&quot;&gt;https://github.com/d2l-ai/d2l-zh&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;《动手学深度学习》(Pytorch ver.)：&lt;a href=&quot;https://tangshusen.me/Dive-into-DL-PyTorch/#/&quot;&gt;https://tangshusen.me/Dive-into-DL-PyTorch/#/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;知识架构：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tangshusen.me/Dive-into-DL-PyTorch/img/book-org.svg&quot; alt=&quot;封面&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文的主要作用是在阅读过程中做一些摘录。对于「机器学习」领域， c7w 虽然曾尝试从各个领域入门，也尝试训过一些模型，但是还是缺少系统性、结构性的学习。希望阅读本书能带来更多的收获吧。&lt;/p&gt;
&lt;p&gt;与前面的一些笔记相比，本文更加侧重于「实践」。也就是说切实地提升自己的代码能力。&lt;/p&gt;
&lt;p&gt;本部分包含：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;{Unfinished} [5-5] 实现一个可以实现表情识别的 LeNet 并训练，重点在于造出一个机器学习的框架，然后评估其准确率。&lt;/li&gt;
&lt;li&gt;{Finished} [5-11, 5-12] 实现 ResNet 和 DenseNet，注意体会怎样才能使得运算维度匹配。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="理论" scheme="https://www.c7w.tech/categories/%E7%90%86%E8%AE%BA/"/>
    
    <category term="理论/机器学习" scheme="https://www.c7w.tech/categories/%E7%90%86%E8%AE%BA/%E7%90%86%E8%AE%BA-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://www.c7w.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>《动手学深度学习》 Pytorch ver. 阅读摘录 Part B</title>
    <link href="https://www.c7w.tech/dive-into-dl-pytorch-B/"/>
    <id>https://www.c7w.tech/dive-into-dl-pytorch-B/</id>
    <published>2022-01-26T04:01:54.000Z</published>
    <updated>2022-01-27T06:59:48.550Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/cover.png" alt=""></p><ul><li>《动手学深度学习》原书地址：<a href="https://github.com/d2l-ai/d2l-zh">https://github.com/d2l-ai/d2l-zh</a></li><li>《动手学深度学习》(Pytorch ver.)：<a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/">https://tangshusen.me/Dive-into-DL-PyTorch/#/</a></li></ul><p>知识架构：</p><p><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/book-org.svg" alt="封面"></p><p>本文的主要作用是在阅读过程中做一些摘录。对于「机器学习」领域， c7w 虽然曾尝试从各个领域入门，也尝试训过一些模型，但是还是缺少系统性、结构性的学习。希望阅读本书能带来更多的收获吧。</p><p>与前面的一些笔记相比，本文更加侧重于「实践」。也就是说切实地提升自己的代码能力。</p><p>Part B 包含：</p><ul><li>§ 5 CNN<ul><li>基本概念：卷积层、填充与步长、多通道、池化、批量归一化</li><li>模型的例子：LeNet、AlexNet、VGG、NiN、GoogLeNet、ResNet、DenseNet</li></ul></li><li>§ 6 RNN<ul><li>语言模型及其计算，N-gram 的概念</li><li>RNN 基本模型及其实现，字符数据集的制作</li><li>GRU, LSTM 的原理</li><li>Deep-RNN, bi-RNN</li></ul></li></ul><a id="more"></a><h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><h4 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h4><ul><li><strong>二维互相关运算</strong></li></ul><p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.1_correlation.svg" alt="img"></p><p>如图所示，输入是一个高和宽均为3的二维数组。我们将该数组的形状记为 $3 \times 3$ 或$（3，3）$。</p><p>核数组的高和宽分别为 2。该数组在卷积计算中又称卷积核或过滤器（filter）。</p><p>卷积核窗口（又称卷积窗口）的形状取决于卷积核的高和宽，即 $2 \times 2$。</p><p>图中的阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：</p><p>$0\times0+1\times1+3\times2+4\times3=19$​​​​。</p><p>在二维互相关运算中，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，按照特定的步长，依次在输入数组上滑动。</p><p>当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素。</p><ul><li><strong>从互相关运算到卷积运算</strong></li></ul><p>实际上，卷积运算与互相关运算类似。<strong>为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算</strong>。</p><ul><li><strong>Feature Map 与 Receptive Field</strong></li></ul><p>二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。</p><p>影响元素 $x$ 的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做的 $x$ 感受野（receptive field）。</p><p>以图为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。</p><p>我们将图中形状为 $2 \times 2$​​​ 的输出记为 $Y$​​，并考虑一个更深的卷积神经网络：将 $Y$ 与另一个形状为 $2 \times 2$ 的核数组做互相关运算，输出单个元素 $z$。那么，$z$ 在 $Y$ 上的 Receptive Field 为 $Y$ 的全部四个元素，在 $x$ 上的感受野包括其中全部 9 个元素。</p><p>可见，我们可以<strong>通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征</strong>。</p><h4 id="Padding-amp-Stride"><a href="#Padding-amp-Stride" class="headerlink" title="Padding &amp; Stride"></a>Padding &amp; Stride</h4><p>本节我们将介绍卷积层的两个超参数，即填充和步幅。它们可以对给定形状的输入和卷积核改变输出形状。</p><ul><li><strong>Padding</strong></li></ul><p>填充（padding）是指在输入高和宽的两侧填充元素（通常是 0 元素）。</p><p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.2_conv_pad.svg" alt="img"></p><p>图中我们在原输入高和宽的两侧分别添加了值为 0 的元素，使得输入高和宽从 3 变成了 5 ，并导致输出高和宽由 2 增加到 4。</p><p>一般来说，如果在高的两侧一共填充 $p_h$ 行，在宽的两侧一共填充 $p_w$ 列，在很多情况下，我们会设置 $p_h=k_h-1$ 和 $p_w=k_w-1$ 来使输入和输出具有相同的高和宽，其中 $k_h\times k_w$ 是卷积核窗口形状。这样会方便在构造网络时推测每个层的输出形状。</p><p>假设这里 $k_h$ 是奇数，我们会在高的两侧分别填充 $p_h/2$ 行。如果 $k_h$ 是偶数，一种可能是在输入的顶端一侧填充 $\lceil p_h/2\rceil$ 行，而在底端一侧填充 $\lfloor p_h/2\rfloor$​ 行。在宽的两侧填充同理。卷积神经网络经常使用<strong>奇数高宽的卷积核</strong> $k_h \times k_w$，如 1、3、5 和 7，所以两端上的填充个数相等。</p><p>对任意的二维数组 <code>X</code>，设它的第 <code>i</code> 行第 <code>j</code> 列的元素为 <code>X[i,j]</code>。当两端上的填充个数相等，并使输入和输出具有相同的高和宽时，我们就知道输出 <code>Y[i,j]</code> 是由输入以 <code>X[i,j]</code> 为中心的窗口同卷积核进行互相关计算得到的。</p><ul><li><strong>Stride</strong></li></ul><p>在上一节里我们介绍了二维互相关运算。卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。我们将每次滑动的行数和列数称为步幅（stride）。</p><p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.2_conv_stride.svg" alt="img"></p><p>目前我们看到的例子里，在高和宽两个方向上步幅均为1。我们也可以使用更大步幅。</p><p>图中展示了在高上步幅为 3、在宽上步幅为 2 的二维互相关运算。可以看到，输出第一列第二个元素时，卷积窗口向下滑动了 3 行，而在输出第一行第二个元素时卷积窗口向右滑动了 2 列。当卷积窗口在输入上再向右滑动 2 列时，由于输入元素无法填满窗口，无结果输出。</p><h4 id="多通道"><a href="#多通道" class="headerlink" title="多通道"></a>多通道</h4><p>前面两节里我们用到的输入和输出都是二维数组，但真实数据的维度经常更高。</p><p>例如，彩色图像在高和宽 2 个维度外还有 RGB（红、绿、蓝）3 个颜色通道。</p><p>假设彩色图像的高和宽分别是 $h$ 和 $w$（像素），那么它可以表示为一个 $3\times h\times w$ 的多维数组。</p><p>我们将大小为 3 的这一维称为通道（channel）维。</p><p>本节我们将介绍含多个输入通道或多个输出通道的卷积核。</p><ul><li><strong>多输入通道</strong></li></ul><p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.3_conv_multi_in.svg" alt="img"></p><p>当输入数据含多个通道时，我们需要构造一个输入通道数与输入数据的通道数相同的卷积核，从而能够与含多通道的输入数据做互相关运算。</p><p>含多个通道的输入数据与多输入通道的卷积核做二维互相关运算的输出：在各个通道上对输入的二维数组和卷积核的二维核数组做互相关运算，再将这些互相关运算的输出相加。</p><ul><li><strong>多输出通道</strong></li></ul><p>当输入通道有多个时，因为我们对各个通道的结果做了累加，所以不论输入通道数是多少，输出通道数总是为 1。设卷积核输入通道数和输出通道数分别为 $c_i$ 和 $c_o$，高和宽分别为 $k_h$ 和 $k_w$。</p><p>如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为 $c_i\times k_h\times k_w$ 的核数组。将它们在输出通道维上连结，卷积核的形状即 $c_o\times c_i\times k_h\times k_w$。</p><ul><li><strong>1 x 1 卷积层</strong></li></ul><p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.3_conv_1x1.svg" alt="img"></p><p>因为使用了最小窗口，$1\times 1$ 卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。实际上，$1\times 1$ 卷积的主要计算发生在通道维上。</p><p>值得注意的是，输入和输出具有相同的高和宽。输出中的每个元素来自输入中在高和宽上相同位置的元素<strong>在不同通道之间的按权重累加</strong>。</p><p>假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，<strong>那么 $1\times 1$​ 卷积层的作用与全连接层等价</strong>。</p><h4 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h4><p>设任意二维数组 <code>X</code> 的 <code>i</code>行 <code>j</code> 列的元素为 <code>X[i, j]</code>。如果我们构造的 $1 \times 2$卷积核 $[1, -1]$ 输出 <code>Y[i, j]=1</code>，那么说明输入中 <code>X[i, j]</code> 和 <code>X[i, j+1]</code> 数值不一样。这可能意味着物体边缘通过这两个元素之间。</p><p>实际图像里，我们感兴趣的物体不会总出现在固定位置：即使我们连续拍摄同一个物体也极有可能出现像素位置上的偏移。这会导致同一个边缘对应的输出可能出现在卷积输出 <code>Y</code> 中的不同位置，进而对后面的模式识别造成不便。</p><p>在本节中我们介绍池化（pooling）层，它的提出是<strong>为了缓解卷积层对位置的过度敏感性</strong>。</p><ul><li><strong>2D-MaxPooling &amp; Mean Pooling</strong></li></ul><p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.4_pooling.svg" alt="img"></p><p>同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出。</p><p>不同于卷积层里计算输入和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也分别叫做最大池化或平均池化。</p><p>让我们再次回到本节开始提到的物体边缘检测的例子。现在我们将卷积层的输出作为 $2\times 2$ 最大池化的输入。设该卷积层输入是 <code>X</code>、池化层输出为 <code>Y</code>。无论是 <code>X[i, j]</code> 和 <code>X[i, j+1]</code> 值不同，还是 <code>X[i, j+1]</code> 和 <code>X[i, j+2]</code> 不同，池化层输出均有 <code>Y[i, j]=1</code>。也就是说，使用 $2\times 2$ 最大池化层时，只要卷积层识别的模式在高和宽上移动不超过一个元素，我们依然可以将它检测出来。</p><ul><li><strong>Padding &amp; Stride</strong></li></ul><p>同卷积层一样，池化层也可以在输入的高和宽两侧的填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。我们将通过 <code>nn</code> 模块里的二维最大池化层 <code>MaxPool2d</code> 来演示池化层填充和步幅的工作机制。</p><pre class="language-python" data-language="python"><code class="language-python">X <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># C_o * C_i * K_h * K_w</span><span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span></code></pre><p>输出：<br><pre class="language-none"><code class="language-none">tensor([[[[ 0.,  1.,  2.,  3.],          [ 4.,  5.,  6.,  7.],          [ 8.,  9., 10., 11.],          [12., 13., 14., 15.]]]])</code></pre></p><p>默认情况下，<code>MaxPool2d</code> 实例里步幅和池化窗口形状相同。下面使用形状为 $(3, 3)$ 的池化窗口，默认获得形状为 $(3, 3)$ 的步幅。</p><pre class="language-python" data-language="python"><code class="language-python">pool2d <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>pool2d<span class="token punctuation">(</span>X<span class="token punctuation">)</span> </code></pre><p>输出：<br><pre class="language-none"><code class="language-none">tensor([[[[10.]]]])</code></pre></p><p>我们可以手动指定步幅和填充。</p><pre class="language-python" data-language="python"><code class="language-python">pool2d <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>pool2d<span class="token punctuation">(</span>X<span class="token punctuation">)</span></code></pre><p>输出：<br><pre class="language-none"><code class="language-none">tensor([[[[ 5.,  7.],          [13., 15.]]]])</code></pre></p><p>当然，我们也可以指定非正方形的池化窗口，并分别指定高和宽上的填充和步幅。</p><pre class="language-python" data-language="python"><code class="language-python">pool2d <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>pool2d<span class="token punctuation">(</span>X<span class="token punctuation">)</span></code></pre><p>输出：<br><pre class="language-none"><code class="language-none">tensor([[[[ 1.,  3.],          [ 9., 11.],          [13., 15.]]]])</code></pre></p><ul><li><strong>多通道</strong></li></ul><p>在处理多通道输入数据时，<strong>池化层对每个输入通道分别池化，而不是像卷积层那样将各通道的输入按通道相加</strong>。</p><p>这意味着池化层的输出通道数与输入通道数相等。下面将数组 <code>X</code> 和 <code>X+1</code> 在通道维上连结来构造通道数为 2 的输入。</p><pre class="language-python" data-language="python"><code class="language-python">X <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> X <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>X</code></pre><p>输出：<br><pre class="language-none"><code class="language-none">tensor([[[[ 0.,  1.,  2.,  3.],          [ 4.,  5.,  6.,  7.],          [ 8.,  9., 10., 11.],          [12., 13., 14., 15.]],         [[ 1.,  2.,  3.,  4.],          [ 5.,  6.,  7.,  8.],          [ 9., 10., 11., 12.],          [13., 14., 15., 16.]]]])</code></pre></p><p>池化后，我们发现输出通道数仍然是2。</p><pre class="language-python" data-language="python"><code class="language-python">pool2d <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>pool2d<span class="token punctuation">(</span>X<span class="token punctuation">)</span></code></pre><p>输出：<br><pre class="language-none"><code class="language-none">tensor([[[[ 5.,  7.],          [13., 15.]],         [[ 6.,  8.],          [14., 16.]]]])</code></pre></p><h4 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h4><p>本节我们介绍批量归一化（batch normalization）层，它能让较深的神经网络的训练变得更加容易。</p><ul><li>为什么要有 Batch Normalization?</li></ul><p>在预测回归问题里，我们对输入数据做了标准化处理：处理后的任意一个特征在数据集中所有样本上的均值为 0、标准差为 1。标准化处理输入数据使各个特征的分布相近：这往往更容易训练出有效的模型。</p><p>通常来说，数据标准化预处理对于浅层模型就足够有效了。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。但对深层神经网络来说，即使输入数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。</p><p>批量归一化的提出正是为了应对深度模型训练的挑战。在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。<strong>批量归一化和下一节将要介绍的残差网络为训练和设计深度模型提供了两类重要思路</strong>。</p><ul><li>怎么做 Batch Normalization?</li></ul><p>对全连接层和卷积层做批量归一化的方法稍有不同。下面我们将分别介绍这两种情况下的批量归一化。</p><p><strong>对 Fully Connected Layer 的 Batch Normalization</strong></p><p>我们先考虑如何对全连接层做批量归一化。通常，我们将批量归一化层置于全连接层中的仿射变换和激活函数之间。设全连接层的输入为 $\boldsymbol{u}$​​，权重参数和偏差参数分别为 $\boldsymbol{W}$​ ​和 $\boldsymbol{b}$​​，激活函数为 $\phi$​​。设批量归一化的运算符为 $\text{BN}$​​。那么，使用批量归一化的全连接层的输出为 $\phi(\text{BN}(\boldsymbol{Wu+b}))$​。</p><p>下面我们解释 $\text{BN}$​ 算符是什么。</p><p>考虑一个由 $m$​​ 个样本组成的 Mini-batch，仿射变换的输出为一个新的 Mini-batch $\mathcal{B} = {\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(m)} }$​​。它们正是批量归一化层的输入。对于小批量 $\mathcal{B}$​ ​中任意样本 $\boldsymbol{x}^{(i)} \in \mathbb{R}^d, 1 \leq i \leq m$​​，批量归一化层的输出同样是 $d$​ ​维向量$\boldsymbol{y}^{(i)} = \text{BN}(\boldsymbol{x}^{(i)})$​，并由以下几步求得。</p><script type="math/tex; mode=display">\boldsymbol{\mu}_\mathcal{B} \leftarrow \frac{1}{m}\sum_{i = 1}^{m} \boldsymbol{x}^{(i)} \\\boldsymbol{\sigma}_\mathcal{B}^2 \leftarrow \frac{1}{m} \sum_{i=1}^{m}(\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_\mathcal{B})^2 \\ \hat{\boldsymbol{x}}^{(i)} \leftarrow \frac{\boldsymbol{x}^{(i)} - \boldsymbol{\mu}_\mathcal{B}}{\sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon}}</script><p>这里 $\epsilon &gt; 0$ 是一个很小的常数，是为了保证分母大于 0。在上面标准化的基础上，批量归一化层引入了两个可以学习的模型参数，拉伸（scale）参数 $\boldsymbol{\gamma}$ 和偏移（shift）参数 $\boldsymbol{\beta}$。这两个参数和 $\boldsymbol{x}^{(i)}$ 形状相同，皆为 $d$ 维向量。它们与 $\boldsymbol{x}^{(i)}$ 分别做 Hadamard Product（符号$\odot$​）和加法计算：</p><script type="math/tex; mode=display">{\boldsymbol{y}}^{(i)} \leftarrow \boldsymbol{\gamma} \odot \hat{\boldsymbol{x}}^{(i)} + \boldsymbol{\beta}</script><p>至此，我们得到了 $\boldsymbol{x}^{(i)}$​ ​的批量归一化的输出 $\boldsymbol{y}^{(i)}$​​。值得注意的是，可学习的拉伸和偏移参数保留了不对 $\hat{\boldsymbol{x}}^{(i)}$ ​​做批量归一化的可能：此时只需学出 $\boldsymbol{\gamma} = \sqrt{\boldsymbol{\sigma}_\mathcal{B}^2 + \epsilon}$ ​​和 $\boldsymbol{\beta} = \boldsymbol{\mu}_\mathcal{B}$​​。我们可以对此这样理解：如果批量归一化无益，理论上，学出的模型可以不使用批量归一化。</p><p><strong>对 Conv. Layer 的 Batch Normalization</strong></p><p>对卷积层来说，批量归一化发生在卷积计算之后、应用激活函数之前。</p><p>如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且<strong>每个通道都拥有独立的拉伸和偏移参数，并均为标量</strong>。</p><p>设小批量中有 $m$ 个样本，在单个通道上，假设卷积计算输出的高和宽分别为 $p$ 和 $q$。我们需要对该通道中 $m \times p \times q$ 个元素同时做批量归一化。对这些元素做标准化计算时，我们使用相同的均值和方差，即该通道中 $m \times p \times q$​ 个元素的均值和方差。</p><p><strong>预测时的 Batch Normalization</strong></p><p>使用批量归一化训练时，我们可以将批量大小设得大一点，从而使批量内样本的均值和方差的计算都较为准确。将训练好的模型用于预测时，我们希望模型对于任意输入都有确定的输出。因此，<strong>单个样本的输出不应取决于批量归一化所需要的随机小批量中的均值和方差</strong>。一种常用的方法是通过移动平均<strong>估算整个训练数据集的样本均值和方差</strong>，并在预测时使用它们得到确定的输出。可见，和丢弃层一样，批量归一化层在训练模式和预测模式下的计算结果也是不一样的。</p><ul><li>实现（Simple ver.）</li></ul><p>与我们刚刚自己定义的 <code>BatchNorm</code> 类相比，Pytorch 中 <code>nn</code> 模块定义的 <code>BatchNorm1d</code> 和 <code>BatchNorm2d</code> 类使用起来更加简单，二者分别用于全连接层和卷积层，都需要指定输入的 <code>num_features</code> 参数值。下面我们用 PyTorch 实现使用批量归一化的 LeNet。</p><pre class="language-python" data-language="python"><code class="language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># in_channels, out_channels, kernel_size</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># kernel_size, stride</span>                nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                d2l<span class="token punctuation">.</span>FlattenLayer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token operator">*</span><span class="token number">4</span><span class="token operator">*</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span></code></pre><h3 id="CNN-的例子"><a href="#CNN-的例子" class="headerlink" title="CNN 的例子"></a>CNN 的例子</h3><h4 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h4><p>之前我们曾使用 MLP 对 Fashion-MNIST 数据集中的图像进行分类。每张图像高和宽均是 28 像素。我们将图像中的像素逐行展开，得到长度为 784 的向量，并输入进全连接层中。然而，这种分类方法有一定的局限性。</p><ol><li>图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。</li><li>对于大尺寸的输入图像，使用全连接层容易造成模型过大。假设输入是高和宽均为 1000 像素的彩色照片（含 3 个通道）。即使全连接层输出个数仍是 256，该层权重参数的形状是$ 3,000,000\times 256$​：它占用了大约 3 GB 的内存或显存。这带来过复杂的模型和过高的存储开销。</li></ol><p>卷积层尝试解决这两个问题。一方面，卷积层保留输入形状，使图像的像素在高和宽两个方向上的相关性均可能被有效识别；另一方面，卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。</p><p>卷积神经网络就是含卷积层的网络。本节里我们将介绍一个早期用来识别手写数字图像的卷积神经网络：LeNet。</p><p><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.5_lenet.png" alt="img"></p><p>LeNet分为卷积层块和全连接层块两个部分。下面我们分别介绍这两个模块。</p><p>卷积层块里的基本单位是<strong>卷积层后接最大池化层</strong>：</p><ul><li>卷积层用来识别图像里的空间模式，如线条和物体局部</li><li>之后的最大池化层则用来降低卷积层对位置的敏感性</li></ul><p>卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用 $5\times 5$​ 的窗口，并在输出上使用 sigmoid 激活函数。第一个卷积层输出通道数为 6，第二个卷积层输出通道数则增加到 16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层的窗口形状均为 $2\times 2$​​，且步幅为 2。由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。</p><p>卷积层块的输出形状为 (批量大小, 通道, 高, 宽)。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含 3 个全连接层。它们的输出个数分别是 120、84 和 10，其中 10 为输出的类别个数。</p><p>下面我们通过 <code>Sequential</code> 类来实现 LeNet 模型。</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LeNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>LeNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv <span class="