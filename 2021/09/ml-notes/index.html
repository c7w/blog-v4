<!DOCTYPE html>
<html lang="zh">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.jpg">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"cc7w.cf","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Notes for LeeML-Notes 正如篇名，是机器学习笔记的笔记。 主要还是记录一些重要概念和思考的过程吧。 笔记的笔记中，第一个笔记的链接：  https:&#x2F;&#x2F;datawhalechina.github.io&#x2F;leeml-notes&#x2F;#&#x2F;  对应视频的链接：  https:&#x2F;&#x2F;www.bilibili.com&#x2F;video&#x2F;BV1Ht411g7Ef">
<meta property="og:type" content="article">
<meta property="og:title" content="《机器学习笔记》笔记">
<meta property="og:url" content="https://cc7w.cf/2021/09/ml-notes/index.html">
<meta property="og:site_name" content="c7w 的破站 v2.0">
<meta property="og:description" content="Notes for LeeML-Notes 正如篇名，是机器学习笔记的笔记。 主要还是记录一些重要概念和思考的过程吧。 笔记的笔记中，第一个笔记的链接：  https:&#x2F;&#x2F;datawhalechina.github.io&#x2F;leeml-notes&#x2F;#&#x2F;  对应视频的链接：  https:&#x2F;&#x2F;www.bilibili.com&#x2F;video&#x2F;BV1Ht411g7Ef">
<meta property="og:locale">
<meta property="og:image" content="https://datawhalechina.github.io/leeml-notes/chapter1/res/chapter1-21.png">
<meta property="og:image" content="https://i.loli.net/2021/09/08/eqEIoViJlbtsuyj.png">
<meta property="og:image" content="https://i.loli.net/2021/09/08/jmIgTdBbVQr9Al5.jpg">
<meta property="og:image" content="https://i.loli.net/2021/09/08/lXDf6xgFKIP3HZs.png">
<meta property="og:image" content="https://i.loli.net/2021/09/08/B5v8uoTOKUFHEMD.png">
<meta property="og:image" content="https://i.loli.net/2021/09/08/j64LqduhrsWzeTa.png">
<meta property="og:image" content="https://i.loli.net/2021/09/08/dz9XaEv26imuFY3.png">
<meta property="og:image" content="https://i.loli.net/2021/09/08/hruapvB7snqGtPk.png">
<meta property="og:image" content="https://datawhalechina.github.io/leeml-notes/chapter6/res/chapter6-23.png">
<meta property="og:image" content="https://datawhalechina.github.io/leeml-notes/chapter10/res/chapter10-7.png">
<meta property="og:image" content="https://datawhalechina.github.io/leeml-notes/chapter10/res/chapter10-9.png">
<meta property="og:image" content="https://i.loli.net/2021/09/09/dT4isEYZSD5QGnH.png">
<meta property="og:image" content="https://datawhalechina.github.io/leeml-notes/chapter11/res/chapter11-5.png">
<meta property="og:image" content="https://datawhalechina.github.io/leeml-notes/chapter11/res/chapter11-6.png">
<meta property="og:image" content="https://i.loli.net/2021/09/09/I2isBxThM9nrNV4.png">
<meta property="og:image" content="https://datawhalechina.github.io/leeml-notes/chapter11/res/chapter11-18.png">
<meta property="og:image" content="https://datawhalechina.github.io/leeml-notes/chapter11/res/chapter11-19.png">
<meta property="og:image" content="https://datawhalechina.github.io/leeml-notes/chapter11/res/chapter11-23.png">
<meta property="og:image" content="https://datawhalechina.github.io/leeml-notes/chapter11/res/chapter11-26.png">
<meta property="og:image" content="https://i.loli.net/2021/10/02/Y3GhrSRlVWsnofc.png">
<meta property="og:image" content="https://i.loli.net/2021/10/02/eJHzUvSWnOcTtBD.png">
<meta property="og:image" content="https://i.loli.net/2021/10/02/r4fRUpxLKGBbnyc.png">
<meta property="og:image" content="https://i.loli.net/2021/10/02/FQIxTbOJPH25DvY.png">
<meta property="og:image" content="https://i.loli.net/2021/10/02/Z1V2qWHmAg354ku.png">
<meta property="og:image" content="https://i.loli.net/2021/10/03/WhAsVmULjZCRGMq.png">
<meta property="og:image" content="https://i.loli.net/2021/10/03/nWAefx6phy41tUK.png">
<meta property="og:image" content="https://i.loli.net/2021/10/03/YEya7komzgsGPM6.png">
<meta property="article:published_time" content="2021-09-07T14:33:15.000Z">
<meta property="article:modified_time" content="2021-10-02T17:15:08.228Z">
<meta property="article:author" content="c7w">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://datawhalechina.github.io/leeml-notes/chapter1/res/chapter1-21.png">

<link rel="canonical" href="https://cc7w.cf/2021/09/ml-notes/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh'
  };
</script>

  <title>《机器学习笔记》笔记 | c7w 的破站 v2.0</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">c7w 的破站 v2.0</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh">
    <link itemprop="mainEntityOfPage" href="https://cc7w.cf/2021/09/ml-notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="c7w">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="c7w 的破站 v2.0">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          《机器学习笔记》笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-07 22:33:15" itemprop="dateCreated datePublished" datetime="2021-09-07T22:33:15+08:00">2021-09-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-10-03 01:15:08" itemprop="dateModified" datetime="2021-10-03T01:15:08+08:00">2021-10-03</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><strong>Notes for LeeML-Notes</strong></p>
<p>正如篇名，是机器学习笔记的笔记。</p>
<p>主要还是记录一些重要概念和思考的过程吧。</p>
<p>笔记的笔记中，第一个笔记的链接：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/leeml-notes/#/">https://datawhalechina.github.io/leeml-notes/#/</a></li>
</ul>
<p>对应视频的链接：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Ht411g7Ef">https://www.bilibili.com/video/BV1Ht411g7Ef</a></li>
</ul>
<span id="more"></span>
<h2 id="机器学习介绍"><a href="#机器学习介绍" class="headerlink" title="机器学习介绍"></a>机器学习介绍</h2><h3 id="发展历程及基础概念"><a href="#发展历程及基础概念" class="headerlink" title="发展历程及基础概念"></a>发展历程及基础概念</h3><ul>
<li>在存在深度学习之前，通过 hand-crafted rules 来设定过滤规则</li>
<li>机器学习的过程<ul>
<li>Training<ul>
<li>Define a set of functions as <strong>Model</strong></li>
<li>Evaluate the goodness of these functions</li>
<li>Pick the best function $f^<em>$ from the <em>*Model</em></em></li>
</ul>
</li>
<li>Testing<ul>
<li>Using $f^*$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://datawhalechina.github.io/leeml-notes/chapter1/res/chapter1-21.png" alt="img"></p>
<h3 id="相关技术"><a href="#相关技术" class="headerlink" title="相关技术"></a>相关技术</h3><ul>
<li>监督学习 Supervised learning<ul>
<li>Tasks<ul>
<li>Regression<ul>
<li>The output of the target function $f$ is scalar.</li>
</ul>
</li>
<li>Classification<ul>
<li>Binary classification (Output: yes/no)</li>
<li>Multi-class classification</li>
</ul>
</li>
<li>Structured Learning<ul>
<li>The output is well-structured.</li>
</ul>
</li>
</ul>
</li>
<li>How to select function set?<ul>
<li>Non-linear model, the most famous of which is <strong>Deep Learning</strong></li>
<li>Other non-linear models, like SVM…</li>
</ul>
</li>
</ul>
</li>
<li>半监督学习 Semi-supervised Learning<ul>
<li>non-labelled data</li>
</ul>
</li>
<li>迁移学习 Transfer Learning<ul>
<li>Pictures that are not related to the topic could help…?</li>
</ul>
</li>
<li>无监督学习 Unsupervised Learning</li>
<li>强化学习 Reinforcement Learning<ul>
<li>我们没有告诉机器正确的答案是什么，机器所拥有的只有一个分数，就是他做的好还是不好</li>
</ul>
</li>
</ul>
<h2 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h2><p>找到函数$f$，使得对于任意给定特征$x$，输出数值$scalar$.</p>
<h3 id="Steps"><a href="#Steps" class="headerlink" title="Steps"></a>Steps</h3><ul>
<li>模型假设，选择模型框架（线性模型）</li>
<li>模型评估，如何判断众多模型的好坏（损失函数）</li>
<li>模型优化，如何筛选最优的模型（梯度下降）</li>
</ul>
<h3 id="可能出现的问题"><a href="#可能出现的问题" class="headerlink" title="可能出现的问题"></a>可能出现的问题</h3><ul>
<li>过拟合</li>
<li>Customize learning rate</li>
</ul>
<h3 id="步骤优化"><a href="#步骤优化" class="headerlink" title="步骤优化"></a>步骤优化</h3><ul>
<li>合并多个线性模型，使用 $\delta$ 函数</li>
<li>给予更多参数</li>
<li>正则化 Regularization<ul>
<li>$L=\sum_n (y-(b+\sum_iw_ix_i)) + \lambda\sum w_i^2$</li>
<li>使得 Loss function 更加平滑</li>
</ul>
</li>
</ul>
<h2 id="分析误差"><a href="#分析误差" class="headerlink" title="分析误差"></a>分析误差</h2><p>通过分析误差的来源，以期达到改善模型时有着手点的效果.</p>
<h3 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h3><ul>
<li>Average Error = error due to “bias” + error due to variance</li>
<li>Notation<ul>
<li>$\hat f$ := the actual function</li>
<li>$f^*$:= the best function picked from the model trained from the training data</li>
<li>$f^*$ is an $estimator$ of $\hat f$​ </li>
</ul>
</li>
</ul>
<p><img src="https://i.loli.net/2021/09/08/eqEIoViJlbtsuyj.png" alt="image-20210908141646887"></p>
<p><img src="https://i.loli.net/2021/09/08/jmIgTdBbVQr9Al5.jpg" alt="img"></p>
<ul>
<li>Conclusion<ul>
<li>Simple Model<ul>
<li>Large Bias</li>
<li>Small Variance</li>
</ul>
</li>
<li>Complex Model<ul>
<li>Small Bias</li>
<li>Large Variance</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="https://i.loli.net/2021/09/08/lXDf6xgFKIP3HZs.png" alt="image-20210908143937650"></p>
<p><strong>How to diagnose?</strong></p>
<ul>
<li>If your model cannot fit the training data, then you have large bias. (Underfitting)<ul>
<li>Redesign your model.<ul>
<li>Add more features…</li>
<li>A more complex model…</li>
</ul>
</li>
</ul>
</li>
<li>If your model can fit the training data, but has large error on the testing data, then you have the large variance. (Overfitting)<ul>
<li>More data.<ul>
<li>Effective but not always practical.</li>
</ul>
</li>
<li>Regularization. (May do harm to bias)</li>
</ul>
</li>
</ul>
<h3 id="Cross-Validation"><a href="#Cross-Validation" class="headerlink" title="Cross Validation"></a>Cross Validation</h3><ul>
<li>In each epoch, divide your training set into <strong>training set</strong> and <strong>validation set</strong>.</li>
<li>Use <strong>training set</strong> to train your model, and use <strong>validation set</strong> to pick the best one.</li>
<li>Then by this way, the average error of <strong>testing set</strong> could represent the real error when the model is applied.</li>
<li>What if the <strong>validation set</strong> has its biases? <strong>N-fold Cross Validation</strong></li>
</ul>
<p><img src="https://i.loli.net/2021/09/08/B5v8uoTOKUFHEMD.png" alt="image-20210908145850518"></p>
<ul>
<li>Firstly pick the best model using the validation approach, then train it using the whole training set.</li>
</ul>
<h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><h3 id="Tuning-learning-rates"><a href="#Tuning-learning-rates" class="headerlink" title="Tuning learning rates"></a>Tuning learning rates</h3><p>Visualize the figure of <strong>the loss</strong> and <strong>the turn of parameters updated</strong></p>
<p><img src="https://i.loli.net/2021/09/08/j64LqduhrsWzeTa.png" alt="image-20210908153835553"></p>
<h4 id="Adaptive-learning-rates"><a href="#Adaptive-learning-rates" class="headerlink" title="Adaptive learning rates"></a>Adaptive learning rates</h4><ul>
<li>Popular &amp; Simple Idea: Reduce the learning rate by some factor every few epochs.<ul>
<li>(E.g.) $\frac 1 t \ Decay$: $\eta^{(t)} = \frac {\eta^{(0)}} {\sqrt {t+1}}$</li>
<li>But tuning learning rate cannot be one-size for all parameters. That is to say, we need to <strong>give different parameters different learning rates</strong>.</li>
</ul>
</li>
<li><p>Adagrad</p>
<ul>
<li>Divide the learning rate of each parameter by the root mean square of its previous derivatives.</li>
<li>Vanilla gradient descent: $w^{(t+1)} \leftarrow w^{(t)} - \eta^{(t)}g^{(t)}$​​​, $t\ge0$.<ul>
<li>$g$​​​ is partial derivatives</li>
</ul>
</li>
<li>Adagrad: $w^{(t+1)} \leftarrow w^{(t)} - \frac {\eta^{(t)}}{\sigma^{(t)}}g^{(t)}$, $t\ge0$.<ul>
<li>$\sigma^{(t)} = \sqrt{\frac 1 {t+1} \sum_{i=0}^t[(g^{(i)})^2]}$</li>
</ul>
</li>
<li>If we use $\frac 1 t\ Decay$ and $Adagrad$ together, we could easily have:<ul>
<li>$w^{(t+1)} = w^{(t)}-\frac{\eta^{(0)}}{\sqrt {\sum_{i=0}^t[(g^{(i)})^2]}}g^{(t)}$​</li>
</ul>
</li>
</ul>
</li>
<li><p>The best step is $\frac{|一阶偏导|}{二阶偏导}$</p>
</li>
</ul>
<h3 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h3><p>随机梯度下降法. Make your training faster.</p>
<ul>
<li>每处理一个例子就更新.</li>
</ul>
<h3 id="Feature-Scaling-特征缩放"><a href="#Feature-Scaling-特征缩放" class="headerlink" title="Feature Scaling 特征缩放"></a>Feature Scaling 特征缩放</h3><p><img src="https://i.loli.net/2021/09/08/dz9XaEv26imuFY3.png" alt="image-20210908161121857"></p>
<ul>
<li>两个输入的分布的范围很不一样，建议把他们的范围缩放，使得不同输入的范围是一样的.</li>
</ul>
<p><img src="https://i.loli.net/2021/09/08/hruapvB7snqGtPk.png" alt="image-20210908161952204"></p>
<h3 id="Possible-Problems"><a href="#Possible-Problems" class="headerlink" title="Possible Problems"></a>Possible Problems</h3><p><img src="https://datawhalechina.github.io/leeml-notes/chapter6/res/chapter6-23.png" alt="img"></p>
<h2 id="Classification-概率分类模型"><a href="#Classification-概率分类模型" class="headerlink" title="Classification 概率分类模型"></a>Classification 概率分类模型</h2><h3 id="回归模型与概率模型"><a href="#回归模型与概率模型" class="headerlink" title="回归模型与概率模型"></a>回归模型与概率模型</h3><ul>
<li>回归模型有其缺陷.</li>
<li>Ideal Alternatives<ul>
<li><img src="https://datawhalechina.github.io/leeml-notes/chapter10/res/chapter10-7.png" alt="img"></li>
</ul>
</li>
</ul>
<h3 id="Generative-Model"><a href="#Generative-Model" class="headerlink" title="Generative Model"></a>Generative Model</h3><p><img src="https://datawhalechina.github.io/leeml-notes/chapter10/res/chapter10-9.png" alt="img"></p>
<ul>
<li>如何进行问题的转化?<ul>
<li>两个盒子中抽一个球，抽到的是盒子1中蓝色球的概率是多少？</li>
<li>相当于两个类别中抽一个 x，抽到的是类别1中 x 的概率是多少？</li>
<li>可以转化成，随机给出一个 x，那么它属于哪一个类别（属于<strong>概率相对比较大</strong>的类别）？<ul>
<li>If $P(C_1|x) \ge 0.5$, then output $C_1$​.</li>
<li>Else output $C_2$​.</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>Prior<ul>
<li>计算 $P(C_1), P(C_2)$: $P(C_1) = N(C_1)/N(All)$</li>
</ul>
</li>
<li>Probability from Class?<ul>
<li>我们假设 Training Data 中的数据全部是从一个 Gaussian Distribution 中 sample 出来.</li>
<li><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/zh-hans/%E5%A4%9A%E5%85%83%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83">https://zh.wikipedia.org/zh-hans/%E5%A4%9A%E5%85%83%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83</a></li>
<li>$f_{\mu, \Sigma}(x) = \frac 1 {(2\pi)^{D/2}}\frac 1 {|\Sigma|^{1/2}} exp{-\frac 1 2 (x-\mu)^T\Sigma^{-1}(x-\mu)}$</li>
</ul>
</li>
<li>那么如何找 $\mu$ 与 $\Sigma$ ？<strong>Maximum Likelihood</strong>, 最大似然估计.<ul>
<li>Likelihood of a <em>Gaussian</em> with mean $\mu$ and covariance matrix $\Sigma$:</li>
<li>$L(\mu, \Sigma) = \Pi<em>{i=1}^n f</em>{\mu, \Sigma}(x^{(i)})$</li>
<li>Assume that $\mu^<em>, \Sigma^</em>$ is the argument of the Gaussian Distribution with the maximum likelihood.</li>
<li>And the solution…<ul>
<li>$\mu^* = \frac 1 n \sum_{i=1}^nx^{(i)}$</li>
<li>$\Sigma ^{<em>} = \frac 1 n \sum_{i=1}^n (x^{(i)}-\mu^</em>)(x^{(i)}-\mu^*)^T$​}</li>
</ul>
</li>
</ul>
</li>
<li><p>Modifying Model</p>
<ul>
<li>Using the same Covariant Matrix</li>
<li>$L(\mu^1, \mu^2, \Sigma)$ <ul>
<li>Where $\Sigma = \frac {N(C_1)} {N(All)}\Sigma^1 + \frac {N(C_2)} {N(All)}\Sigma^2$ ​</li>
</ul>
</li>
<li>经过推导，我们的 Model 可以写成 $P_{w,b}(C_1|x) = \sigma(z), z=w \cdot x + b$</li>
</ul>
</li>
<li><p>假设所有的feature都是相互独立产生的，这种分类叫做 <strong>Naive Bayes Classifier</strong>（朴素贝叶斯分类器）</p>
</li>
</ul>
<h3 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h3><ul>
<li>Review<ul>
<li>Step 1: Function Set<ul>
<li>We want to find $P_{w,b}(C_1|x)$​.<ul>
<li>If $P_{w,b}(C_1|x) \ge 0.5$ then output $C_1$, else output $C_2$.</li>
</ul>
</li>
<li>$f<em>{w,b}(x):=P</em>{w,b}(C_1|x) = \sigma(z)$, where $z=w\cdot x + b$​</li>
</ul>
</li>
<li>Step 2: Goodness of the function<ul>
<li><img src="https://i.loli.net/2021/09/09/dT4isEYZSD5QGnH.png" alt="image-20210909203309727"></li>
<li><img src="https://datawhalechina.github.io/leeml-notes/chapter11/res/chapter11-5.png" alt="img"></li>
<li><img src="https://datawhalechina.github.io/leeml-notes/chapter11/res/chapter11-6.png" alt="img"></li>
<li>Cross Entropy</li>
</ul>
</li>
<li>Step 3: Find the best<ul>
<li><img src="https://i.loli.net/2021/09/09/I2isBxThM9nrNV4.png" alt="image-20210909205827591"></li>
<li>The partial derivatives are the same as those in linear regression.</li>
<li>The logistic regression is called discriminative method.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Discriminative-v-s-Generative"><a href="#Discriminative-v-s-Generative" class="headerlink" title="Discriminative v.s. Generative"></a>Discriminative v.s. Generative</h3><ul>
<li>Same model. $P(C_1|x) = \sigma(w\cdot x + b)$<ul>
<li>Logistic Regression: Directly find $w$ and $b$</li>
<li>Generative Model: Find $\mu^1$, $\mu^2$, $\Sigma^{-1}$</li>
<li>But we won’t obtain the same set of $w$ and $b$.</li>
</ul>
</li>
</ul>
<h3 id="Multi-class-Classification"><a href="#Multi-class-Classification" class="headerlink" title="Multi-class Classification"></a>Multi-class Classification</h3><h4 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h4><p><img src="https://datawhalechina.github.io/leeml-notes/chapter11/res/chapter11-18.png" alt="img"></p>
<ul>
<li>Definition of the target</li>
</ul>
<p><img src="https://datawhalechina.github.io/leeml-notes/chapter11/res/chapter11-19.png" alt="img"></p>
<h3 id="Limitation-of-logistic-regression"><a href="#Limitation-of-logistic-regression" class="headerlink" title="Limitation of logistic regression"></a>Limitation of logistic regression</h3><h4 id="Feature-Transformation"><a href="#Feature-Transformation" class="headerlink" title="Feature Transformation"></a>Feature Transformation</h4><p><img src="https://datawhalechina.github.io/leeml-notes/chapter11/res/chapter11-23.png" alt="img"></p>
<ul>
<li>Middle Layer!<ul>
<li>可以将很多的逻辑回归接到一起，就可以进行特征转换.</li>
<li><img src="https://datawhalechina.github.io/leeml-notes/chapter11/res/chapter11-26.png" alt="img"></li>
</ul>
</li>
</ul>
<h2 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h2><p><strong>Step 1: Neural Network</strong></p>
<ul>
<li>Fully Connect Feedforward Network<ul>
<li>Why call it deep? <strong>Deep = Many Hidden Layers</strong></li>
<li>本质：通过 Hidden Layers 进行 Feature Transformation</li>
</ul>
</li>
</ul>
<p><strong>Step 2: Loss Function</strong></p>
<ul>
<li>Cross Entropy</li>
</ul>
<p><strong>Step 3: Find the best function</strong></p>
<ul>
<li>Gradient Descent</li>
</ul>
<h3 id="Why-deep"><a href="#Why-deep" class="headerlink" title="Why deep?"></a>Why deep?</h3><ul>
<li>More parameters, better performance</li>
<li><p>Universality Theorem</p>
<ul>
<li>Any continuous function $f:R^N \rightarrow R^M$​ can be realized by a network with one hidden layer with enough neurons.</li>
<li>So why <strong>Deep Learning</strong>, not <strong>Fat Learning</strong>?</li>
</ul>
<h2 id="CNN-Convolutional-Neuronal-Network）"><a href="#CNN-Convolutional-Neuronal-Network）" class="headerlink" title="CNN (Convolutional Neuronal Network）"></a>CNN (Convolutional Neuronal Network）</h2></li>
</ul>
<h3 id="Why-CNN-for-image"><a href="#Why-CNN-for-image" class="headerlink" title="Why CNN for image"></a>Why CNN for image</h3><ul>
<li>Some patterns are much smaller than the whole image. That is to say, a neuron only need to have connection with a small region of the image, but not the whole image.</li>
<li>But the same pattern may appear in different regions in different images.<ul>
<li>We could let these neurons share their parameters…</li>
</ul>
</li>
<li>Subsampling the pixels will not change the object.</li>
</ul>
<h3 id="The-whole-CNN-architecture"><a href="#The-whole-CNN-architecture" class="headerlink" title="The whole CNN architecture"></a>The whole CNN architecture</h3><p>Image -&gt; (Convolution -&gt; Max Pooling)$^{+}$ -&gt; Flatten -(as input)-&gt; Fully Connected Feedforward Network </p>
<h4 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h4><ul>
<li>Calculation approaches</li>
<li>Feature Map</li>
<li>Colorful Image</li>
</ul>
<p><img src="https://i.loli.net/2021/10/02/Y3GhrSRlVWsnofc.png" alt="image-20211002101802253"></p>
<ul>
<li><strong>Channel</strong>: 颜色通道</li>
<li>What does CNN learn? 使用 gradient ascent 寻找 input $x^{*} = arg \max_x{a^k}$</li>
<li>Deep Dream: let CNN exaggerate what it sees</li>
<li>以 Alpha Go 为例讲解 Architecture 的可选择性</li>
</ul>
<h2 id="RNN-Recurrent-Neural-Network"><a href="#RNN-Recurrent-Neural-Network" class="headerlink" title="RNN (Recurrent Neural Network)"></a>RNN (Recurrent Neural Network)</h2><ul>
<li>Example Application<ul>
<li>Slot filling: Nerual Network needs memory!</li>
<li><img src="https://i.loli.net/2021/10/02/eJHzUvSWnOcTtBD.png" alt="image-20211002120218869"></li>
<li>Bidirectional RNN<ul>
<li><img src="https://i.loli.net/2021/10/02/r4fRUpxLKGBbnyc.png" alt="image-20211002120401629"></li>
<li>Why? Have a broader view of context. 正向只看前面，反向只看后面.</li>
</ul>
</li>
<li>Long Short-term Memory (LSTM)<ul>
<li>Input signal and output signal are learned by the network itself.</li>
<li><img src="https://i.loli.net/2021/10/02/FQIxTbOJPH25DvY.png" alt="image-20211002120742395"></li>
<li><img src="https://i.loli.net/2021/10/02/Z1V2qWHmAg354ku.png" alt="image-20211002121407528"></li>
</ul>
</li>
<li>How to train RNN?<ul>
<li>Loss function? Sum over cross entropy.</li>
<li>Learning? Gradient descent. <ul>
<li>How to calc partial derivative? BPTT (Back propagation through time).</li>
<li>The error surface may be very flat or very steep. Clipping…</li>
<li>LSTM may deal with gradient vanishing, but not with gradient explode.</li>
</ul>
</li>
</ul>
</li>
<li>Applications</li>
</ul>
</li>
</ul>
<h2 id="Semi-supervised-Learning-半监督学习"><a href="#Semi-supervised-Learning-半监督学习" class="headerlink" title="Semi-supervised Learning 半监督学习"></a>Semi-supervised Learning 半监督学习</h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul>
<li>Semi-supervised learning ${(x^r, \hat y^r)}<em>{r=1}^R, {x^u}</em>{u=R}^{R+U}$<ul>
<li>A set of unlabeled data, usually $U\gg R$</li>
</ul>
</li>
<li>Classification 半监督学习的分类<ul>
<li>Transductive learning: unlabelled data is the testing data</li>
<li>Inductive learning: unlabelled data is not in the testing data</li>
</ul>
</li>
<li>Why semi-supervised learning?<ul>
<li>Collecting data is easy, but collecting “labelled” data is expensive</li>
<li>We do semi-supervised learning in our lives</li>
</ul>
</li>
</ul>
<h3 id="Semi-supervised-learning-for-Generative-Model"><a href="#Semi-supervised-learning-for-Generative-Model" class="headerlink" title="Semi-supervised learning for Generative Model"></a>Semi-supervised learning for Generative Model</h3><p><img src="https://i.loli.net/2021/10/03/WhAsVmULjZCRGMq.png" alt="image-20211003005516326"></p>
<h3 id="Low-density-separation-assumption"><a href="#Low-density-separation-assumption" class="headerlink" title="Low-density separation assumption"></a><strong>Low-density separation assumption</strong></h3><ul>
<li>Assumption<ul>
<li>两个 Class 之间非黑即白 (Black or white)</li>
</ul>
</li>
</ul>
<p><img src="https://i.loli.net/2021/10/03/nWAefx6phy41tUK.png" alt="image-20211003010157597"></p>
<p><img src="https://i.loli.net/2021/10/03/YEya7komzgsGPM6.png" alt="image-20211003011038595"></p>
<h3 id="Smoothness-assumption"><a href="#Smoothness-assumption" class="headerlink" title="Smoothness assumption"></a>Smoothness assumption</h3><ul>
<li>近朱者赤 近墨者黑<ul>
<li>“similar” x has the same $\hat y$</li>
<li>More precisely:<ul>
<li>x is not uniform.</li>
<li>if $x^1$ and $x^2$​ are close in a high density region (connected by a high density path)</li>
<li>then $y^1$ and $y^2$ are the same</li>
</ul>
</li>
</ul>
</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/09/python-asyncio-websocket/" rel="prev" title="Python asyncio, thread 与 websocket 库初探">
      <i class="fa fa-chevron-left"></i> Python asyncio, thread 与 websocket 库初探
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/10/ml-transformer/" rel="next" title="【机器学习】Transformer 笔记">
      【机器学习】Transformer 笔记 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.</span> <span class="nav-text">机器学习介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B%E5%8F%8A%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5"><span class="nav-number">1.1.</span> <span class="nav-text">发展历程及基础概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF"><span class="nav-number">1.2.</span> <span class="nav-text">相关技术</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Regression"><span class="nav-number">2.</span> <span class="nav-text">Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Steps"><span class="nav-number">2.1.</span> <span class="nav-text">Steps</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AF%E8%83%BD%E5%87%BA%E7%8E%B0%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">2.2.</span> <span class="nav-text">可能出现的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A5%E9%AA%A4%E4%BC%98%E5%8C%96"><span class="nav-number">2.3.</span> <span class="nav-text">步骤优化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E6%9E%90%E8%AF%AF%E5%B7%AE"><span class="nav-number">3.</span> <span class="nav-text">分析误差</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Concept"><span class="nav-number">3.1.</span> <span class="nav-text">Concept</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cross-Validation"><span class="nav-number">3.2.</span> <span class="nav-text">Cross Validation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-Descent"><span class="nav-number">4.</span> <span class="nav-text">Gradient Descent</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Tuning-learning-rates"><span class="nav-number">4.1.</span> <span class="nav-text">Tuning learning rates</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Adaptive-learning-rates"><span class="nav-number">4.1.1.</span> <span class="nav-text">Adaptive learning rates</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Stochastic-Gradient-Descent"><span class="nav-number">4.2.</span> <span class="nav-text">Stochastic Gradient Descent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Feature-Scaling-%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE"><span class="nav-number">4.3.</span> <span class="nav-text">Feature Scaling 特征缩放</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Possible-Problems"><span class="nav-number">4.4.</span> <span class="nav-text">Possible Problems</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Classification-%E6%A6%82%E7%8E%87%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B"><span class="nav-number">5.</span> <span class="nav-text">Classification 概率分类模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B"><span class="nav-number">5.1.</span> <span class="nav-text">回归模型与概率模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Generative-Model"><span class="nav-number">5.2.</span> <span class="nav-text">Generative Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Logistic-Regression"><span class="nav-number">5.3.</span> <span class="nav-text">Logistic Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Discriminative-v-s-Generative"><span class="nav-number">5.4.</span> <span class="nav-text">Discriminative v.s. Generative</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-class-Classification"><span class="nav-number">5.5.</span> <span class="nav-text">Multi-class Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Softmax"><span class="nav-number">5.5.1.</span> <span class="nav-text">Softmax</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Limitation-of-logistic-regression"><span class="nav-number">5.6.</span> <span class="nav-text">Limitation of logistic regression</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Feature-Transformation"><span class="nav-number">5.6.1.</span> <span class="nav-text">Feature Transformation</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Deep-Learning"><span class="nav-number">6.</span> <span class="nav-text">Deep Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Why-deep"><span class="nav-number">6.1.</span> <span class="nav-text">Why deep?</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CNN-Convolutional-Neuronal-Network%EF%BC%89"><span class="nav-number">7.</span> <span class="nav-text">CNN (Convolutional Neuronal Network）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Why-CNN-for-image"><span class="nav-number">7.1.</span> <span class="nav-text">Why CNN for image</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-whole-CNN-architecture"><span class="nav-number">7.2.</span> <span class="nav-text">The whole CNN architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Convolution"><span class="nav-number">7.2.1.</span> <span class="nav-text">Convolution</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN-Recurrent-Neural-Network"><span class="nav-number">8.</span> <span class="nav-text">RNN (Recurrent Neural Network)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Semi-supervised-Learning-%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">9.</span> <span class="nav-text">Semi-supervised Learning 半监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Introduction"><span class="nav-number">9.1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Semi-supervised-learning-for-Generative-Model"><span class="nav-number">9.2.</span> <span class="nav-text">Semi-supervised learning for Generative Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Low-density-separation-assumption"><span class="nav-number">9.3.</span> <span class="nav-text">Low-density separation assumption</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Smoothness-assumption"><span class="nav-number">9.4.</span> <span class="nav-text">Smoothness assumption</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="c7w"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">c7w</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">44</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">c7w</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
