<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="baidu-site-verification" content="codeva-kligclmjAj" />
  
  
  <title>Note for CS 285, Deep Reinforcement Learning | c7w&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <meta name="description" content="This is note for CS 285, DRL @ UCB.
Following http://rail.eecs.berkeley.edu/deeprlcourse-fa21/.
Lec 2. Supervised Learning of Behaviors
首先引入一些记号。输入的 O">
  
  
  
    <link rel="shortcut icon" href="../../favicon.ico">
  
  <link rel="stylesheet" href="../../css/style.css">
  
    <link rel="stylesheet" href="../../fancybox/jquery.fancybox-1.3.4.css">
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <div id="nav-outer">
  <nav id="main-nav" class="outer">
    <a id="main-nav-toggle" class="nav-icon"></a>
    
      <a class="main-nav-link" href="../../index.html">Home</a>
    
      <a class="main-nav-link" href="../../archives">Posts</a>
    
      <a class="main-nav-link" href="../../about">About</a>
    
      <a class="main-nav-link" href="../../friends">Friends</a>
    
    <div class="main-nav-space-between"></div>
    
  </nav>
</div>
<!-- <div id="header-title">
  <h1 id="logo-wrap">
    <a href="../../index.html" id="logo">c7w&#39;s Blog</a>
  </h1>
  
    <h2 id="subtitle-wrap">
      <a href="../../index.html" id="subtitle">Ready for some fun stuff :)</a>
    </h2>
  
</div> -->

      <div id="content" class="outer">
        <section id="main"><article id="post-cs285" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a class="article-date">
  <time class="dt-published" datetime="2023-10-07T21:32:06.960Z" itemprop="datePublished">2023-10-08</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="../../categories/RL/">RL</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Note for CS 285, Deep Reinforcement Learning
    </h1>
  

      </header>
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>This is note for CS 285, DRL @ UCB.</p>
<p>Following <a target="_blank" rel="noopener" href="http://rail.eecs.berkeley.edu/deeprlcourse-fa21/">http://rail.eecs.berkeley.edu/deeprlcourse-fa21/</a>.</p>
<h2 id="lec-2 supervised learning of behaviors">Lec 2. Supervised Learning of Behaviors</h2>
<p>首先引入一些记号<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>输入的 Observation 记为 <span class="markdown-them-math-inline">$\mathbf o$</span><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>输入图像<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>输出的 Action 记为 <span class="markdown-them-math-inline">$\mathbf a$</span><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>预测标签<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而策略网络<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>Policy<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>记为 <span class="markdown-them-math-inline">$\pi_\theta(\mathbf a \mid \mathbf o)$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>我们使用下标 <span class="markdown-them-math-inline">$t$</span> 来表示离散化之后的时间步<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>即 <span class="markdown-them-math-inline">$\mathbf o_t$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$\mathbf a_t$</span> 和 <span class="markdown-them-math-inline">$\pi_\theta(\mathbf a_t \mid \mathbf o_t)$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>有时我们使用 <span class="markdown-them-math-inline">$\mathbf s_t$</span> 来表示当前时间步的状态<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>State<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$\mathbf s_t$</span> 和 <span class="markdown-them-math-inline">$\mathbf o_t$</span> 的区别在于<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>(1) Observation 是观察到的现象<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>State 是由 Action 决定的反应系统状态的量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>(2) 仅使用 Observation 可能没办法去推断 State<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不同的 Observation 可能对应同一个 State<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>(3) 状态 State 往往是被认为满足马尔科夫性质的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而 Observation 却不是如此<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>也有的记号用 <span class="markdown-them-math-inline">$\mathbf x_t$</span> 表示 State<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$\mathbf u_t$</span> 表示 Action<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="cs285.assets/image-20230910152826762.png" alt="image-20230910152826762"></p>
<p><strong>Imitation Learning</strong> 是一种学习 Policy 的方式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>它也叫 <strong>behavioral cloning</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>即收集了大笔 <span class="markdown-them-math-inline">$\mathbf o_t$</span> 和专家给出的 <span class="markdown-them-math-inline">$\mathbf a_t$</span> 对作为训练数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使用有监督学习的方式去进行学习<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>跟常见的有监督学习不同<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这种方式通常来说是不工作的<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>除非训练数据照顾到足够多的 Corner Cases<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这是因为 State 的误差会随着多次 Inference 的时间步累积<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如下图所示<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><img src="cs285.assets/image-20230909231207503.png" alt="image-20230909231207503"></p>
<p><strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>【</h-inner></h-char></span>解决方案 1<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>DAgger<span class="bd-box"><h-char class="bd bd-beg"><h-inner>】</h-inner></h-char></span></strong> 想要解决上述的问题<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>Distributional drift Problem<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>解决思路 1 应该是尽可能减少因为误差累积导致的 State 偏差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>一个可能的解决方案是从训练数据的角度出发<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们尽可能照顾到更多的情况<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如下图所示<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><img src="cs285.assets/image-20230909231654997.png" alt="image-20230909231654997"></p>
<p>也就是说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们尝试去修改我们的训练数据 <span class="markdown-them-math-inline">$p_\text{data} (\mathbf o_t)$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>我们可以试着以 <span class="markdown-them-math-inline">$p_{\pi_\theta} (\mathbf o_t)$</span> 作为训练状态<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后让人工标注在这个训练状态下需要什么样的 Action <span class="markdown-them-math-inline">$\mathbf a_t$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后使用当前的策略网络去进行推理<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>收集下一个需要标注的状态<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这种方法叫做 DAgger<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>Dataset Aggregation<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><img src="cs285.assets/image-20230909232153665.png" alt="image-20230909232153665"></p>
<p><strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>【</h-inner></h-char></span>解决方案 2<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>更好地拟合专家<span class="bd-box"><h-char class="bd bd-beg"><h-inner>】</h-inner></h-char></span></strong> 另一种解决方案是更好地去拟合专家给出的 Action 数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>减少单步产生的误差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这主要从以下两个方面着手<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<ul>
<li>Observation 不满足马尔科夫性质<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>将 Policy 的 Formulation 从 <span class="markdown-them-math-inline">$\pi_\theta(\mathbf a_t \mid \mathbf o_t)$</span> 改为 <span class="markdown-them-math-inline">$\pi_\theta(\mathbf a_t \mid \mathbf o_t, \mathbf o_{t-1}, \cdots, \mathbf o_1)$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这通过采用一些具有记忆功能的网络结构来实现<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>
<ul>
<li>这样做会一定程度上减轻 Causal Confusion 的问题<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>fixed-capacity policies that are trained to fixed training error on the same demonstrations with access to more information can actually yield worse performance<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></li>
</ul>
</li>
<li>从人类中学到的 Action 并不是唯一的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>(1) 可以考虑输出 mixture of Gaussians 作为答案<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>(2) 可以考虑让策略网络输入 latent variable 作为输入<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>将 observation 作为 condition 对 random latent variable 进行 decode<span class="bd-box"><h-char class="bd bd-beg"><h-inner>；</h-inner></h-char></span>(3) 自回归离散化 Autoregressive discretization<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>逐个输出维度进行离散化后<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>对 softmax 之后的结果采样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>将采样得到的结果输入另一个网络中进行下一个输出维度的离散化和采样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如此迭代<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></li>
</ul>
<p><strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>【</h-inner></h-char></span>解决方案 3<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>更稳定的 trajectory<span class="bd-box"><h-char class="bd bd-beg"><h-inner>】</h-inner></h-char></span></strong> 我们也可以让 trajectory 的性质更稳定<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>对微小误差有容忍度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>来解决 Distributional drift 的问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>具体方案在后面的章节介绍<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>小结<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>Imitation Learning 这种范式的固有问题就是需要人类专家来提供数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而这往往是有限或者在表示方式上十分困难的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><strong>Cost functions<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>Reward functions</strong> 的相关介绍<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>从一个具体例子出发<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们可以形式化表示我们的目标为<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$\min _\theta E_{\mathbf{a} \sim \pi_\theta(\mathbf{a} \mid \mathbf{s}), \mathbf{s}^{\prime} \sim p\left(\mathbf{s}^{\prime} \mid \mathbf{s}, \mathbf{a}\right)}\left[\delta\left(\mathbf{s}^{\prime}=\text { eaten by tiger }\right)\right] \\
\Rightarrow \min _\theta E_{\mathbf{s}_{1: T}, \mathbf{a}_{1: T}}\left[\sum_t \delta\left(\mathbf{s}_t=\text { eaten by tiger }\right)\right] \\
\Rightarrow  \min _\theta E_{\mathbf{s}_{1: T}, \mathbf{a}_{1: T}}\left[\sum_t c\left(\mathbf{s}_t, \mathbf{a}_t\right)\right] (*)
$$</div><p>这里 <span class="markdown-them-math-inline">$c(\mathbf s_t, \mathbf a_t)$</span> 就是 cost function<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>或者我们将 min 改成 max 就是 reward function <span class="markdown-them-math-inline">$r$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><strong>Goal-conditioned Policies.</strong> 我们将最终的 Goal State 也作为 Policy Network 的输入<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>通过仿真环境采集足够多的数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>并筛选到达某个最终 Goal 的数据出来进行 Retrai</p>
<p><img src="cs285.assets/image-20230910002855958.png" alt="image-20230910002855958"></p>
<h2 id="lec-4 introduction to reinforcement learning">Lec 4. Introduction to Reinforcement Learning</h2>
<h3 id="notations">Notations</h3>
<p><strong>Markov Decision Process</strong> (MDP)</p>
<p><img src="cs285.assets/image-20230910153641900.png" alt="image-20230910153641900"></p>
<p><span class="markdown-them-math-inline">$r$</span> : <span class="markdown-them-math-inline">$\mathcal S \times \mathcal A \rightarrow \mathbb R$</span>, 是 reward function.</p>
<p><strong>Partially-observed MDP</strong></p>
<p><img src="cs285.assets/image-20230910153835737.png" alt="image-20230910153835737"></p>
<p><strong><span class="bd-box"><h-char class="bd bd-end"><h-inner>【</h-inner></h-char></span>强化学习的 Objective<span class="bd-box"><h-char class="bd bd-beg"><h-inner>】</h-inner></h-char></span></strong> 由于 Markov 性质<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们可以将强化学习的目标写成<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$\begin{gathered}
\underbrace{p_\theta\left(\mathbf{s}_1, \mathbf{a}_1, \ldots, \mathbf{s}_T, \mathbf{a}_T\right)}_{p_\theta(\tau)}=p\left(\mathbf{s}_1\right) \prod_{t=1}^T \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t\right) p\left(\mathbf{s}_{t+1} \mid \mathbf{s}_t, \mathbf{a}_t\right) \\
\\
\theta^{\star} =\arg \max _\theta E_{\tau \sim p_\theta(\tau)}\left[\sum_t r\left(\mathbf{s}_t, \mathbf{a}_t\right)\right] \\
=\arg \max _\theta \sum_{t=1}^T E_{\left(\mathbf{s}_t, \mathbf{a}_t\right) \sim p_\theta\left(\mathbf{s}_t, \mathbf{a}_t\right)}\left[r\left(\mathbf{s}_t, \mathbf{a}_t\right)\right]

\end{gathered}
$$</div><p>这里 <span class="markdown-them-math-inline">$\tau$</span> 表示整个 trajectory<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$p_\theta\left(\mathbf{s}_t, \mathbf{a}_t\right)$</span> 是 state-action marginal distribution<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>当我们考虑无穷步的时候<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>由于 <span class="markdown-them-math-inline">$p_\theta\left(\mathbf{s}_t, \mathbf{a}_t\right)$</span> 最终会收敛成静态分布 <span class="markdown-them-math-inline">$p_\theta\left(\mathbf{s}, \mathbf{a}\right)$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们可以将强化学习的目标写成<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$\theta^{\star}=\arg \max _\theta \frac{1}{T} \sum_{t=1}^T E_{\left(\mathbf{s}_t, \mathbf{a}_t\right) \sim p_\theta\left(\mathbf{s}_t, \mathbf{a}_t\right)}\left[r\left(\mathbf{s}_t, \mathbf{a}_t\right)\right] \rightarrow E_{(\mathbf{s}, \mathbf{a}) \sim p_\theta(\mathbf{s}, \mathbf{a})}[r(\mathbf{s}, \mathbf{a})]
$$</div><h3 id="value-functions">Value Functions</h3>
<p><img src="cs285.assets/image-20230910162227134.png" alt="image-20230910162227134"></p>
<p><img src="cs285.assets/image-20230910162501088.png" alt="image-20230910162501088"></p>
<h3 id="algorithms">Algorithms</h3>
<p>The anatomy of a RL Algo. is a loop of following three steps:</p>
<ul>
<li>Generate Samples (i.e. run the policy)</li>
<li>Fit a model, and then estimate the return</li>
<li>Improve the policy</li>
</ul>
<p><img src="cs285.assets/image-20230910162725476.png" alt="image-20230910162725476"></p>
<p><img src="cs285.assets/image-20230910163605865.png" alt="image-20230910163605865"></p>
<p>在考虑 sample efficiency 时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>off policy 是说 sample 与 policy 无关<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>on policy 是说如果 policy 修改<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>就需要生成新的 sample<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="cs285.assets/image-20230910163741440.png" alt="image-20230910163741440"></p>
<h2 id="lec-5 policy gradients">Lec 5. Policy Gradients</h2>
<p>我们考虑对 Objective 进行求梯度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$\begin{aligned}
&amp;\theta^{\star}=\arg \max _\theta \underbrace{E_{\tau \sim p_\theta(\tau)}\left[\sum_t r\left(\mathbf{s}_t, \mathbf{a}_t\right)\right]}_{J(\theta)}\\
&amp;\begin{gathered}
J(\theta)=E_{\tau \sim p_\theta(\tau)}[r(\tau)]=\int p_\theta(\tau) r(\tau) d \tau \\
\end{gathered}
\end{aligned}
$$</div><p>进行简单的代换<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们有<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><img src="cs285.assets/image-20230910224304717.png" alt="image-20230910224304717"></p>
<p>由于<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$\log p_\theta(\tau)=\log p\left(\mathbf{s}_1\right)+\sum_{t=1}^T \left( \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t\right)+\log p\left(\mathbf{s}_{t+1} \mid \mathbf{s}_t, \mathbf{a}_t\right)\right)
$$</div><p>而且<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$\nabla_\theta\log p\left(\mathbf{s}_1\right) = 0, \\
\nabla_\theta\log p\left(\mathbf{s}_{t+1} \mid \mathbf{s}_t, \mathbf{a}_t\right) = 0,
$$</div><p>于是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$\nabla_\theta J(\theta)=E_{\tau \sim p_\theta(\tau)}\left[\left(\sum_{t=1}^T \nabla_\theta \log \pi_\theta\left(\mathbf{a}_t \mid \mathbf{s}_t\right)\right)\left(\sum_{t=1}^T r\left(\mathbf{s}_t, \mathbf{a}_t\right)\right)\right]
$$</div><p>然后我们可以用采样 N 个小样本作为一个 Batch 的方法来对期望值进行估计<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="cs285.assets/image-20230910224815354.png" alt="image-20230910224815354"></p>
<p><img src="cs285.assets/image-20230910225128255.png" alt="image-20230910225128255"></p>
<p>这个 formulation 在 partially observed (no states but observations) 的情况下也适用<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>Policy gradient 的方法也有一定的问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>比如在下图所示<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span></p>
<p><img src="cs285.assets/image-20230910234333172.png" alt="image-20230910234333172"></p>
<p><img src="cs285.assets/image-20230910234411440.png" alt="image-20230910234411440"></p>
<p>这是 Policy gradient 方法存在的 <strong>High Variance</strong> 问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>具体来说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>上述两幅图的有颜色的线段代表 sample<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>bar 代表 reward 的大小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>横轴表示 trajectory<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>蓝色的线表示在某个 Policy 下的 possibility distribution<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>上下两幅图的区别是给 reward function 加了一个常量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而这导致这次对策略的更新变得十分不稳定<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>如果采到的数据集足够大这个问题自然可以缓解<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但在有限数据集下这个问题仍然存在<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>This problem can be identified as follows. Without any a prior model of the system we’re seeking to optimize, we begin with a policy whose distribution of actions over a given state is effectively uniform. Of course, as we train the model we hope to shape the probability density so that it’s unimodal on a single action, or possibly multimodal over a few successful actions that can be taken in that state. However, acquiring this knowledge requires our model to observe the outcomes of many different actions taken in many different states. This is made exponentially worse in continuous action or state spaces as visiting even close to every state-action pair is computationally intractable.</p>
<p><strong>Variance Reduction</strong> 的有关方法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这里首先介绍 <strong>reward to go</strong> 方法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这个方法基于 causality<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>即当 <span class="markdown-them-math-inline">$t \lt t'$</span> 时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在 <span class="markdown-them-math-inline">$t$</span> 时刻获得的 reward 不会被 <span class="markdown-them-math-inline">$t'$</span> 时刻做出的策略影响<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>即现在的策略的更新方向不应该与过去的 reward 有关<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这也就是说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们可以将 policy gradient 的 formulation 修改为<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla_\theta \log \pi_\theta\left(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t}\right) \underbrace{(\sum_{t'=t}^T r\left(\mathbf{s}_{i, t^{\prime}}, \mathbf{a}_{i, t^{\prime}}\right)}_{\begin{array}{c}
\text { &quot;reward to go&quot; } \\
\hat{Q}_{i, t}
\end{array}})
$$</div><p>然后是 <strong>Baseline</strong> 方法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这个方法的 intuition 非常简单<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>即我们让比平均更好的 policy 的 reward 是正数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>不然 reward 是负数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们只需要简单修改<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><img src="cs285.assets/image-20231008024353485.png" alt="image-20231008024353485"></p>
<p>方法的正确性可以被证明<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>即使从 reward function 中减去了一项<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>对于 policy gradient 的期望估计仍然是无偏的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>、</h-inner></h-char></span>同时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们还可以通过对 policy gradient 的方差对 b 求偏导的方式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>derive 出可以使 policy gradient 方差最小的 b 值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><strong>Off-policy Policy Gradients.</strong> 上述的 Policy Gradients 方法是经典的 On-policy 方法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因为每次对 Policy Network 进行更新后<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>都要重新采样数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因为 underlying distribution of trajectories 发生了改变<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而我们在估计 <span class="markdown-them-math-inline">$\nabla_\theta J(\theta)$</span> 时需要在 <span class="markdown-them-math-inline">$E_{\tau \sim p_\theta(\tau)}$</span> 的设定下采样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这里我们尝试解决如果我们没有从 <span class="markdown-them-math-inline">$p_\theta(\tau)$</span> 从采样的数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而只有从另一个分布 <span class="markdown-them-math-inline">$\bar{p}(\tau)$</span> 中采样的数据的问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这里的解决方法叫做 <strong>Importance Sampling</strong>.</p>
<p><img src="cs285.assets/image-20231008025328594.png" alt="image-20231008025328594"></p>
<p><img src="cs285.assets/image-20231008025458518.png" alt="image-20231008025458518"></p>
<p>这样我们可以从某个时刻的 Policy 中采样一些数据<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后用来优化另一个时间步的 Policy<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><img src="cs285.assets/image-20231008025847265.png" alt="image-20231008025847265"></p>
<p>我们可以再将 causality 的考量加进去<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这样得到 Off-policy 的 Policy Gradients 算法的新版本<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><img src="cs285.assets/image-20231008030141540.png" alt="image-20231008030141540"></p>
<p>但是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>最后一项<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>从 t’‘=t 到 t’‘=t’<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>Importance Weight 往往因为是多个 &lt;1 的数相乘而变得很小<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在实际运用时往往忽略本项<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>即使忽略也不影响正确性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>会得到一个 Policy Iteration 算法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="cs285.assets/image-20231008031023217.png" alt="image-20231008031023217"></p>
<p>同时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>Policy gradient 还有一些其他的数值上的问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>比如下面这个例子<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><img src="cs285.assets/image-20231008034637690.png" alt="image-20231008034637690"></p>
<p>我们可知这时最优的 <span class="markdown-them-math-inline">$\theta$</span> 在 <span class="markdown-them-math-inline">$k=-1, \sigma \rightarrow 0$</span> 时取得<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>但是我们可以看到右图中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>policy gradient 的方向并不总是指向 optimal solution<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这时因为在 <span class="markdown-them-math-inline">$\sigma \rightarrow 0$</span> 时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>由于导数的数值精度问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>大部分精力用于优化 <span class="markdown-them-math-inline">$\sigma$</span> 上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>从数值分析的角度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这是只考虑了一阶导数所导致的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>下面是解决这个问题的方法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们考虑一次优化的过程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>我们现在是在 Parameter Space 搜索<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使得在一个 Step 变化不超过 <span class="markdown-them-math-inline">$\epsilon$</span> 的前提下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>寻找可以最大化 reward value 的方式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>而这样做是有问题的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因为有些参数的 gradient 相对于其他参数的 gradient 来说在数值意义上更加 bad-conditioned<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>于是取而代之的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们不在参数空间考虑<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>而是在 Policy space 上考虑<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>寻找一个在 Policy 变化不大的情况下的优化 Step<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其中 Policy 的变化用类似于 KL 散度的 divergence 度量来衡量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这里<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们用 KL 散度的二阶近似<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>展开结果如下图所示<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="cs285.assets/image-20231008035712021.png" alt="image-20231008035712021"></p>
<p>这里可以将结果写成<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><img src="cs285.assets/image-20231008040240492.png" alt="image-20231008040240492"></p>
<p>c.f. Trust Region Policy Optimization.</p>
<h2 id="lec-6 actor-critic algorithms">Lec 6. Actor-Critic Algorithms</h2>
<p>Actor-Critic 算法基于 Policy Gradients 算法的基础上改进<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>将表示 reward to go 的 Q 函数与表示状态价值的 V 函数视为可学习的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这样做是为了减少只 sample 一条或几条 trajectory 去估计 Q 和 V 时的误差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><img src="cs285.assets/image-20231008045136911.png" alt="image-20231008045136911"></p>
<p>我们这里给出 Advantage Function <span class="markdown-them-math-inline">$A^\pi(\mathbf s_t, \mathbf a_t)$</span> 的定义如下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><img src="cs285.assets/image-20231008045352674.png" alt="image-20231008045352674"></p>
<p>有了这些函数的定义后<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们首先决定要去拟合其中的哪些函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>我们首先可以近似地将 Q 和 A 表示成 V<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><img src="cs285.assets/image-20231008050002515.png" alt="image-20231008050002515"></p>
<p><strong>Policy Evaluation.</strong> 我们接下来关注拟合 <span class="markdown-them-math-inline">$V^\pi$</span> 函数的过程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这个过程也被叫做 Policy Evaluation<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>比较 Naive 的拟合方法是使用 Monte Carlo evaluation<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>即每次都将仿真器的状态 reset 回 <span class="markdown-them-math-inline">$\mathbf s_t$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后 rollout 多次取 reward 的平均值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>我们可以进一步优化这个 rollout 的过程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一般叫做 bootstrapped estimate<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>将考虑拟合的 V 函数 target 近似如下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><img src="cs285.assets/image-20231008051058971.png" alt="image-20231008051058971"></p>
<p><strong>Discount Factors.</strong> 使用 bootstrapped estimate 去做 policy evaluation 时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果是对于 episodic 的任务可能没有很大问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>但如果是对于 infinite horizon 的任务<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果每一步都用估计的 V 值的话<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可能最终会 accumulate 到正无穷<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>解决的方法是用下图所示的 simple trick, where <span class="markdown-them-math-inline">$\gamma \lt 1$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><img src="cs285.assets/image-20231008052047673.png" alt="image-20231008052047673"></p>
<p>我们可以根据以上分析导出 online 的 Actor-Critic 算法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><img src="cs285.assets/image-20231008053205903.png" alt="image-20231008053205903"></p>

      
    </div>
    <footer class="article-footer">
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="../diffusion-dpo/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Diffusion-DPO 公式推导
        
      </div>
    </a>
  
  
    <a href="../diffusion-guidance/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">
        
          Diffusion-Guidance &amp; Latent Space
        
      </div>
    </a>
  
</nav>

  
</article>


</section>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
        <div><p><span id="busuanzi_container_site_pv">Site total view count: <span id="busuanzi_value_site_pv">Loading...</span></span></p><p>Powered by <a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/">Hexo</a> & Theme <a target="_blank" rel="noopener" href="https://github.com/bill-xia/hexo-theme-mashiro">Mashiro</a> & GitHub Pages</p><p>Copyright © 2020-2024 c7w. LICENSE CC BY-NC-SA 4.0.</p></div>
      
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="../../index.html" class="mobile-nav-link">Home</a>
  
    <a href="../../archives" class="mobile-nav-link">Posts</a>
  
    <a href="../../about" class="mobile-nav-link">About</a>
  
    <a href="../../friends" class="mobile-nav-link">Friends</a>
  
</nav>
    
<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="../../js/clipboard.min.js"></script>
<script src="../../js/jquery-1.4.3.min.js"></script>

<script src="../../fancybox/jquery.fancybox-1.3.4.pack.js"></script>


<script src="../../js/script.js"></script>








<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '$', right: '$', display: false},
        {left: '\\(', right: '\\)', display: false},
        {left: '\\[', right: '\\]', display: true}
      ],
      throwOnError : false
    });
  });
</script>

  </div>
</body>
</html>