<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="baidu-site-verification" content="codeva-kligclmjAj" />
  
  
  <title>随笔：A Mathematical Framework for Transformer Circuits | c7w&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <meta name="description" content="这篇文章试着分析了 Transformer 架构的可解释性。它分析了 Zero-Layer, One-Layer Attn-Only 和 Two-Layer Attn-Only 三种 Transformer 在处理文本 next token prediction 任务时，模型是怎么通过权重“学会”序">
  
  
  
    <link rel="shortcut icon" href="../../favicon.ico">
  
  <link rel="stylesheet" href="../../css/style.css">
  
    <link rel="stylesheet" href="../../fancybox/jquery.fancybox-1.3.4.css">
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <div id="nav-outer">
  <nav id="main-nav" class="outer">
    <a id="main-nav-toggle" class="nav-icon"></a>
    
      <a class="main-nav-link" href="../../index.html">Home</a>
    
      <a class="main-nav-link" href="../../archives">Posts</a>
    
      <a class="main-nav-link" href="../../about">About</a>
    
      <a class="main-nav-link" href="../../friends">Friends</a>
    
    <div class="main-nav-space-between"></div>
    
  </nav>
</div>
<!-- <div id="header-title">
  <h1 id="logo-wrap">
    <a href="../../index.html" id="logo">c7w&#39;s Blog</a>
  </h1>
  
    <h2 id="subtitle-wrap">
      <a href="../../index.html" id="subtitle">Ready for some fun stuff :)</a>
    </h2>
  
</div> -->

      <div id="content" class="outer">
        <section id="main"><article id="post-transformer-circuit" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a class="article-date">
  <time class="dt-published" datetime="2024-11-24T14:51:47.374Z" itemprop="datePublished">2024-11-24</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="../../categories/LLM/">LLM</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      随笔：A Mathematical Framework for Transformer Circuits
    </h1>
  

      </header>
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>这篇文章试着分析了 Transformer 架构的可解释性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>它分析了 Zero-Layer, One-Layer Attn-Only 和 Two-Layer Attn-Only 三种 Transformer 在处理文本 next token prediction 任务时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>模型是怎么通过权重<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>学会<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char></span>序列建模的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>本随笔中记录了一些方法论和结论上的有趣之处<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h2 id="transformer-overview">Transformer Overview</h2>
<p><img src="transformer-circuit.assets/image.png" alt="image.png"></p>
<p>相比于原始 Transformer 的简化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>1. Attn-Only, no MLP, 2. No Bias Term, 3. No Normalization.</p>
<p>关于 Transformer 架构的一些结论<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><img src="transformer-circuit.assets/image%201.png" alt="image.png"></p>
<p>(A) The residual stream serves as a communication channel, allowing layers to interact without performing processing itself. Its deeply linear structure implies that it doesn’t have a “privileged basis” and can be rotated without changing model behavior.</p>
<p>(B) Virtual weights directly connect any pair of layers by multiplying out their interactions through the residual stream. These virtual weights describe how later layers read information written by previous layers.</p>
<p><img src="transformer-circuit.assets/image%202.png" alt="image.png"></p>
<p>© Subspaces and Residual Stream Bandwidth: The residual stream is a high-dimensional vector space, allowing layers to store information in different subspaces. Dimensions of the residual stream act like “memory” or “bandwidth,” with high demand due to the large number of computational dimensions relative to residual stream dimensions. Some MLP neurons and attention heads may manage memory by clearing residual stream dimensions set by other layers.</p>
<p><img src="transformer-circuit.assets/image%203.png" alt="image.png"></p>
<p>(D) Attention Heads are Independent and Additive: Attention heads operate independently and add their output back into the residual stream. This independence allows for a clearer theoretical understanding of transformer layers.</p>
<p><img src="transformer-circuit.assets/image%204.png" alt="image.png"></p>
<p>(E) Attention Heads as Information Movement:</p>
<ul>
<li><strong>Attention heads move information from one token’s residual stream to another’s.</strong></li>
<li>The process involves two linear operations: the attention pattern (<span class="markdown-them-math-inline">$A$</span>) and the output-value matrix (<span class="markdown-them-math-inline">$W_O W_V$</span>).</li>
<li>The attention pattern determines which token’s information is moved, while the output-value matrix determines what information is read and how it is written. These two processes are independent from each other.</li>
</ul>
<p>(F) Other Observations about Attention Heads:</p>
<ul>
<li>Attention heads perform a linear operation if the attention pattern is fixed.</li>
<li>The matrices <span class="markdown-them-math-inline">$W_Q$</span> and <span class="markdown-them-math-inline">$W_K$</span> always operate together, as do <span class="markdown-them-math-inline">$W_O$</span> and <span class="markdown-them-math-inline">$W_V$</span>.</li>
<li>Keys, queries, and value vectors are intermediary by-products of computing low-rank matrices.</li>
<li>Combined matrices <span class="markdown-them-math-inline">$W_{OV} = W_O W_V$</span> and <span class="markdown-them-math-inline">$W_{QK} = W_Q^T W_K$</span> can be defined for simplicity.</li>
<li>Products of attention heads behave much like attention heads themselves. (Attention heads across different layers can be combined. We call these virtual attention.)</li>
</ul>
<h2 id="zero-layer-transformers">Zero-Layer Transformers</h2>
<p><img src="transformer-circuit.assets/image%205.png" alt="image.png"></p>
<p>在语言模型中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>bigram 模型是一种简单的统计模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它根据前一个词预测下一个词<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>对于零层 Transformer<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>由于它只能使用当前 token 的信息来预测下一个 token<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因此它的行为类似于 bigram 模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>具体来说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>零层 Transformer 通过 <span class="markdown-them-math-inline">$ W_U W_E $</span> 直接将当前 token 的嵌入映射到预测下一个 token 的 logits<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这与 bigram 模型根据前一个词预测下一个词的方式相似<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h2 id="one-layer-attn-only transformer">One-Layer Attn-Only Transformer</h2>
<h3 id="formulation">Formulation</h3>
<p><img src="transformer-circuit.assets/image%206.png" alt="image.png"></p>
<p><img src="transformer-circuit.assets/image%207.png" alt="image.png"></p>
<ol>
<li><strong>Effect of Source Token on Logits</strong>:
<ul>
<li>If multiple destination tokens attend to the same source token with the same attention weight, the source token will influence the logits for the predicted output token in the same way for all those destination tokens.</li>
</ul>
</li>
<li><strong>Independent Consideration of OV and QK Circuits</strong>:
<ul>
<li>It can be useful to think of the OV (Output-Value) and QK (Query-Key) circuits <strong>separately</strong> since they are individually understandable functions (linear or bilinear operations on matrices).</li>
</ul>
</li>
</ol>
<h3 id="skip-trigrams">Skip-Trigrams</h3>
<p>One-Layer Attn-Only Transformer 可以被解释为 Skip-Trigrams<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="transformer-circuit.assets/image%208.png" alt="image.png"></p>
<ul>
<li>The QK circuit determines which “source” token<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>提供 K<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span> the “destination”<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>提供 Q<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span> token attends to and copies information from.</li>
<li>The OV circuit describes the effect on the “out” predictions for the next token.</li>
<li>Together, they form a “skip-trigram” [source]… [destination] [out].</li>
</ul>
<p>这使得 Copying and Primitive In-Context Learning 成为了可能<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>具体来说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><img src="transformer-circuit.assets/image%209.png" alt="image.png"></p>
<ul>
<li>Attention heads often dedicate capacity to copying tokens, increasing the probability of the token and similar tokens.</li>
<li>Tokens are copied to plausible positions based on bigram statistics.</li>
</ul>
<p>但是这种 Skip-Trigram 的建模方式也有问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="transformer-circuit.assets/image%2010.png" alt="image.png"></p>
<p>我们的单层模型以<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>分解形式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char></span>在 OV 和 QK 矩阵之间表示 skip-trigrams<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这有点像将函数 <span class="markdown-them-math-inline">$ f(a, b, c) = f_1(a, b)f_2(a, c) $</span> 表示出来<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>它们无法灵活地捕捉三方交互<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>例如<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果一个注意力头增加了 <code>keep</code>…<code>in mind</code> 和 <code>keep</code>…<code>at bay</code> 的概率<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它也必须增加 <code>keep</code>…<code>in bay</code> 和 <code>keep</code>…<code>at mind</code> 的概率<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这里 in 或者 at 提供了 query<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>成功的 attend 到了 keep 这个 token 提供的 key<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>但是<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>keep 这个 token 提供的 value<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>mind<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>bay<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>却没有与 query 做有效的建模<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这个问题会在 Two-Layer Attn-Only Transformers 中得到解决<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>如何检测这种存在于 Attn Head 之中的复制行为<span class="bd-box"><h-char class="bd bd-beg"><h-inner>？</h-inner></h-char></span></p>
<p><img src="transformer-circuit.assets/image%2011.png" alt="image.png"></p>
<p>To detect copying behavior in transformer models, researchers focus on identifying <strong>matrices that</strong> <strong>map the same vector to itself, thus increasing the probability of a token recurring</strong>. This involves examining the <strong>eigenvalues and eigenvectors</strong> of the OV circuit matrices. If a matrix has positive eigenvalues, it suggests that a linear combination of tokens increases the logits of those same tokens, indicating copying behavior.</p>
<p>This method reveals that many attention heads possess positive eigenvalues, aligning with the copying structure. While positive eigenvalues are strong indicators of copying, they are not definitive proof, as some matrices with positive eigenvalues may still decrease the logits of certain tokens. Alternative methods, like analyzing the matrix diagonal, also point to copying but lack robustness. Despite its imperfections, the eigenvalue-based approach serves as a useful summary statistic for identifying copying behavior.</p>
<h2 id="two-layer-attn-only transformer">Two-Layer Attn-Only Transformer</h2>
<h3 id="formulation-1">Formulation</h3>
<p><img src="transformer-circuit.assets/image%2012.png" alt="image.png"></p>
<h3 id="**term-importance analysis**"><strong>Term Importance Analysis</strong></h3>
<p><img src="transformer-circuit.assets/image%2013.png" alt="image.png"></p>
<p>我们使用上面的方法分析二层 Transformer 公式中每一项的重要性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><img src="transformer-circuit.assets/image%2014.png" alt="image.png"></p>
<p>We conclude that for understanding two-layer attention only models, we shouldn’t prioritize understanding the second order <span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>virtual attention heads<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char></span> but instead focus on the direct path (which can only contribute to bigram statistics) and individual attention head terms.</p>
<p>We can further subdivide these individual attention head terms into those in layer 1 and layer 2:</p>
<p><img src="transformer-circuit.assets/image%2015.png" alt="image.png"></p>
<p>This suggests we should focus on the second layer head terms.</p>
<h3 id="induction-head">Induction Head</h3>
<p>二层 Transformer 与 一层 Transformer 的最大区别在于<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>Q, K, V  read features in a subspace affected by a previous head.</p>
<p><img src="transformer-circuit.assets/image%2016.png" alt="image.png"></p>
<p><img src="transformer-circuit.assets/image%2017.png" alt="image.png"></p>
<p>By dividing the Frobenius norm of the product of the matrices by the Frobenius norms of the individual matrices, we obtain a normalized measure that accounts for the sizes of the matrices involved. This normalization helps to compare the relative importance of different compositions. This ratio indicates how much the product deviates from what would be expected if the matrices were random, providing insight into the significance of the interaction.</p>
<p>我们看出 K-Composition Term 里有很显著的 pair<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们接下来详细分析这些<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>induction head<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><img src="transformer-circuit.assets/image%2018.png" alt="image.png"></p>
<p>以第四行为例<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这里 D 作为 query<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>响应最大的 key 是 urs<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这说明<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这两层的 attention 协同交互起到了一个作用<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>即将前一个 token 的 信息映射到第二个 token 的 key 中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使得<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p><img src="transformer-circuit.assets/image%2019.png" alt="image.png"></p>
<p>然后<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>有进一步的实验来验证这个结论<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span> Given that we believe <strong>induction heads are attending to previous copies of the token and shifting forward</strong>, they should be able to do this on <em><strong>totally random</strong></em> repeated patterns. This is likely the hardest test one can give them, since they can’t rely on normal statistics about which tokens typically come after other tokens.</p>
<p><img src="transformer-circuit.assets/image%2020.png" alt="image.png"></p>
<p><img src="transformer-circuit.assets/image%2021.png" alt="image.png"></p>
<p>这两张图展示了 attention to &lt;5767&gt; 的 token<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p><strong>怎么找出 induction heads?</strong> Our mechanistic theory suggestions that induction heads must do two things:</p>
<ul>
<li>Have a “copying” OV circuit matrix (in previous layer).</li>
<li>Have a “same matching” QK circuit matrix. (As the previous layer has transported information of the previous token into current hidden state.)</li>
</ul>
<p><img src="transformer-circuit.assets/image%2022.png" alt="image.png"></p>

      
    </div>
    <footer class="article-footer">
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="../sit/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          SiT：Scalable Interpolant Transformers
        
      </div>
    </a>
  
  
    <a href="../test-time-scaling/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">
        
          随笔：Test-time Computation Scaling
        
      </div>
    </a>
  
</nav>

  
</article>


</section>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
        <div><p><span id="busuanzi_container_site_pv">Site total view count: <span id="busuanzi_value_site_pv">Loading...</span></span></p><p>Powered by <a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/">Hexo</a> & Theme <a target="_blank" rel="noopener" href="https://github.com/bill-xia/hexo-theme-mashiro">Mashiro</a> & GitHub Pages</p><p>Copyright © 2020-2024 c7w. LICENSE CC BY-NC-SA 4.0.</p></div>
      
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="../../index.html" class="mobile-nav-link">Home</a>
  
    <a href="../../archives" class="mobile-nav-link">Posts</a>
  
    <a href="../../about" class="mobile-nav-link">About</a>
  
    <a href="../../friends" class="mobile-nav-link">Friends</a>
  
</nav>
    
<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="../../js/clipboard.min.js"></script>
<script src="../../js/jquery-1.4.3.min.js"></script>

<script src="../../fancybox/jquery.fancybox-1.3.4.pack.js"></script>


<script src="../../js/script.js"></script>








<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '$', right: '$', display: false},
        {left: '\\(', right: '\\)', display: false},
        {left: '\\[', right: '\\]', display: true}
      ],
      throwOnError : false
    });
  });
</script>

  </div>
</body>
</html>