<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="baidu-site-verification" content="codeva-kligclmjAj" />
  
  
  <title>Diffusion-DPO 公式推导 | c7w&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <meta name="description" content="数学推导
在这篇笔记中我们给出 Diffusion DPO 的推导，推导过程参考了 [1] 的内容。
Diffusion DPO Loss Objective
我们先给出我们终极的推导目标。
$$L(\theta) = -\mathbb{E}_{(x_0^w, x_0^l) \sim \mathca">
  
  
  
    <link rel="shortcut icon" href="../../favicon.ico">
  
  <link rel="stylesheet" href="../../css/style.css">
  
    <link rel="stylesheet" href="../../fancybox/jquery.fancybox-1.3.4.css">
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <div id="nav-outer">
  <nav id="main-nav" class="outer">
    <a id="main-nav-toggle" class="nav-icon"></a>
    
      <a class="main-nav-link" href="../../index.html">Home</a>
    
      <a class="main-nav-link" href="../../archives">Posts</a>
    
      <a class="main-nav-link" href="../../about">About</a>
    
      <a class="main-nav-link" href="../../friends">Friends</a>
    
    <div class="main-nav-space-between"></div>
    
  </nav>
</div>
<!-- <div id="header-title">
  <h1 id="logo-wrap">
    <a href="../../index.html" id="logo">c7w&#39;s Blog</a>
  </h1>
  
    <h2 id="subtitle-wrap">
      <a href="../../index.html" id="subtitle">Ready for some fun stuff :)</a>
    </h2>
  
</div> -->

      <div id="content" class="outer">
        <section id="main"><article id="post-diffusion-dpo" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a class="article-date">
  <time class="dt-published" datetime="2024-08-27T13:45:18.426Z" itemprop="datePublished">2024-08-27</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="../../categories/Diffusion/">Diffusion</a>►<a class="article-category-link" href="../../categories/Diffusion/Post-training/">Post-training</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Diffusion-DPO 公式推导
    </h1>
  

      </header>
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="数学推导">数学推导</h1>
<p>在这篇笔记中我们给出 Diffusion DPO 的推导<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>推导过程参考了 [1] 的内容<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h2 id="diffusion-dpo loss objective">Diffusion DPO Loss Objective</h2>
<p>我们先给出我们终极的推导目标<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$L(\theta) = -\mathbb{E}_{(x_0^w, x_0^l) \sim \mathcal{D}, t \sim \mathcal{U}(0, T), x_t^w \sim q(x_t^w|x_0^w), x_t^l \sim q(x_t^l|x_0^l)} \\
\log \sigma \left( -\beta T \omega(\lambda_t) \left( 
\| \epsilon^w - \epsilon_\theta(x_t^w, t) \|_2^2 - \| \epsilon^w - \epsilon_{\text{ref}}(x_t^w, t) \|_2^2 
- \left( \| \epsilon^l - \epsilon_\theta(x_t^l, t) \|_2^2 - \| \epsilon^l - \epsilon_{\text{ref}}(x_t^l, t) \|_2^2 \right) 
\right) \right)
$$</div><h2 id="from-rlhf to dpo">From RLHF to DPO</h2>
<p>我们先介绍 Bradley-Terry 模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>它将人类偏好 <span class="markdown-them-math-inline">$x_0^w$</span> over <span class="markdown-them-math-inline">$x_0^l$</span> 这件事用一个 Sigmoid 函数来建模<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$\begin{equation}p_{\text{BT}}(x_0^w \succ x_0^l \mid c) = \sigma \left( r(c, x_0^w) - r(c, x_0^l) \right)\end{equation}
$$</div><p>这里 r 是一个 reward model<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>用来衡量 <span class="markdown-them-math-inline">$x_0^*$</span> 与 <span class="markdown-them-math-inline">$c$</span> 和人类偏好的符合程度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>一个很自然的 idea 就是把 r 用另一个 pretrained 知识源来替代<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>比如随便搜了一下发现有很多 CLIP-DPO<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>如果没有这个外部的 r 函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>也可以用 data-driven 的方式学一个 r 函数出来<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$\begin{equation}L_{\text{BT}}(\phi) = -\mathbb{E}_{c, x_0^w, x_0^l} \left[ \log \sigma \left( r_{\phi}(c, x_0^w) - r_{\phi}(c, x_0^l) \right) \right]\end{equation}
$$</div><p>有了 r 函数之后<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们定义 RLHF 是一个对生成式模型 <span class="markdown-them-math-inline">$p_\theta(\mathbf{x}_0|\mathbf{c})$</span> 训练过程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这个过程可以使得我们通过上述方式定义的 reward function <span class="markdown-them-math-inline">$r(\mathbf{c}, \mathbf{x}_0)$</span> 在微调后的生成分布上取得较大的值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>同时通过 KL-Divergence 约束微调导致的分布偏移量并不大<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$\max_{p_\theta} \mathbb{E}_{\mathbf{c} \sim \mathcal{D}_c, \mathbf{x}_0 \sim p_\theta(\mathbf{x}_0|\mathbf{c})} \left[r(\mathbf{c}, \mathbf{x}_0)\right]    - \beta \, \text{KL}\left[p_\theta(\mathbf{x}_0|\mathbf{c}) \| p_{\text{ref}}(\mathbf{x}_0|\mathbf{c})\right]
$$</div><p>这个 RLHF 的极点事实上是有闭式解的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以将其表示为<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$\begin{aligned}\operatorname*{max}_{\pi}\mathbb{E}_{x\sim\mathcal{D},y\sim\pi}&amp; \left[r(x,y)\right]-\beta\mathbb{D}_{\mathrm{KL}}\left[\pi(y|x)\parallel\pi_{\mathrm{ref}}(y|x)\right] \\&amp;=\max_\pi\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\left[r(x,y)-\beta\log\frac{\pi(y|x)}{\pi_{\mathrm{ref}}(y|x)}\right] \\&amp;=\min_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\left[\log\frac{\pi(y|x)}{\pi_{\mathrm{ref}}(y|x)}-\frac{1}{\beta}r(x,y)\right] \\&amp;=\min_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi(y|x)}\left[\log\frac{\pi(y|x)}{\frac{1}{Z(x)}\pi_{\mathrm{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)}-\log Z(x)\right]\end{aligned}
$$</div><p>这里 KL Divergence 在对 y 取期望的意义下展开<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>Z(x) 是归一化函数<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>原文将其称为 partition function<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$Z(x)=\sum_y\pi_\text{ref}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)
$$</div><p>由于 <span class="markdown-them-math-inline">$\log Z(x)$</span> 项与 <span class="markdown-them-math-inline">$\pi$</span> 无关<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>让这个熵最小等价于分子和分母两个分布完全相同<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>即<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$\pi^*(y|x)=\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)\exp\left(\frac{1}{\beta}r(x,y)\right)
$$</div><p>换元回生成式模型的表示<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$p_\theta^*(\boldsymbol{x}_0|\boldsymbol{c})=p_{\mathrm{ref}}(\boldsymbol{x}_0|\boldsymbol{c})\exp\left(r(\boldsymbol{c},\boldsymbol{x}_0)/\beta\right)/Z(\boldsymbol{c})
$$</div><p>因此<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以移项表示 reward function<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$r(\boldsymbol{c},\boldsymbol{x}_0)=\beta\log\frac{p_\theta^*(\boldsymbol{x}_0|\boldsymbol{c})}{p_\text{ref}(\boldsymbol{x}_0|\boldsymbol{c})}+\beta\log Z(\boldsymbol{c})
$$</div><p>代入 <span class="markdown-them-math-inline">$L_{\text{BT}}(\phi)$</span> 中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们发现模型参数 <span class="markdown-them-math-inline">$\theta$</span> 本身变成了一个 reward model<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$L_{\mathrm{DPO}}(\theta)=-\mathbb{E}_{\boldsymbol{c},\boldsymbol{x}_{0}^{w},\boldsymbol{x}_{0}^{l}}\left[\log\sigma\left(\beta\log\frac{p_{\theta}(\boldsymbol{x}_{0}^{w}|\boldsymbol{c})}{p_{\mathrm{ref}}(\boldsymbol{x}_{0}^{w}|\boldsymbol{c})}-\beta\log\frac{p_{\theta}(\boldsymbol{x}_{0}^{l}|\boldsymbol{c})}{p_{\mathrm{ref}}(\boldsymbol{x}_{0}^{l}|\boldsymbol{c})}\right)\right]
$$</div><h2 id="dpo-on diffusion">DPO on Diffusion</h2>
<p>在这个阶段<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>最大的挑战是 <span class="markdown-them-math-inline">$p_\theta(\boldsymbol{x}_0|\boldsymbol{c})$</span> 可以由 <span class="markdown-them-math-inline">$p(\boldsymbol{x}_T)$</span> 由多条 Diffusion Path 生成出来<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>于是我们重新利用 Diffusion 利用 ELBO 的想法<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们将奖励函数定义在 <span class="markdown-them-math-inline">$\boldsymbol{x}_{0:T}$</span> 上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$r(\boldsymbol{c},\boldsymbol{x}_0)=\mathbb{E}_{p_\theta(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0,\boldsymbol{c})}\left[R(\boldsymbol{c},\boldsymbol{x}_{0:T})\right].
$$</div><p>这样定义下的 RLHF 过程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>应该满足<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$\max_{p_{\theta}}\mathbb{E}_{\boldsymbol{c}\sim\mathcal{D}_{c},\boldsymbol{x}_{0:T}\sim p_{\theta}(\boldsymbol{x}_{0:T}|\boldsymbol{c})}\left[r(\boldsymbol{c},\boldsymbol{x}_{0})\right]-\beta\mathbb{D}_{\mathrm{KL}}\left[p_{\theta}(\boldsymbol{x}_{0:T}|\boldsymbol{c})\|p_{\mathrm{ref}}(\boldsymbol{x}_{0:T}|\boldsymbol{c})\right]
$$</div><p>我们也可以为这个过程推导出 DPO 的 Loss 函数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$L_{\mathrm{DPO-Diffusion}}(\theta)=-\mathbb{E}_{(\boldsymbol{x}_{0}^{w},\boldsymbol{x}_{0}^{t})\sim\mathcal{D}}\log\sigma\bigg(\beta\mathbb{E}_{\boldsymbol{x}_{1:T}^{w}\sim p_{\theta}(\boldsymbol{x}_{1:T}^{w}|\boldsymbol{x}_{0}^{w})}\left[\log\frac{p_{\theta}(\boldsymbol{x}_{0:T}^{w})}{p_{\mathrm{ref}}(\boldsymbol{x}_{0:T}^{w})}-\log\frac{p_{\theta}(\boldsymbol{x}_{0:T}^{l})}{p_{\mathrm{ref}}(\boldsymbol{x}_{0:T}^{l})}\right]\bigg)
$$</div><p>接下来我们做两个优化<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>第一个是解决这需要优化多个时间步的问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>通过和 Diffusion 推导过程类似的 ELBO 推导<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们得到<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$L_{\mathrm{DPO-Diffusion}}(\theta)\leq-\mathbb{E}_{(\boldsymbol{x}_{0}^{w},\boldsymbol{x}_{0}^{l})\sim\mathcal{D},t\sim\mathcal{U}(0,T),\boldsymbol{x}_{t-1,t}^{w}\sim p_{\theta}(\boldsymbol{x}_{t-1,t}^{w}|\boldsymbol{x}_{0}^{w}),\boldsymbol{x}_{t-1,t}^{l}\sim p_{\theta}(\boldsymbol{x}_{t-1,t}^{l}|\boldsymbol{x}_{0}^{l})}\\\log\sigma\left(\beta T\log\frac{p_{\theta}(\boldsymbol{x}_{t-1}^{w}|\boldsymbol{x}_{t}^{w})}{p_{\mathrm{ref}}(\boldsymbol{x}_{t-1}^{w}|\boldsymbol{x}_{t}^{w})}-\beta T\log\frac{p_{\theta}(\boldsymbol{x}_{t-1}^{l}|\boldsymbol{x}_{t}^{l})}{p_{\mathrm{ref}}(\boldsymbol{x}_{t-1}^{l}|\boldsymbol{x}_{t}^{l})}\right)
$$</div><p>然后是 <span class="markdown-them-math-inline">$p_\theta(x_{t-1},x_t|x_0,c)$</span> 不可解的问题<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们使用 <span class="markdown-them-math-inline">$q$</span> 过程来对这个取变量算期望的过程做估计<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$\begin{aligned}L(\theta)=-&amp; \mathbb{E}_{(\boldsymbol{x}_{0}^{w},\boldsymbol{x}_{0}^{l})\sim\mathcal{D},t\sim\mathcal{U}(0,T),\boldsymbol{x}_{t}^{w}\sim q(\boldsymbol{x}_{t}^{w}|\boldsymbol{x}_{0}^{w}),\boldsymbol{x}_{t}^{l}\sim q(\boldsymbol{x}_{t}^{l}|\boldsymbol{x}_{0}^{l})} \\&amp;\log\sigma(-\beta T( \\&amp;+\mathbb{D}_{\mathrm{KL}}(q(\boldsymbol{x}_{t-1}^w|\boldsymbol{x}_{0,t}^w)\|p_\theta(\boldsymbol{x}_{t-1}^w|\boldsymbol{x}_t^w)) \\&amp;-\mathbb{D}_{\mathrm{KL}}(q(\boldsymbol{x}_{t-1}^w|\boldsymbol{x}_{0,t}^w)\|p_{\mathrm{ref}}(\boldsymbol{x}_{t-1}^w|\boldsymbol{x}_t^w)) \\&amp;-\mathbb{D}_{\mathrm{KL}}(q(\boldsymbol{x}_{t-1}^l|\boldsymbol{x}_{0,t}^l)\|p_\theta(\boldsymbol{x}_{t-1}^l|\boldsymbol{x}_t^l)) \\&amp;+\mathbb{D}_{\mathrm{KL}}(q(\boldsymbol{x}_{t-1}^l|\boldsymbol{x}_{0 t}^l)\|p_{\mathrm{ref}}(\boldsymbol{x}_{t-1}^l|\boldsymbol{x}_{t}^l))).\end{aligned}
$$</div><p>根据 Diffusion Loss 的推导<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这可以被写成<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$L(\theta) =-\mathbb{E}_{(\boldsymbol{x}_{0}^{w},\boldsymbol{x}_{0}^{l})\sim\mathcal{D},t\sim\mathcal{U}(0,T),\boldsymbol{x}_{t}^{w}\sim q(\boldsymbol{x}_{t}^{w}|\boldsymbol{x}_{0}^{w}),\boldsymbol{x}_{t}^{l}\sim q(\boldsymbol{x}_{t}^{l}|\boldsymbol{x}_{0}^{l})} \\\log\sigma\left(-\beta T\omega(\lambda_{t})\right)( \|\epsilon^w-\epsilon_\theta(\boldsymbol{x}_t^w,t)\|_2^2-\|\epsilon^w-\epsilon_{\mathrm{ref}}(\boldsymbol{x}_t^w,t)\|_2^2 -\left(\|\boldsymbol{\epsilon}^{l}-\boldsymbol{\epsilon}_{\theta}(\boldsymbol{x}_{t}^{l},t)\|_{2}^{2}-\|\boldsymbol{\epsilon}^{l}-\boldsymbol{\epsilon}_{\mathrm{ref}}(\boldsymbol{x}_{t}^{l},t)\|_{2}^{2}\right)\Big)\Big)
$$</div><h2 id="reference">Reference</h2>
<p>[1] Wallace B, Dang M, Rafailov R, et al. Diffusion model alignment using direct preference optimization[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 8228-8238.</p>
<p>[2] Rafailov R, Sharma A, Mitchell E, et al. Direct preference optimization: Your language model is secretly a reward model[J]. Advances in Neural Information Processing Systems, 2024, 36.</p>

      
    </div>
    <footer class="article-footer">
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="../csarch/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">
        
          《计算机系统结构》 课程复习 Note
        
      </div>
    </a>
  
</nav>

  
</article>


</section>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
        <div><p><span id="busuanzi_container_site_pv">Site total view count: <span id="busuanzi_value_site_pv">Loading...</span></span></p><p>Powered by <a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/">Hexo</a> & Theme <a target="_blank" rel="noopener" href="https://github.com/bill-xia/hexo-theme-mashiro">Mashiro</a> & GitHub Pages</p><p>Copyright © 2020-2024 c7w. LICENSE CC BY-NC-SA 4.0.</p></div>
      
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="../../index.html" class="mobile-nav-link">Home</a>
  
    <a href="../../archives" class="mobile-nav-link">Posts</a>
  
    <a href="../../about" class="mobile-nav-link">About</a>
  
    <a href="../../friends" class="mobile-nav-link">Friends</a>
  
</nav>
    
<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="../../js/clipboard.min.js"></script>
<script src="../../js/jquery-1.4.3.min.js"></script>

<script src="../../fancybox/jquery.fancybox-1.3.4.pack.js"></script>


<script src="../../js/script.js"></script>








<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '$', right: '$', display: false},
        {left: '\\(', right: '\\)', display: false},
        {left: '\\[', right: '\\]', display: true}
      ],
      throwOnError : false
    });
  });
</script>

  </div>
</body>
</html>