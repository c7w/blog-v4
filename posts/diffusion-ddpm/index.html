<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="baidu-site-verification" content="codeva-kligclmjAj" />
  
  
  <title>Diffusion-DDPM | c7w&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <meta name="description" content="这一系列博文我们对 Diffusion 模型进行简单的介绍 :)

我们认为比较重要的 Paper List：

重新回顾 Diffusion：更具多样性的生成式模型

[1503.03585] Deep Unsupervised Learning using Nonequilibrium Thermodynamics (ICML’15)
[2006.11239] Denoising Diffusion Probabilistic Models (NIPS’20)
[2102.09672] Improved Denoising Diffusion Probabilistic Models (ICML’21)
[2010.02502] Denoising Diffusion Implicit Models (ICLR’21)
[2106.15282] Cascaded Diffusion Models for High Fidelity Image Generation (JMLR)
[2104.07636] Image Super-Resolution via Iterative Refinement (TPAMI)


为生成添加引导

[2105.05233] Diffusion Models Beat GANs on Image Synthesis (NIPS’21)

[2112.10741] GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models (ICML’22)


[2207.12598] Classifier-Free Diffusion Guidance (NIPS’21 Workshop)


Departure to Latent Space

[2112.10752] High-resolution image synthesis with latent diffusion models (CVPR’22 Oral)

[2302.05543] Adding Conditional Control to Text-to-Image Diffusion Models


[2204.06125] Hierarchical Text-Conditional Image Generation with CLIP Latents (DALL·E 2)
[2205.11487] Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding (NIPS’22, Imagen)


未分类（还没读，咕咕咕）

Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer



DDPM

DDPM（Denoising Diffusion Probabilistic Models）是一种生成式模型，其可以将噪声 $\mathbf{x}_T$ 经过一系列级联的去噪（Denoising）过程生成具有真实感的图像 $\mathbf{x}_0$。
在 DDPM 中，具有两个重要的过程：前向过程（forward process, or diffusion process）和去噪过程（denoising process）：">
  
  
  
    <link rel="shortcut icon" href="../../favicon.ico">
  
  <link rel="stylesheet" href="../../css/style.css">
  
    <link rel="stylesheet" href="../../fancybox/jquery.fancybox-1.3.4.css">
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <div id="nav-outer">
  <nav id="main-nav" class="outer">
    <a id="main-nav-toggle" class="nav-icon"></a>
    
      <a class="main-nav-link" href="../../index.html">Home</a>
    
      <a class="main-nav-link" href="../../archives">Posts</a>
    
      <a class="main-nav-link" href="../../about">About</a>
    
      <a class="main-nav-link" href="../../friends">Friends</a>
    
    <div class="main-nav-space-between"></div>
    
  </nav>
</div>
<!-- <div id="header-title">
  <h1 id="logo-wrap">
    <a href="../../index.html" id="logo">c7w&#39;s Blog</a>
  </h1>
  
    <h2 id="subtitle-wrap">
      <a href="../../index.html" id="subtitle">Ready for some fun stuff :)</a>
    </h2>
  
</div> -->

      <div id="content" class="outer">
        <section id="main"><article id="post-diffusion-ddpm" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a class="article-date">
  <time class="dt-published" datetime="2023-06-11T16:00:38.893Z" itemprop="datePublished">2023-06-12</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="../../categories/AIGC/">AIGC</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Diffusion-DDPM
    </h1>
  

      </header>
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>这一系列博文我们对 Diffusion 模型进行简单的介绍 :)</p>
<p><img src="diffusion-ddpm.assets/Pasted%20image%2020230403223246.png" alt=""></p>
<p>我们认为比较重要的 Paper List<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<ul>
<li>重新回顾 Diffusion<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>更具多样性的生成式模型
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1503.03585">[1503.03585]</a> Deep Unsupervised Learning using Nonequilibrium Thermodynamics (ICML’15)</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.11239">[2006.11239]</a> <strong>Denoising Diffusion Probabilistic Models</strong> (NIPS’20)</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2102.09672">[2102.09672]</a> Improved Denoising Diffusion Probabilistic Models (ICML’21)</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.02502">[2010.02502]</a> Denoising Diffusion Implicit Models (ICLR’21)</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.15282">[2106.15282]</a> Cascaded Diffusion Models for High Fidelity Image Generation (JMLR)</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.07636">[2104.07636]</a> Image Super-Resolution via Iterative Refinement (TPAMI)</li>
</ul>
</li>
<li>为生成添加引导
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.05233">[2105.05233]</a> <strong>Diffusion Models Beat GANs on Image Synthesis (NIPS’21)</strong>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.10741">[2112.10741]</a> GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models (ICML’22)</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2207.12598">[2207.12598]</a> <strong>Classifier-Free Diffusion Guidance (NIPS’21 Workshop)</strong></li>
</ul>
</li>
<li>Departure to Latent Space
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.10752v2">[2112.10752]</a> <strong>High-resolution image synthesis with latent diffusion models</strong> (CVPR’22 Oral)
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.05543">[2302.05543]</a> Adding Conditional Control to Text-to-Image Diffusion Models</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2204.06125">[2204.06125]</a> Hierarchical Text-Conditional Image Generation with CLIP Latents (DALL·E 2)</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.11487">[2205.11487]</a> Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding (NIPS’22, Imagen)</li>
</ul>
</li>
<li>未分类<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>还没读<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>咕咕咕<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>
<ul>
<li>Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer</li>
</ul>
</li>
</ul>
<h2 id="ddpm">DDPM</h2>
<p><img src="diffusion-ddpm.assets/Pasted%20image%2020230404215439.png" alt=""></p>
<p>DDPM<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>Denoising Diffusion Probabilistic Models<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>是一种生成式模型<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其可以将噪声 <span class="markdown-them-math-inline">$\mathbf{x}_T$</span> 经过一系列级联的去噪<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>Denoising<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>过程生成具有真实感的图像 <span class="markdown-them-math-inline">$\mathbf{x}_0$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>在 DDPM 中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>具有两个重要的过程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span>前向过程<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><em>forward process, or diffusion process</em><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>和去噪过程<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span><em>denoising process</em><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<span id="more"></span>
<h3 id="前向过程">前向过程</h3>
<p>前向过程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>也就是图中的 <span class="markdown-them-math-inline">$q$</span> 过程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>通过不断加高斯噪音的方式将原始数据分布 <span class="markdown-them-math-inline">$q(\mathbf{x}_0)$</span> 转换为简单易处理的标准正态分布<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>在前向过程中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>一个初始的噪声图像 <span class="markdown-them-math-inline">$\mathbf{x}_T$</span> 被迭代地与一系列高斯噪声进行混合<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>生成一系列经过随机扰动的图像 <span class="markdown-them-math-inline">$\mathbf{x}_t$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其中 <span class="markdown-them-math-inline">$t=T-1,T-2,\ldots,0$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>更加形式化地<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们可以表示为<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right):=\mathcal{N}\left(\mathbf{x}_t ; \sqrt{1-\beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I}\right)
$$</div><p>这里 <span class="markdown-them-math-inline">$q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right)$</span> 表示隐变量 <span class="markdown-them-math-inline">$\mathbf{x}_t$</span> 相对于 <span class="markdown-them-math-inline">$\mathbf{x}_{t-1}$</span> 的条件概率分布<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>也就是说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果我们已知隐变量 <span class="markdown-them-math-inline">$\mathbf{x}_{t-1}$</span> 的取值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>想要获得隐变量 <span class="markdown-them-math-inline">$\mathbf{x}_t$</span> 的取值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们就要从这个关于 <span class="markdown-them-math-inline">$\mathbf{x}_t$</span> 的正态分布中采样<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其均值为 <span class="markdown-them-math-inline">$\sqrt{1-\beta_t}\mathbf{x}_{t-1}$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>方差为 <span class="markdown-them-math-inline">$\beta_t\mathbf{I}$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这里的数列 <span class="markdown-them-math-inline">${\beta_t}$</span> 由人工手动定义<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>在原文的实现中实现为在 <span class="markdown-them-math-inline">$\beta_1=10^{-4}$</span> 到 <span class="markdown-them-math-inline">$\beta_T=0.02$</span> 的线性插值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这里 <span class="markdown-them-math-inline">$T=1000$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>直观上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>每一步前向过程的操作就是对输入的数据加入高斯噪声的扰动<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>在该算法中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>从原始图片输入 <span class="markdown-them-math-inline">$\mathbf{x}_0$</span> 到完全的高斯噪声 <span class="markdown-them-math-inline">$\mathbf{x}_T$</span> 的过程是由一系列 <span class="markdown-them-math-inline">$q$</span> 过程级联产生的<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>我们认为这个级联后的过程是一个马尔科夫过程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这意味着在任意时间 <span class="markdown-them-math-inline">$t$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>当前状态 <span class="markdown-them-math-inline">$\mathbf{x}_t$</span> 的条件概率分布只依赖于其前一个状态 <span class="markdown-them-math-inline">$\mathbf{x}_{t-1}$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>即<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}, \mathbf{x}_{t-2}, \ldots, \mathbf{x}_0\right)=q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right)
$$</div><p>这里 <span class="markdown-them-math-inline">$q(\mathbf{x}_t \mid \mathbf{x}_{t-1}, \mathbf{x}_{t-2}, \ldots, \mathbf{x}_0)$</span> 表示在给定之前的所有状态的条件下<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>当前状态 <span class="markdown-them-math-inline">$\mathbf{x}_t$</span> 的条件概率分布<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$q(\mathbf{x}_t \mid \mathbf{x}_{t-1})$</span> 表示当前状态 <span class="markdown-them-math-inline">$\mathbf{x}_t$</span> 相对于 <span class="markdown-them-math-inline">$\mathbf{x}_{t-1}$</span> 的条件概率分布<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>根据马尔科夫过程的性质<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们有<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$q(\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_T \mid \mathbf{x}_0) =\prod_{t=1}^T q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right)
$$</div><p>这里 <span class="markdown-them-math-inline">$q(\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_T \mid \mathbf{x}_0)$</span> 表示隐变量 <span class="markdown-them-math-inline">$\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_T$</span><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>之后简记为 <span class="markdown-them-math-inline">$\mathbf{x}_{1:T}$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>相对于 <span class="markdown-them-math-inline">$\mathbf{x}_0$</span> 的联合条件概率分布<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>同时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>根据高斯分布的可叠加性<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们可以得出<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$q\left(\mathbf{x}_t \mid \mathbf{x}_0\right)=\mathcal{N}\left(\mathbf{x}_t ; \sqrt{\bar{\alpha}_t} \mathbf{x}_0,\left(1-\bar{\alpha}_t\right) \mathbf{I}\right),
$$</div><p>这里 <span class="markdown-them-math-inline">$\alpha_t:=1-\beta_t$</span>,  <span class="markdown-them-math-inline">$\bar{\alpha}_t:=\prod_{s=1}^t \alpha_s$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h3 id="去噪过程">去噪过程</h3>
<p>我们希望去噪过程是上述前向过程的逆过程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>也就是说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们要获得 <span class="markdown-them-math-inline">$q(\mathbf{x}_t \mid \mathbf{x}_{t-1})$</span>  的逆分布<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>我们知道<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>如果 <span class="markdown-them-math-inline">$q(\mathbf{x}_t \mid \mathbf{x}_{t-1})$</span> 是正态分布且 <span class="markdown-them-math-inline">$\beta_t$</span> 足够接近 0<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>那么其逆过程也近似为一个正态分布<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>我们通过一个神经网络 <span class="markdown-them-math-inline">$\theta$</span> 来拟合这个正态分布的均值<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>与方差<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>Improved DDPM<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right):=\mathcal{N}\left(\mathbf{x}_{t-1} ; \boldsymbol{\mu}_\theta\left(\mathbf{x}_t, t\right), \boldsymbol{\Sigma}_\theta\left(\mathbf{x}_t, t\right)\right)
$$</div><p>如果这个网络 <span class="markdown-them-math-inline">$\theta$</span> 被成功训练到可以拟合 <span class="markdown-them-math-inline">$p$</span> 过程<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>那么我们就具有了一个具有<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>去噪<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char></span>能力的网络<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>进而<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们可以从标准正态分布中采样 <span class="markdown-them-math-inline">$\mathbf{x}_T$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>然后经过 <span class="markdown-them-math-inline">$T$</span> 次 <span class="markdown-them-math-inline">$p_\theta$</span> 的变换<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>便得到了具有真实感的图像 <span class="markdown-them-math-inline">$\mathbf{x}_0$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>马尔可夫过程保证了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$p_\theta\left(\mathbf{x}_{0: T}\right):=p\left(\mathbf{x}_T\right) \prod_{t=1}^T p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right)
$$</div><h3 id="训练过程">训练过程</h3>
<p>训练的目标是通过模型生成的数据分布 <span class="markdown-them-math-inline">$p_\theta(\mathbf{x}_0)$</span> 与真实数据分布 <span class="markdown-them-math-inline">$q(\mathbf{x}_0)$</span> 尽可能相近<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>这可以通过最小化假设检验 <span class="markdown-them-math-inline">$\mathbb{E}\left[-\log p_\theta\left(\mathbf{x}_0\right)\right]$</span> 来实现<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>由于<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$\begin{aligned}
p_\theta\left(\boldsymbol{x}_0\right) &amp; =\int p_\theta\left(\boldsymbol{x}_{0: T}\right) d \boldsymbol{x}_{1: T}\\
p_\theta\left(\boldsymbol{x}_{0: T}\right) &amp; =p_\theta\left(\boldsymbol{x}_T\right) \prod_{t=1}^T p_\theta\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t\right)
\end{aligned}
$$</div><p>我们尝试去推出 <span class="markdown-them-math-inline">$\mathbb{E}\left[-\log p_\theta\left(\mathbf{x}_0\right)\right]$</span> 的上界<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>通过将其上界优化小的方式去优化该假设检验值<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$\begin{aligned}
\mathcal{L}_{c e} &amp; =-\mathbb{E}_{q\left(\boldsymbol{x}_0\right)}\left(\log p_\theta\left(\boldsymbol{x}_0\right)\right) \\
&amp; =-\mathbb{E}_{q\left(\boldsymbol{x}_0\right)}\left(\log \int p_\theta\left(\boldsymbol{x}_{0: T}\right) d \boldsymbol{x}_{1: T}\right) \\
&amp; =-\mathbb{E}_{q\left(\boldsymbol{x}_0\right)}\left(\log \int q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right) \frac{p_\theta\left(\boldsymbol{x}_{0: T}\right)}{q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)} d \boldsymbol{x}_{1: T}\right) \\
&amp; =-\mathbb{E}_{q\left(\boldsymbol{x}_0\right)}\left(\log \mathbb{E}_{q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)}\left(\frac{p_\theta\left(\boldsymbol{x}_{0: T}\right)}{q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)}\right)\right) \\
&amp; \leq-\mathbb{E}_{q\left(\boldsymbol{x}_0\right)}\left(\mathbb{E}_{q\left(\boldsymbol{x}_{1: T \mid} \mid \boldsymbol{x}_0\right)}\left(\log \frac{p_\theta\left(\boldsymbol{x}_{0: T}\right)}{q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)}\right)\right) \\
&amp; =-\iint q\left(\boldsymbol{x}_0\right) q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right) \log \frac{p_\theta\left(\boldsymbol{x}_{0: T}\right)}{q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)} d \boldsymbol{x}_0 d \boldsymbol{x}_{1: T} \\
&amp; =-\int q\left(\boldsymbol{x}_{0: T}\right) \log \frac{p_\theta\left(\boldsymbol{x}_{0: T}\right)}{q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)} d \boldsymbol{x}_{0: T} \\
&amp; =-\mathbb{E}_{q\left(\boldsymbol{x}_{0: T}\right)}\left[\log \frac{p_\theta\left(\boldsymbol{x}_{0: T}\right)}{q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)}\right]=\mathbb{E}_{q\left(\boldsymbol{x}_{0: T}\right)}\left[\log \frac{q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)}{p_\theta\left(\boldsymbol{x}_{0: T}\right)}\right]
\end{aligned}
$$</div><blockquote>
<p>这里用了变分自编码器<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>VAE<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>中的重要不等式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>通常称为<span class="bd-box"><h-char class="bd bd-end"><h-inner>“</h-inner></h-char></span>ELBO<span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>evidence lower bound<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span>不等式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>”</h-inner></h-char><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>下面对式子进行解释<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<p>左边的期望值是关于潜在变量 <span class="markdown-them-math-inline">$\mathbf{z}$</span> 的先验分布 <span class="markdown-them-math-inline">$p(\mathbf{z})$</span> 取期望后的结果<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>其中 <span class="markdown-them-math-inline">$\mathbf{z}$</span> 通过生成模型 <span class="markdown-them-math-inline">$p_\theta(\mathbf{x}_0, \mathbf{z})$</span> 与观测变量 <span class="markdown-them-math-inline">$\mathbf{x}_0$</span> 相关联<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>根据贝叶斯定理<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们有<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$p_\theta\left(\mathbf{x}_0, \mathbf{z}\right)=p_\theta\left(\mathbf{x}_0 \mid \mathbf{z}\right) p(\mathbf{z})
$$</div><p>因此<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以将左边的期望值写成以下形式<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$\mathbb{E}\left[-\log p_\theta\left(\mathbf{x}_0\right)\right]=\mathbb{E}_{p(\mathbf{z})}\left[-\log p_\theta\left(\mathbf{x}_0 \mid \mathbf{z}\right)\right]-\mathbb{H}[p(\mathbf{z})]
$$</div><p>其中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span><span class="markdown-them-math-inline">$\mathbb{H}\left[p(\mathbf{z})\right]$</span> 是潜在变量的先验分布 <span class="markdown-them-math-inline">$p(\mathbf{z})$</span> 的熵<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>可以看作是潜在变量的不确定度的度量<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>在我们的例子中<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>潜在变量 <span class="markdown-them-math-inline">$\mathbf{z}$</span> 为 <span class="markdown-them-math-inline">$\mathbf{x}_{1:T}$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
</blockquote>
<p>接下来我们用马尔科夫过程的性质把 <span class="markdown-them-math-inline">$\mathbb{E}_{q\left(\boldsymbol{x}_{0: T}\right)}\left[\log \frac{q\left(\boldsymbol{x}_{1: T} \mid \boldsymbol{x}_0\right)}{p_\theta\left(\boldsymbol{x}_{0: T}\right)}\right]$</span> 中的联合概率分布拆开<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>同时<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>因为 <span class="markdown-them-math-inline">$q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right)$</span> 的分布的 closed-form solution 不可知<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们引入 <span class="markdown-them-math-inline">$\mathbf{x}_0$</span> 后利用贝叶斯公式可以转换成 <span class="markdown-them-math-inline">$q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0\right)$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这个分布的参数为<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0\right)=\mathcal{N}\left(\mathbf{x}_{t-1} ; \tilde{\boldsymbol{\mu}}_t\left(\mathbf{x}_t, \mathbf{x}_0\right), \tilde{\beta}_t \mathbf{I}\right) \\
\text{where} \quad \tilde{\boldsymbol{\mu}}_t\left(\mathbf{x}_t, \mathbf{x}_0\right):=\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} \mathbf{x}_0+\frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t} \mathbf{x}_t \quad \\
\text{and} \quad \tilde{\beta}_t:=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t
$$</div><p>最终我们发现上述上界等价于<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$\mathbb{E}_q[\underbrace{D_{\mathrm{KL}}\left(q\left(\mathbf{x}_T \mid \mathbf{x}_0\right) \| p\left(\mathbf{x}_T\right)\right)}_{L_T}+\sum_{t&gt;1} \underbrace{D_{\mathrm{KL}}\left(q\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0\right) \| p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right)\right)}_{L_{t-1}} \underbrace{-\log p_\theta\left(\mathbf{x}_0 \mid \mathbf{x}_1\right)}_{L_0}]
$$</div><p>我们希望的优化目标<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>便是这些概率分布之间的 KL 散度<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h3 id="优化目标的进一步改进">优化目标的进一步改进</h3>
<p>我们考虑 <span class="markdown-them-math-inline">$L_{t-1}$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>由于计算 KL 散度的两个分布都是正态分布<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们可以直接得到闭式解<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$\begin{aligned}
\mathcal{L}_{t-1} =\mathbb{E}_{q\left(\boldsymbol{x}_0, \boldsymbol{x}_t\right)}\left[\frac{1}{2 \sigma_t^2}\left\|\tilde{\boldsymbol{\mu}}\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)-\boldsymbol{\mu}_\theta\left(\boldsymbol{x}_t, t\right)\right\|^2\right]+C
\end{aligned}
$$</div><p>这相当于<strong>直接预测 <span class="markdown-them-math-inline">$p_\theta$</span> 过程的均值</strong><span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>而由于我们在训练过程中知道前向过程 <span class="markdown-them-math-inline">$\mathbf{x}_t=\sqrt{\bar{\alpha}_t} \mathbf{x}_0+\sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>且在训练过程中要将 <span class="markdown-them-math-inline">$\boldsymbol{\mu}_\theta\left(\boldsymbol{x}_t, t\right)$</span> 回归到的值只与 <span class="markdown-them-math-inline">$\boldsymbol{x}_0$</span> 和 <span class="markdown-them-math-inline">$\boldsymbol{x}_t$</span> 有关<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>于是我们可以代入消去这个回归值中的 <span class="markdown-them-math-inline">$\boldsymbol{x}_0$</span><span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>得到我们要把 <span class="markdown-them-math-inline">$\boldsymbol{\mu}_\theta\left(\boldsymbol{x}_t, t\right)$</span> 回归到<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$\frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \boldsymbol{\epsilon}\right)
$$</div><p>而这里的 <span class="markdown-them-math-inline">$\boldsymbol{x}_t$</span> 是 <span class="markdown-them-math-inline">$\boldsymbol{\mu}_\theta\left(\boldsymbol{x}_t, t\right)$</span> 的输入<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>本着简便的原则<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们可以设计<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$\boldsymbol{\mu}_\theta\left(\boldsymbol{x}_t, t\right) = \frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta\left(\boldsymbol{x}_t, t\right)\right)
$$</div><p>也就是说<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们的网络 <span class="markdown-them-math-inline">$\boldsymbol{\epsilon}_\theta$</span> 最终只需要输入 <span class="markdown-them-math-inline">$\boldsymbol{x}_t$</span> 和当前时间的 time embedding<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>输出我们在前向过程中相对原图添加的噪声即可<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<p>于是我们可以对 Loss 稍作修改<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$\mathbb{E}_{\mathbf{x}_0, \epsilon}\left[\frac{\beta_t^2}{2 \sigma_t^2 \alpha_t\left(1-\bar{\alpha}_t\right)}\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\sqrt{\bar{\alpha}_t} \mathbf{x}_0+\sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}, t\right)\right\|^2\right]
$$</div><p>再简便一些<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>我们丢弃前面的系数<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<div class="markdown-them-math-block">$$L_{\text {simple }}(\theta):=\mathbb{E}_{t, \mathbf{x}_0, \epsilon}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\sqrt{\bar{\alpha}_t} \mathbf{x}_0+\sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}, t\right)\right\|^2\right]
$$</div><p>这相当于对于不同时间步的 Loss 做了一个分配不同权重的操作<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span>事实上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>这里将 <span class="markdown-them-math-inline">$t$</span> 较小的时间步的 Loss 权重降低<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>使得模型不用花费太多精力在考虑 fine-grained 的细节上<span class="bd-box"><h-char class="bd bd-beg"><h-inner>。</h-inner></h-char></span></p>
<h3 id="reference">Reference</h3>
<p>除了原论文外<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>本节内容的整理我们参考了以下内容<span class="bd-box"><h-char class="bd bd-beg"><h-inner>：</h-inner></h-char></span></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://kxz18.github.io/2022/06/19/Diffusion/">https://kxz18.github.io/2022/06/19/Diffusion/</a><span class="bd-box"><h-char class="bd bd-end"><h-inner>（</h-inner></h-char></span>推导懒得自己写直接参考了这里<span class="bd-box"><h-char class="bd bd-beg"><h-inner>）</h-inner></h-char></span></li>
</ul>
<p>感觉水够一篇的篇幅了<span class="bd-box"><h-char class="bd bd-beg"><h-inner>，</h-inner></h-char></span>换一篇接着写 :)</p>

      
    </div>
    <footer class="article-footer">
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="../cs285/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Note for CS 285, Deep Reinforcement Learning
        
      </div>
    </a>
  
  
    <a href="../hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">
        
          Hello World
        
      </div>
    </a>
  
</nav>

  
</article>


</section>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
        <div><p><span id="busuanzi_container_site_pv">Site total view count: <span id="busuanzi_value_site_pv">Loading...</span></span></p><p>Powered by <a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/">Hexo</a> & Theme <a target="_blank" rel="noopener" href="https://github.com/bill-xia/hexo-theme-mashiro">Mashiro</a> & GitHub Pages</p><p>Copyright © 2020-2024 c7w. LICENSE CC BY-NC-SA 4.0.</p></div>
      
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="../../index.html" class="mobile-nav-link">Home</a>
  
    <a href="../../archives" class="mobile-nav-link">Posts</a>
  
    <a href="../../about" class="mobile-nav-link">About</a>
  
    <a href="../../friends" class="mobile-nav-link">Friends</a>
  
</nav>
    
<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="../../js/clipboard.min.js"></script>
<script src="../../js/jquery-1.4.3.min.js"></script>

<script src="../../fancybox/jquery.fancybox-1.3.4.pack.js"></script>


<script src="../../js/script.js"></script>








<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '$', right: '$', display: false},
        {left: '\\(', right: '\\)', display: false},
        {left: '\\[', right: '\\]', display: true}
      ],
      throwOnError : false
    });
  });
</script>

  </div>
</body>
</html>